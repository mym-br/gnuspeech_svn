<HTML>
<HEAD>
<TITLE>Speech perception, production and physiology concepts for researchers</TITLE>
</HEAD>

<BODY>

<CENTER>
<H1>A Conceptionary for Speech & Hearing in the Context of Machines and Experimentation</H1><BR>
<P>by<P>

<H3>David R. Hill</H3>

Copyright (c) <A HREF="mailto:hill@cpsc.ucalgary.ca">David R. Hill 2001</A>.  All rights reserved.<BR>No part of this work may be reproduced or transmitted in any form or by any means, electronic, mechanical,<BR>
 photocopying, recording or otherwise without prior written permission of the <A HREF="mailto:hill@cpsc.ucalgary.ca">author</A>
</CENTER>

<H2>Introduction</H2>
<P>
A conceptionary, like a dictionary, is useful in learning the meanings of words.  Unlike a dictionary, it is designed to make the reader work a little, and to develop associations and a conceptual framework for the subject of the conceptionary. It is not an encyclopaedia, though it has as one function, in new or inter-disciplinary areas, the drawing together and reconciliation of disparate sources.  Like Marmite (a yeast extract), it is designed to be nutritious and highly concentrated. Do not be put off by the consequent strong flavour.  The aim is to achieve broad coverage of material from areas such as psychology and physics, as well as from speech recognition and synthesis, because such material is highly relevant to speech researchers and often difficult to track down.  Historical material is included for the same reasons.  The view of the field expressed is a rather personal view.  The author wishes to thank Mr. R.L. Jenkins for his careful reading of the manuscript and many useful comments and corrections, while laying sole claim to any errors, ommissions, or lack of clarity that remain.
</P><P>
Note that although URLs that allow access to some of the major speech research centres are included within this document, many more centres exist.  A search using the google search facility, with the phrase "speech communication research labs" will turn up a large selection of these, but not necessarily including any of those provided below!
</P>

<P>
Please email any comments, corrections or suggestions to <A HREF="mailto:hill@cpsc.ucalgary.ca">hill@cpsc.ucalgary.ca</A>.  While every effort has been made to present accurate information, it is not intended to support safety critical systems.  Material should always be cross-checked with other sources.

</P><P>
<A NAME = "AH1/AH2">
<I>AH1/AH2</I>
</A><BR>

Amplitudes of two kinds of hiss used as part of the excitation arrangements of the <A HREF = "#ParametricArtificialTalker">Parametric Artificial Talker</A> (PAT -- the original resonance analog <A HREF = "#speechSynthesiser">speech synthesiser</A>).  Hiss 1 is fed into the <A HREF = "#formant">formant</A> branch of the filter function, and represents <A HREF = "#aspiration">aspiration</A> noise, Hiss 2 is fed to a separate filter and represents the noises (such as the /s/ in sun and the /sh/ in shun) produced by forcing air through a narrow constriction towards the front of the oral cavity, and therefore assumed to be relatively unaffected by the usual oral resonances, or formants.
<P>

<A NAME = "Ax/Ao">
<I>Ax /Ao</I>
</A><BR>

Amplitude of voiced energy excitation (often called larynx amplitude) injected into the filter function of a <A HREF = "#speechSynthesiser">speech synthesiser</A>.  Corresponds to <A HREF = "#voicing">voicing</A> effort in natural speech.
</P>

<A NAME = "absoluteRefractoryPeriod">
<I>absolute refractory period</I>
</A><BR>

See <A HREF = "#nerveCell">nerve cell</A><P>

<A NAME = "acousticAnalog">
<I>acoustic analog</I>
</A><BR>

A construct that models the frequency-time-energy pattern of some sound without regard to how it is produced, or the mechanism involved.  Such a model is only constrained by the bandwidths and dynamic range inherent in its structure.  See also <A HREF = "#resonanceAnalog">resonance analog</A>, <A HREF = "#physiologicalAnalog">physiological analog</A> and <A HREF = "#terminalAnalogSpeechSynthesiser">terminal analog</A>.
</P>

<A NAME = "acousticCorrelates">
<I>acoustic correlates</I>
</A><BR>

Those acoustic phenomena that correlate (or necessarily co-occur) with articulatory events.  If they can be discovered and well defined, they may be used to make inferences about articulation.  For example, a burst of noise usually occurs when a blockage in the vocal passages is released, so the detection of a burst of noise is some evidence that the input speech to a recogniser contained a stop sound.  Unfortunately, the acoustic correlates of articulatory events are are not well-defined.  Speech recognisers require means of normalisation, prediction, extrapolation, and the like.
</P>

<A NAME = "acousticDomain">
<I>acoustic domain</I>
</A><BR>
Description or existence in terms of the amount of energy present at given times and frequencies.  A <A HREF = "#spectrogram">spectrogram</A> is an acoustic domain description of speech.  It is contrasted with time domain descriptions.  A time domain description of speech is the time waveform.
</P>

<A NAME = "acousticIntensity">
<I>acoustic intensity</I>
</A><BR>

Is defined as the average power transmitted per unit area in the direction of wave propagation.
I = p(rms)<SUP>2</SUP>/r.c watts/square metre where p(rms) is the root-mean-square effective acoustic pressure, r is the density of the medium (air at 20 degrees C and standard pressure gives 1.21 kg/cubic metre), and c the speed of sound in the medium (same air gives c=343 metres/sec).
</P>

<A NAME = "acousticIntensityLevel">
<I>acoustic intensity level</I>
</A><BR>

See <A HREF = "#soundIntensityLevel">sound intensity level</A>
<P>

<A NAME = "acuity">
<I>acuity</I>
</A><BR>

The power of discrimination in some sensory mode.  See <A HREF = "#differenceLimen">difference limen</A>.
</P>

<A NAME = "afferent">
<I>afferent</I>
</A><BR>

Incoming from the periphery to the central nervous system.  Thus afferent nerves are sensory nerves running in from the sensory receptors scattered around an organism.
</P>

<A NAME = "affricate">
<I>affricate</I>
</A><BR>

A speech sound fromed by the close juxtaposition of a <A HREF = "#voicedStop">stop</A> and a <A HREF = "#fricative">fricative</A>, in that order.  For English, voiceless and voiced examples occur at the start of "chips" and "judge" respectively.  The affricate is distinguished from the corresponding stop-fricative combination chiefly by the much shorter burst of friction noise which accompanies the explosive separation of the articulators (thus my chip is different from might ship, a problem in notation solved by introducing a hyphen to distinguish the juncture).
</P>

<A NAME = "AGC">
<I>AGC</I>
</A><BR>

see <A HREF = "#automaticGainControl">automatic gain control</A><P>

<A NAME = "allophone">
<I>allophone</I>
</A><BR>

see <A HREF = "#phoneme">phoneme</A><P>

<A NAME = "AmesRoom">
<I>Ames room</I>
</A><BR>

A room of unusual shape, designed by Professor Ames of Princeton University, so that the retinal image of the room from a particular view point coincides exactly with the image that a normal room would produce.  People walking about the room appear to shrink and grow as they occupy larger and smaller parts which are further and nearer (respectively), because the brain assumes the room is normally shaped, and therefore misinterprets the changing image size as size change instead of distance change.  Intellectual knowledge does not dispel the illusion.  Experience of trying to do things inside the room can cause the room to be perceived as it really is.  The effect lends itself to dramatic demonstration on film, as it depends on viewing with only one eye.  See also the <A HREF="#moonIllusion">moon illusion</A>.
</P>

<A NAME = "alveolarRidge">
<I>alveolar ridge</I>
</A><BR>

As the hard palate runs towards the top front teeth from behind, a buttress enclosing the roots of the teeth is encountered.  This is called the alveolar ridge.
</P>

<A NAME = "anacrusis">
<I>anacrusis</I>
</A><BR>

A term borrowed from music.  In music, an anacrusis is a series of notes that are not counted towards the total note duration in a bar–they are "extra".  Jassem uses the term anacrusis for what Abercrombie and others have called proclitic syllables–syllables at the end of a rhythmic unit in speech that belong, grammatically speaking, with the following rhythmic unit.  Jassem chose the term deliberately because, in his theory of British English speech rhythm, the anacruses are not to be counted in the duration of the rhythmic unit (called a footby Abercrombie and others or a rhythm unit by Jassem).  See <A HREF = "#isochrony">isochrony</A>, <A HREF = "#foot">foot</A>, <A HREF = "#proclitic">proclitic</A>, <A HREF = "#enclitic">enclitic</A>.
</P>

<A NAME = "articulation">
<I>articulation</I>
</A><BR>

The process of placing the tongue, lips, teeth, <A HREF = "#velum">velum</A>, <A HREF = "#pharynx">pharynx</A> and <A HREF = "#vocalFolds">vocal folds</A> in the state or succession of states required for some utterance.  Also used for one such state (e.g. "consonant articulation").  There is unconcious ambiguity in just what is meant in normal use (e.g. "He articulated clearly" implying that articulation itself is "sound", rather than a placing of the articulators in some "articulatory posture"..  One could  easily articulate an isolated consonant, without actually producing sound.  Sound only results when excitation energy of some sort is supplied (e.g. by using muscles to expel air from the lungs so that the vocal folds vibrate.
</P>

<A NAME = "articulationIndex">
<I>articulation index</I>
</A><BR>

A method of estimating speech <A HREF = "#intelligibility">intelligibility</A> based upon the division of the speech spectrum into 20 bands contributing equally to intelligibility.  It is assumed that, for each band, the intelligibility contribution is proportional to the <A HREF = "#signalToNoiseRatio">signal to noise ratio</A> in the band, being 100% for s/n greater than or equal to 30 <A HREF = "#db">db</A> and zero for s/n less than or equal to 0 <A HREF = "#db">db</A>.  The original method (French & Steinberg) using 20 bands of equal weight, and of width related to the <A HREF = "#criticalBand">critical band</A> distribution, has been adapted to 15 1/3 octave bands with appropriately differing weights, which more suited to the measuring apparatus that is easily available.  Under some circumstances (e.g. when given directional cues or special instructions, etc.) speech may be intelligible even when masked at an overall signal to noise ratio of -10 or even -15 <A HREF = "#db">db</A>.  This should not be taken as contradiction of the validity of articulation index calculations.
</P>

<A NAME = "articulatorySynthesis">
<I>articulatory synthesis</I>
</A><BR>

<A HREF = "#speechSynthesiser">Speech synthesis</A> based on the use of articulatory constraints.  Articulatory constraints may be applied to the generation of parameters for any synthesiser but true articulatory synthesis depends on the articulatory constraints being inherent in the implementation of the synthesis system, as the reproduction of all the details is otherwise very difficult if not impossible.  A transmission-line analog emulates the propagation of sound waves in the acoustic tube formed by the vocal tract directly.  Thus the full spectrum is produced, with correct energy interaction for nasal sounds, and correct shapes for the <A HREF = "#formant">formants</A>.  True articulatory synthesis is to other approaches to synthesis as true ray tracing is to polygonal modeling in computer graphics.  By modeling the physics of reality directly, articulatory synthesis achieves a high degree of fidelity to nature.  The problems arise from a need for a great deal of computational power, the need to manage the control problem successfully, and the need for accurate articulatory data.  Ideally, the various energy sources should be emulated on the basis pf physics but the only complete articulatory synthesis system currently available (originally a commercial development by Trillium Sound Research for the NeXTSTEP operating system but now under a <A HREF="http://www.gnu.org/copyleft/gpl.html" TARGET="new">General Public Licence</A> from the <A HREF="http://www.gnu.org" TARGET="new">the Free Software Foundation</A>) injects appropriate waveforms at appropriate places in the vocal tract, because of a lack of computational power, as well as lack of suitable models.  The research is continuing.  See the paper <A HREF="../avios95/index.htm" TARGET="new">"Real-time articulatory speech-synthesis-by-rules"</A> for more detail.
</P>

<A NAME = "artificialEar">
<I>artificial ear</I>
</A><BR>

Not a prosthetic device (yet).  A device that simulates the acoustics of the head and the ear cavity of a human being, measuring (by means of a calibrated <A HREF = "#transducer">transducer</A>) the pressure that would occur at the <A HREF = "#earDrum">ear-drum</A> of a real ear.  It is used in determining the total effect of earphones plus cushions on the fidelity of reproduction of speech without resorting to sophisticated, highly trained (i.e. expensive) real listeners for subjective testing.  The latter is a far more reliable method of evaluation.  See <A HREF = "#pinna">pinna</A>.
</P>

<A NAME = "artificialLarynx">
<I>artificial larynx</I>
</A><BR>

A device used for prosthesis in laryngectomized persons.  It is held against the throat and, when activated by a button, it injects vibratory energy into the vocal tract as a substitute for the lost voice capability.  It is not too difficult to use, but less convenient than <A HREF = "#oesophagalSpeech">oesophagal speech</A>.
</P>

<A NAME = "artificialMouth">
<I>artificial mouth</I>
</A><BR>

Not a prosthetic device (yet).  A model designed to simulate the acoustic characteristics of the head and mouth upon the radiated sound.  Used, for example, in evaluating microphones.
</P>

<A NAME = "ARU">
<I>ARU</I>
</A><BR>

See audio response unit.
</P>

<A NAME = "aspiration">
<I>aspiration</I>
</A><BR>

The escape of breath through a relatively unconstricted vocal tract, without accompanying vibration of the <A HREF = "#vocalFolds">vocal folds</A>. Some turbulent or friction noise is generated but it is not clear whether this is due to turbulence at the <A HREF = "#glottis">glottis</A> or along the walls of the vocal tract, in general.
</P>


<A NAME = "ASR">
<I>ASR</I>
</A><BR>

See <A HREF = "#automaticSpeechRecognition">automatic speech recognition</A><P>

<A NAME = "audibility">
<I>audibility</I>
</A><BR>

The degree to which an acoustic signal can be detected by a listener.  See also <A HREF="#intelligibility">intelligibility</A>.
</P>


<A NAME = "audioResponseUnit">
<I>audio response unit</I>
</A><BR>

A peripheral device attached to a computer to allow voice messages to be sent to users connected by telephone.  A Touch-Tone phone is used in most current applications to provide input.  The ARU may replay direct recordings, or compressed recordings, or it may synthesise the speech using rules based on general speech knowledge.  The commonest current method uses compressed recordings based on <A HREF = "#linearPredictiveCoding">Linear Predictive Codingg</A> (LPC) parameters.
</P>

<A NAME = "audiogram">
<I>audiogram</I>
</A><BR>

See <A HREF = "#audiometry">audiometry</A><P>

<A NAME = "audiometry">
<I>audiometry</I>
</A><BR>

The process of determining the frequency distribution of a subject's absolute <A HREF = "#thresholdOfHearing">threshold of hearing</A> (in <A HREF = "#db">db</A>, compared to "normal" threshold).  Pure tone stimuli at selected frequencies are used to sample this distribution by presenting them at various intensities and noting response/no-response from the patient.  Quiet conditions (noise not greater than 10 <A HREF = "#db">db</A> below threshold of hearing in the booth), noise shielded earphones, and special instruments are used.  Each ear is evaluated separately.  The plot of threshold against frequency for the two ears is called an audiogram, and shows threshold in <A HREF = "#db">db</A> (referenced the normal threshold) as a function of frequency.
</P>

<A NAME = "audiometer">
<I>audiometer</I>
</A><BR>

An instrument calibrated to produce tones and other sounds at known intensities.  It is designed to produce audiograms for subjects whose hearing is to be tested.
</P>

<A NAME = "auditoryCortex">
<I>auditory cortex</I>
</A><BR>

That part of the cortex (the convoluted outer layer of the brain, representing the highest degree of evolution) that is primarily responsible for processing auditory signals (i.e. those nerve impulses originating from the ear–specifically from the <A HREF = "#basilarMembrane">basilar membrane</A>).
</P>

<A NAME = "auditoryMeatus">
<I>auditory meatus</I>
</A><BR>

In full, the external auditory meatus.  The canal connecting the eardrum to the outside air.  The external opening of the meatus is within the area of the <A HREF = "#pinna">pinna</A>.  (The pinnae are the things we are talking about when we say "My ears are cold").  The meatus is slightly curved (which prevents the ear-drum being viewed without instruments - e.g. an otoscope); it is about 3 cm long and about 1 cm or less in diameter.  Hairs and wax glands protect against ingress of insects etc.Excess wax can cause some loss of hearing.  See also <A HREF = "#pinna">pinna</A>.
</P>

<A NAME = "auditoryNerve">
<I>auditory nerve</I>
</A><BR>

See <A HREF = "#modiolus">modiolus</A><P>

<A NAME = "auditoryPathways">
<I>auditory pathways</I>
</A><BR>

<A HREF = images/dh15.jpg TARGET="top"><IMG SRC="images/dh15thum.jpg" ALIGN=RIGHT></A>
</P>

The paths traced by the nerves leading from the <A HREF = "#organOfCorti">organ of Corti</A> in the <A HREF = "#cochlea">cochlea</A> to the auditory cortex.
In logical order, the significant staging posts are:  <A HREF = "#organOfCorti">organ of Corti</A>:  <A HREF = "#CortiGanglion">Corti ganglion</A>:  ventral and dorsal cochlear nucleii:  superior olive:  inferior colliculus:  medial geniculate:  and finally auditory cortex.
The signals are fed roughly equally to both halves of the brain (cross-overs occuring at the level of <A HREF = "#medulla">medulla</A> and the path between there and the inferior colliculus):  also side branches occur in the neighbourhood of the <A HREF = "#medulla">medulla</A> (the <A HREF = "#medulla">medulla</A> embraces the trapezoid body, the dorsal and ventral cochlear nucleii, and the two superior olives), at the lateral lemniscate and nucleus, and at the cerebellar vermis.  The dorsal cochlear nucleus sends all its outgoing processes to the <A HREF = "#contralateral">contralateral</A> side of the brain, though there are connections back in the inferior colliculus region.  Based on their neurophysiological studies of cats, Whitfield and Evans (Keele University, U.K.) have stated that the primary function of the auditory cortex is the analysis of time pattern rather than stimulus frequency.<BR>

<A HREF = images/dh15.jpg TARGET="top">Afferent acoustic pathways</A><BR>

<FONT SIZE=1>From <I>Handbook of Experimental Psychology</I>,<BR>S.S. Stevens (ed.). © 1951 John Wiley & Sons.<BR>Used with permission of John Wiley & Sons.</FONT>
</P>
<BR CLEAR=all>

<A NAME = "automaticGainControl">
<I>automatic gain control</I>
</A><BR>

A device often included in electronic audio processing systems to limit the amplitude of the signal without actually <A HREF = "#clippedSpeech">clipping</A> it.  A long-term average of the speech energy is taken and used to control the gain of the amplifier to the system, turning the gain down as the input speech energy increases.  Thus signal levels may be kept near optimum levels without overload.  If required, the control signal may be transmitted along with the compressed signal to allow re-expansion to the original dynamic range. Such a device is called a compander (naturally).  Compression involves distortion as well.  The time constants involved in deriving the control signal are important.  Faster action is more likely to prevent overload, but produces greater distortion.  Longer time constants may be used where the average signal is not expected to vary quickly.
</P>

<A NAME = "automaticSpeechRecognition">
<I>automatic speech recognition</I>
</A><BR>

The process of automatically (i.e., by machine) rendering spoken sounds as correct language symbols.  It has been contrasted with automatic speech understanding which means the production of a correct response by a machine for given verbal imput.  The problem of recognising unlimited vocabulary for arbitrary speakers in continuous speech remains an unsolved goal.  Most current systems require some degree of training, recognise relatively restricted vocabularies, and only deal with isolated words or phrases (IWR).  Continuous Speech Recognition (CSR) is necessary for normal speech as there are no breaks or boundaries between words in normal utterances, unless a pause occurs.  It is quite difficult to speak in such a way that a pause is inserted before every word. <P>

<A NAME = "Ax">
<I>Ax</I>
</A><BR>

The amplitude of glottal excitation supplied to a synthesis model (the "larynx amplitude").
</P>

<A NAME = "axon">
<I>axon</I>
</A><BR>

The output process of a nerve cell (a process is an outgrown limb or filament).
</P>

<A NAME = "bandwidth">
<I>bandwidth</I>
</A><BR>

That contiguous band of frequencies that may be transmitted through some signal processing system with no more than 3 <A HREF = "#db">db</A> loss of power (i.e., no frequency in the band is attenuated to less than half the original power). A signal is often loosely ascribed a "bandwidth".  This really refers to the bandwidth of the processing system that would be needed to process the signal without introducing unacceptable distortion or loss of significant components.  For complete fidelity, the bandwidth of the processing system should at least equal the frequency spectrum range of the signal's significant components.
</P>

<A NAME = "bar">
<I>bar</I>
</A><BR>

In the sense of "unit of measurement" a bar is the pressure exerted by the "standard atmosphere".  Unfortunately, as with early temperature scales, a slight miscalculation has meant that the standard atmosphere accepted today actually exerts a pressure of 1029 millibars (1.029 bars).  In modern terms:  1 bar = 0.1 MPa (MegaPascals) where a Pascal is 1 nt/m2 (Newton per Square Metre), and is the standard <A HREF = "#SI">SI </A>unit of pressure.
</P>

<A NAME = "basilarMembrane">
<I>basilar membrane</I>
</A><BR>

The <A HREF = "#cochlea">cochlea</A>, or organ of hearing, is a spiral chamber.  It is divided, longitudinally, into two thinner tubes (the scala vestibuli and scala tympani) by the cochlear partition.  The cochlear partition is itself a duct, triangular in cross section, bounded on the shortest side by the outer wall of the cochlear wall, by Reissner's membrane, and -- on the third side -- a combination of a bony shelf and the basilar membrane.  The bony shelf is cantilevered out from the <A HREF = "#modiolus">modiolus</A>, or centre-post, of the spiral chamber formed by the <A HREF = "#cochlea">cochlea</A>.  The bony shelf starts wide near the oval and round windows at one end of the cochlear spiral, and narrows down towards the helicotrema, an opening at the other end of the cochlea's spiral tube that joins the two perilymph-filled scala.  Thus the basilar membrane starts off narrow by the windows, and becomes progressively wider towards the helicotrema, since the tuve forming the <A HREF = "#cochlea">cochlea</A> is fairly constant in diameter.  The basilar membrane is about 3.5 cm long and varies in width from 0.1 mm at the window end to 0.5 mm at the helicotrema end, making about 2.5 turns in the spiral.  The scala are about 2 mm in diameter throughout.  Bekesy remarks sanguinely that "it is difficult to carry out experiments with dimensions as small as these."  The basilar memberane bears, on its inner surface, the <A HREF = "#organOfCorti">organ of Corti</A>, which activates the sensitive hair cells and hence generates nerve pulses to the brain.   See also, <A HREF = "#tectorialMembrane">tectorial membrane</A> and <A HREF = "#membranousLabyrinth">membranous labyrinth</A>.
<P>
<CENTER>
<A HREF = images/dh12.jpg TARGET=figs><IMG SRC=images/dh12thum.jpg><BR><B>Cross-section of cochlear canal</B></A>.<BR>
<FONT SIZE=1> From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons.</FONT>
</P>

</CENTER>

<A NAME = "BBN">
<I>BBN</I>
</A><BR>

Bolt, Beranek, and Newman.  An important company in speech, linguistics, and computers.  Beranek, one of the founders, is prominent in the early acoustics research literature.
</P>

<A NAME = "bell">
<I>Bell Telephone Laboratories</I>
</A><BR>

Bell Telephone Laboratories, better known as Bell Labs, is perhaps the best known research establishment in the world with leading edge research in a mind-boggling number of areas, originally funded by profits from telephone network operation.  Formed in 1907 by the combination of AT&T and Western Electric Engineering Departments, but not named till 1925.  A consent decree split AT&T in 1956, but Bell Labs continued as one of the world's top two or three speech and communication research laboratories.  In 1996, AT&T underwent another major re-organisation and splitting to meet changing world market conditions.  Bell Laboratories now forms part of Lucent Technologies.  See the relevant <A HREF="http://www.lucent.com/news/history.html" TARGET="new">Lucent Technologies history page</A> on their <A HREF="http://www.lucent.com" TARGET="new"> web site</A>.  A <A HREF="http://www.bell-labs.com" TARGET="new">web site specific to Bell Labs</A> is also available.  Unix was invented there by Dennis Ritchie and Ken Thompson; James Flanagan, of speech research fame, was there; Claude Shannon, of <A HREF="#informationTheory">information theory</A> fame, was there. The <A HREF="#soundSpectrograph">sound spectrograph</A> was invented there by H.K. Dunn. The first work by Homer Dudley and his colleagues on vocoders and speech communication was carried out there.  The list goes on.  <A HREF="#BSTJ">The Bell Systems Technical Journal</A> provides a major collection of papers on their research.
</P>

<A NAME = "beats">
<I>beats</I>
</A><BR>

When two pure tones fairly close together in frequency are sounded together, a listener hears the intensity waxing and waning at a rate that reflects the difference between the two tones precisely.  The effect is due to the successive cancellation and reinforcement of one tone by the other as they get in and out of step (in and out of <A HREF = "#phase">phase</A>).  Over part of the range where difference effects occur, many listeners hear a third tone of the appropriate pitch (i.e. the difference frequency).
</P>

<A NAME = "bilabial">
<I>bilabial</I>
</A><BR>

Of, or with, two lips (from the Latin).
</P>

<A NAME = "binaryDigit">
<I>binary digit</I>
</A><BR>

A representation of two possible states ("1" and "0") which may form part of the representation of a number in base 2 arithmetic.  See also <A HREF = "#bit">bit</A>.
</P>

<A NAME = "binaural">
<I>binaural</I>
</A><BR>

Pertaining to two ears.  See <A HREF = "#diotic">diotic</A> and <A HREF = "#dichotic">dichotic</A>.
</P>

<A NAME = "bit">
<I>bit</I>
</A><BR>

Strictly speaking, a unit of information.  In common parlance, a bit is a binary digit, which is a physical signal (typically in a computer memory, register, etc.).  It has been suggested that since the term bit (binary digit) is used in information theory as a fundamental unit of information measure, the term binit (binary digit) should be used for the physical signal.  The reason:  although a binit can theoretically transmit one bit of information, it can only do so under noiseless (and hence impossibly perfect) conditions.  See <A HREF="#information">information</A> and <A HREF="#redundancy">redundancy</A>.
</P>

<A NAME = "blackBox">
<I>black box</I>
</A><BR>

A device whose detailed internal construction is unknown, but which has some (usually desired) input-output relationship.  It originated during the second world war, due to the practice of painting aircraft electronic boxes black and ensuring that the insides were secret.
</P>

<A NAME = "boneConduction">
<I>bone conduction</I>
</A><BR>

The path of sound transmitted into the <A HREF = "#cochlea">cochlea</A> by vibration of the surrounding bone, however vibration of the bone is produced.
</P>


<A NAME = "bonyLabyrinth">
<I>bony labyrinth</I>
</A><BR>

The passages inside the temporal bone that contain the mechanical parts associated with hearing and balance.
</P>
see <A HREF = "#membranousLabyrinth">membranous labyrinth</A> and <A HREF = "#cochlea">cochlea</A><P>

<A NAME = "breathyVoice">
<I>breathy voice</I>
</A><BR>

During <A HREF = "#phonation">phonation</A>, the vocal folds vibrate from relatively open to relatively or completely closed.  If the folds are not completely closed, air will pass between the folds and inject random energy into the vocal tract, which adds noise or "breathiness" to the overall voicing energy.  The small gap that remains during closure is called a "glottal chink", and is characteristic of many female vocalisations.
</P>

<A NAME = "BSTJ">
<I>BSTJ</I>
</A><BR>

The Bell System Technical Journal.  An important research journal published by the <A HREF="#bell">Bell Labs</A>.  Volume 57, Number 6, Part 2 for July-August 1978 was the original technical publication on Unix, describing in a collection of papers work which started in 1969 when Ken Thompson started working on a cast-off PDP-7 computer and Unix was conceived.  Shannon's original work on <A HREF = "#informationTheory">information theory</A> was first published there.  An important source of original work on speech communication and procesing. See <A HREF = "#bell">Bell Telephone Laboratories</A>.
</P>

<A NAME = "buzzHissSwitching">
<I>buzz/hiss switching</I>
</A><BR>

That part of a speech bandwidth compression system concerned with detecting and implementing the change in excitation corresponding to the contrast between voicing and fricative noise.  In its simplest form a circuit is provided that detects voicing (not an easy or entirely solved problem) and switches the excitation of the synthesiser part to "buzz" (voiced excitation) or "hiss" (random excitation) accordingly.  Of course, such a simple system does not produce correct voiced fricatives.  See <A HREF = "#voicingDetection">voicing detection</A>.
</P>

<A NAME = "capacity">
<I>capacity</I>
</A><BR>

The rate at which information (bits) may be transmitted over a defined channel.  See also <A HREF = "#bit">bit</A>.
</P>

<A NAME = "CCRMA">
<I>CCRMA</I>
</A><BR>

Centre for Computer Research on Music and Acoustics, at Stanford University, original home of Perry Cook, Julius Smith and others.  Perry Cook is now at Princeton University.
</P>

<A NAME = "cellsOfClaudius">
<I>cells of Claudius</I>
</A><BR>

Cells capable of generating D-C potentials and lying on the <A HREF = "#basilarMembrane">basilar membrane</A> beyond the outer edge of the <A HREF = "#organOfCorti">organ of Corti</A>.
</P>

<A NAME = "centreClipping">
<I>centre clipping</I>
</A><BR>

see <A HREF = "#clippedSpeech">clipped speech</A>.
</P>

<A NAME = "cepstralAnalysis">
<I>cepstral analysis</I>
</A><BR>

If the logarithm of the spectral amplitudes produced by a Fourier analysis of a time series is, itself, treated as a time series (sic) and subjected to a second Fourier analysis, the envelope is broken up into the underlying fine structure (pitch-, or excitation-determined in the case of voiced speech) and broad envelope characteristics (formant-, or vocal-tract-filter-determined in the case of speech).  The terms of the new Fourier series are now called rahmonics (instead of harmonics), and the distribution of rhamonics is called a cepstrum which displays the signal in the quefrency domain.  Such analysis is very attractive for speech since, in theory, and to a large extent in practice, one can take the cepstrum, remove some component and then carry out an inverse transform back into the frequency domain.  Thus, for example, one can remove the effect of the fine structure on the spectrum and leave only the formant envelope.  At the same time, the presence of voicing, and its frequency, may be determined from the component removed.
</P>

<A NAME = "cepstrum">
<I>cepstrum</I>
</A><BR>

see <A HREF = "#cepstralAnalysis">cepstral analysis</A>.
</P>

<A NAME = "cerebellarVermis">
<I>cerebellar vermis</I>
</A><BR>

see <A HREF = "#auditoryPathways">auditory pathways</A>.
</P>

<A NAME = "channelVocoder">
<I>channel vocoder</I>
</A><BR>

see <A HREF = "#vocoder">vocoder</A>.
</P>

<A NAME = "clippedSpeech">
<I>clipped speech</I>
</A><BR>

a speech waveform makes amplitude excursions about some zero level as time progresses.  Removal of some of these in various amplitude bands is called "clipping."  Peak clipping refers to removal of the largest positive and negative amplitude bands (leaving flattened peaks and troughs not exceeding the largest bands remaining).  Centre clipping involves the removal of the centre bands of amplitude.  The following:
<CENTER>
<A HREF=images/clip.jpg TARGET=figs><BR>
<BR><IMG SRC=images/clipthum.jpg>
<BR>
<I>Peak and centre clipped speech</I>
</A>
</P>
</CENTER>

illustrates these concepts.  The amount of clipping is expressed in terms of the number of decibels by which the maximum amplitude is reduced:
clipping = 20 log (Ao/A) <A HREF = "#db">db</A>,
where the reference is A  (the original amplitude) and Ao (the clipped amplitude), but the number of decibels is kept positive by inverting the ratio to keep it greater than 1.  Clipping need not always be symmetric about the zero axis, though it usually is.  It introduces distortion which may or may not be audible.
</P>

<A NAME = "coarticulation">
<I>coarticulation</I>
</A><BR>

The effect of of speech sounds that are fairly close together upon each other.  The vowel at the beginning of "abbey" is not the same as the vowel at the beginning of "abu", because coarticulation works backwards as well as forwards, and affects more than just the immediate neighbours.  It is due at least in part to the anticipation of articulations to come, and the effects of articulations that are past upon the current posture, and hence upon the sounds that are produced at any given moment when speaking.
</P>

<A NAME = "cochlea">
<I>cochlea</I>
</A><BR>

That part of the bony labyrinth concerned with transducing mechanical pressure waves into nerve signals.  The active part is the <A HREF = "#basilarMembrane">basilar membrane</A> and associated structures.  Its name comes from the Latin meaning "snail" which is what the cochlea spiral resembles.  The whole structure is deeply embedded in the temporal bone and exceedingly difficult to work on, either surgically or experimentally.  Georg von Bekesy obtained a Nobel prize for his work on hearing, which involved exceedingly delicate dissection, and study of the cochlea and other related structures, by careful ingenious experiments.  The space inside the cochlear spiral is a fairly uniform tube of total diameter about 5 mm, 35 mm long, wound around the <A HREF = "#modiolus">modiolus</A> (or centre-post) in a decreasing spiral, from the start near the round and oval windows to the end where is found the helicotrema, making roughly 2 1/2 turns.  This space is divided longitudinally into two (the scala vestibuli and the scala tympani) by the cochlear partitionóitself a fluid filled duct containing among other things the <A HREF = "#organOfCorti">organ of Corti</A>.  The cochlear partition is filled with endolymph, while the scala are filled with perilymph, and are joined by the helicotrema. Sound pressure waves are transmitted into the scala vestibuli by the <A HREF = "#stapes">stapes</A> acting on the oval window.  The complex hydrodynamic and elastic behaviour of the fluids and structures as the pressure waves affect the fluids on both sides of the cochlear partition, lead to frequency discriminating nerve signals being generated in monotonic progression from low to high frequency along the <A HREF = "#basilarMembrane">basilar membrane</A> by the relative movement of the <A HREF="#tectorialMembrane">tectorial membrane</A> and the <A HREF="#hairCells">hair cells</A>.  The nerve pulses at any given frequency below about 3000 <A HREF="#hz"> hz</A> tend to occur in phase with the input sound pressure waveform, which is the basis of the <A HREF="#volleyTheory"> "VolleyTheory"</A> of pitch perception.

</P>

<CENTER>
<A HREF ="images/dh12.jpg" TARGET="new"><IMG SRC="images/dh12thum.jpg"><BR>
Cross-section of cochlea canal</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>
<A HREF = "images/dh13.jpg" TARGET="new"><IMG SRC="images/dh13thum.jpg"><BR>
Cross-section of Organ of Corti</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>
<A HREF = "images/dh16.jpg" TARGET="new"><IMG SRC="images/dh16thum.jpg"><BR>
Diagram of ear components</A><BR>
<FONT SIZE=1> (From <I>Experiments in Hearing</I>, by Georg von BÈkÈsy.<BR>© 1960 McGraw-Hill Book Company. Used with permission of McGraw-Hill Book Company.)</FONT>
</P>
<A HREF = "images/dh17.jpg" TARGET="new"><IMG SRC="images/dh17thum.jpg"><BR>
Figure 6: Cross-section of cochlea</A>.<BR>
<FONT SIZE=1> (From <I>Experiments in Hearing</I>, by Georg von BÈkÈsy.<BR>© 1960 McGraw-Hill Book Company. Used with permission of McGraw-Hill Book Company.)</FONT>

</CENTER>

<P>
<A NAME = "cochlearDuct">
<I>cochlear duct</I>
</A><BR>

The space inside the cochlear partition.  It is filled with fluid called endolymph.  See <A HREF = "#scalaMedia">scala media</A>.
</P>

<A NAME = "cochleaMicrophonics">
<I>cochlea microphonics</I>
</A><BR>

There are first order and second order cochlear microphonics which, physically, are varying electric potentials recorded from the <A HREF="#perilymph">perilymph</A> usually at the <A HREF="#roundWindow">round window</A>.  The first order microphonics have a form very like the pressure waveform of the input sound.  They are believed to have no role in hearing to originate from the <A HREF = "#organOfCorti">organ of Corti</A>.  The second order microphonics are associated with the operation of the hair cells which also generate the nerve pulses on which auditory sensation actually depends.
</P>

<A NAME = "cochlearNucleus">
<I>cochlear nucleus</I>
</A><BR>

see <A HREF = "#auditoryPathways">auditory pathways</A>.
</P>

<A NAME = "cochlearPartition">
<I>cochlear partition</I>
</A><BR>

see <A HREF = "#cochlea">cochlea</A>.
</P>

<A NAME = "cocktailPartyProblem">
<I>cocktail party problem</I>
</A><BR>

Idiomatic name (derived by analogy) for the problem faced when speech is masked by other speechóthe problem, in fact, faced by conversationalists at cocktail parties!  Humans have unexplained abilities in accomplishing this task, though some cues (common pitch structure, signal level, directional cues, lip reading cues, semantic context, voice quality) all undoubtedly play a part.  Listening to someone at a cocktail party, and listening to a recording taken of the same conversation are quite different tasks, the latter being much more difficult.
</P>

<A NAME = "coding">
<I>coding</I>
</A><BR>

The process of mapping one set of symbols onto another.  Coding may be carried out in order to obtain information security (encryption), to combat noise (by building in noise-resisting <A HREF = "#redundancy">redundancy</A>), or to maximise the rate of information transmission in a communication channel (by matching the characteristics of the source of information to the characteristics of a channel.  Morse code, invented by Samuel Morse long before Shannon derived his important results on the subject, provides an excellent match between English letter frequencies and a binary channel.
</P>

<A NAME = "compression">
<I>compression</I>
</A><BR>

A term applied to the process of removing <A HREF = "#redundancy">redundancy</A> from collections of information.  Such collections contain more physical data than are necessary to represent the information content (<A HREF="#information">information</A> in the sense developed by Shannon).  Reducing the physical data to the absolute minimum needed to represent the information results in maximum compression.  It has been estimated that the change between typical successive frames of video information can be represented by less than two <A HREF = "#bit">bits</A>.
</P>

<A NAME = "conductiveDeafness">
<I>conductive deafness</I>
</A><BR>

Deafness due to some defect in transmitting sound pressure waves from the air to the oval window.  See also nerve deafness.
</P>

<A NAME = "confusionMatrix">
<I>confusion matrix</I>
</A><BR>

A square matrix having rows (say) corresponding to stimuli and columns (say) to responses, showing, in each cell, a number which represents the probability of confusion, or number of confusions in a given test, occurring between the various stimulus choices when making responses, during a psychophysical experiment.  For example, a number of words may be presented to subjects under some condition of noise or distortion and listeners responses recorded. Mistakes will be made, and the pattern of confusion can be preserved as a confusion matrix.  In using such data it is vital to distinguish significant confusions from those due to chance.  The conditions of such an experiment are such as to cause mistaken responses.  It is only the systematic mistakes that are of interest.  Mistakes that are not due to some systematic effect will be distributed among possible responses on a purely chance basis.  The usual statistical techniques may be used to set levels of significance and detect systematic confusions that represent systematic effects, thereby increasing our understanding of speech perception or production.
</P>

<A NAME = "connotation">
<I>connotation</I>
</A><BR>

The associative or indirect meaning, as opposed to the direct meaningóimplying the attributes while denoting the subject.  Intensive as opposed to extensive indication of some set.  See also denotation.
</P>

<A NAME = "consonant">
<I>consonant</I>
</A><BR>
As opposed to vowel.  A stricter dichotomy applies between related contoids and vocoids.  Consonants (contoids) are distinguished from vowels (vocoids) by a higher degree of constriction in the vocal tract.
</P>

<A NAME = "context">
<I>context</I>
</A><BR>

The surroundings or co-occurring circumstances of an entity or situation. In language, a phoneme occurs in the context of other phonemes, during utterances, and the phonetic context (among other things) determines the particular allophone that occurs.  Allophones are different acoustic variations of the basic (abstract) phoneme category.  At a higher level, a word occurs in the context of other words.  If the word is not clearly heard, there will very likely be enough contextual redundancy to allow the word to be recognised by inference.
</P>

<A NAME = "contourSpectrogram">
<I>contour spectrogram</I>
</A><BR>

In a conventional spectrogram, the amount of energy at a given frequency and time is represented as grey-scale values which are not easily quantified. The apparatus may be modified by including a circuit which generates pips each time the marker output crosses one of a set of internally generated reference levels.  In the final analysis these extra dots join up to form contours of energy on the output just like the contours marking the height of the ground on a map.  The grey scale marking is still generated and the result is a highly readable, quantifiable energy display.
</P>

<A NAME = "contralateral">
<I>contralateral</I>
</A><BR>

The opposite side.
</P>

<A NAME = "coronalSection">
<I>coronal section</I>
</A><BR>

A section through an organism orthogonal to the axis running from head to tail.
</P>

<A NAME = "cortex">
<I>cortex</I>
</A><BR>

The outer layer or portion of an organóespecially the outer layer of the brain, which is newest in evolutionary terms.
</P>

<A NAME = "CortiGanglion">
<I>Corti ganglion</I>
</A><BR>

see <A HREF = "#auditoryPathways">auditory pathways</A>.
</P>

<A NAME = "cps">
<I>cps</I>
</A><BR>

Cycles per second.  Now termed herz, abbreviated hz.
</P>

<A NAME = "creakyVoice">
<I>creaky voice</I>
</A><BR>

See <A HREF = "#glottalFlap">glottal flap</A>.
</P>

<A NAME = "criticalBand">
<I>critical band</I>
</A><BR>

If a pure tone is just masked by white noise covering the entire audio spectrum, it is found that the <A HREF = "#bandwidth">bandwidth</A> of the noise may be progressively reduced, without affecting the <A HREF = "#masking">masking</A>, until it covers a band around the frequency of the pure tone of a certain critical width.  Further reduction of the white noise band, either raising the lower frequency limit, or lowering the upper limit, causes a tone that was just masked before to become audible.  This band is called the critical band.  It is thought (Broadbent, for example) that it may correspond to some kind of neurological "catchment area" on the <A HREF = "#basilarMembrane">basilar membrane</A>.  The width is around 50 hz at 100 hz, and increases to 1000 hz at 10,000hz.  Between 50 hz and 2,000 hz the critical band remains between 50 and 100 hz and then rises roughly as the logarithm of frequency.  Components of the noise outside the critical band do not contribute to the masking of the tone.  (This fact should not be allowed to confuse the issue when a tone is masked by noise outside the band (e.g., a high frequency tone may be masked by a low frequency tone with around a 40 <A HREF = "#db">db</A> threshold shift).
</P>

<A NAME = "crossTalk">
<I>cross-talk</I>
</A><BR>

Leakage of unwanted signals into a communication path from adjacent paths.  An extreme case occurs at <A HREF = "#cocktailPartyProblem">cocktail parties</A>, as far as speech is concerned, but (for example) cross-talk between telephone lines causes problems (the term "line" here includes the exchange).
</P>

<A NAME = "CSR">
<I>CSR</I>
</A><BR>

Continuous Speech Recognition.  <A HREF = "#automaticSpeechRecognition">Automatic Speech Recognition</A> in which the speaker speaks naturally, rather than trying to pause between words.
</P>

<A NAME = "cybernetics">
<I>cybernetics</I>
</A><BR>

A term originated and defined by Norbert Weiner as "the science of control and communication in animals and machines".  The term has been much degraded since then (there is a book entitled "The Psycho-Cybernetics of Sex" for instance), but it is still useful, with care.
</P>

<A NAME = "cyclesPerSecond">
<I>cycles per second</I>
</A><BR>

A term replaced by herz (hz).  The term is self-explanatory allowing that a "cycle" is some repeating progression of entities, operations or values.
</P>

<A NAME = "cyton">
<I>cyton</I>
</A><BR>

The body of a nerve cell as opposed to its various <A HREF = "#process">processes</A>.
</P>

<A NAME = "damageRisk">
<I>damage risk</I>
</A><BR>

In relation to hearing this refers to the likelihood that a given noise exposure will result in permanent hearing loss (NIPTSónoise induced permanent threshold shift).  The subject is complicated since exposure history, noise spectrum and other characteristics all contribute.  The likelihood of NIPTS is usually inferred from TTS (temporary <A HREF = "#threshold shift">threshold shift</A>) data measured in the laboratory.  In general, a noise that produces no TTS will produce no NIPTS, if exposure is limited to no more than eight hours per day, with a 16 hour rest period between exposures.
</P>

<A NAME = "db">
<I>db</I>
</A><BR>

See <A HREF="#decibel">decibel</A>.
</P>

<A NAME = "dbm">
<I>dbm</I>
</A><BR>

A decibel scale based on a reference value of 1 milliwatt in 600 ohms and used by broadcast engineers.  The equivalent voltage across a 600 ohm register is 0.775 volts-rms.  A steady tone is required for calibration.   Peak program meters habased on this reference, but calibration marks are not in dbm directly.  See <A HREF="#rms">rms</A>
</P>

<A NAME = "decibel">
<I>decibel</I>
</A><BR>

A dimensionless unit of power (energy) measurement, abbreviated to db, which expresses power as a ratio between the target power and some defined reference: thus, for some measured power W, db re. W<SUB>o</SUB>  = 10 log (W/W<SUB>o</SUB>) (taking logs to the base 10).
</P><P>
The constant "10" is a multiplier to convert the "bels" resulting from the application of the basic formula into decibels (1 bel equals 10 decibels).  The reference (in this case W<SUB>o</SUB>) must always be stated.  The standard for speech and noise is approximately 10<SUP>-12</SUP> watts/m2.  The reference or base is very often given as a pressure, rather than a power.  Thus the base for speech and noise in terms of pressure is  0.0002 dynes/cm  or 2 x 10<SUP>-5</SUP>  nt/m2, but translating this into power (energy) requires a knowledge of the air characteristics prevailing at the time.  This emphasises that any db comparisons based on pressures must be transmitted in a medium of identical acoustic impedance.  Under these assumptions, power being proportional to the square of pressure, the formula becomes:
db re p<SUB>o</SUB> = 10 log (p<SUP>2</SUP>/p<SUB>o</SUB><SUP>2</SUP>) = 20 log (p/p<SUB>o</SUB>).
Being logarithmic, the measure is well suited to quantifying the large range of intensities encountered in acoustic measurement.
</P>

<A NAME = "dectalk">
<I>DECtalk</I>
</A><BR>

The most popular and successful text-to-speech system over the last few decades, based on work by Jonathan Allen, Dennis Klatt, their colleagues and their students on MITalk at MIT.  The system uses a <A HREF="#formantSynthesiser">formant synthesiser</A>, togther with a dictionary, letter-to-sound rules and some grammatical analysis to convert ordinary text into spoken words.  There are a number of look-alikes and derivatives.  The work is well described in the book by Allen, Hunnicutt and Klatt: <I>From text to speech: the MITalk system</I>, published in 1987 by Cambridge University Press.
</P>

<A NAME = "dendrite">
<I>dendrite(s)</I>
</A><BR>

The input <A HREF = "#process">process(es)</A> of a nerve cell.
</P>

<A NAME = "denotation">
<I>denotation</I>
</A><BR>

The meaning of something by direct example.  Extensive as opposed to intensive definition.  See also <A HREF = "#connotation">connotation</A>.
</P>

<A NAME = "dental">
<I>dental</I>
</A><BR>

To do with teeth.  A dental <A HREF = "#fricative">fricative</A> would involve the teeth forming part of the constriction in the vocal tract that produced the <A HREF = "#fricative">fricative</A> noise.
</P>

<A NAME = "dichotic">
<I>dichotic</I>
</A><BR>

See <A HREF = "#monotic">monotic</A>.
</P>

<A NAME = "differenceLimen">
<I>difference limen</I>
</A><BR>

Limen is the Latin for threshold (plural limina).  The difference limen is that difference or change in the stimulus that is at the threshold of detectability (since it varies from moment to moment in a random manner, it is normally defined as the change or difference that will be detected in 50% of trials).  It is abbreviated to DL and is the same as the Just Noticeable Difference, or JND.  The Reiz Limen or RL is the absolute threshold of detectability, below which the stimulus is not noticed at all in 50 per cent of trials.  The TL or Terminal Limen is that limit beyond which the stimulus elicits pain.  The DL is not constant for a given dimension of stimulus, as might be supposed.  If we let DI stand for the DL at a given level of stimulus, then to a very close approximation:
DI/I = K (a constant) (Weber's Law)
where I is the intensity of the stimulus in the dimension concerned.  In words "a stimulus must be increased by some fixed percentage of its current value for the difference to be noticeable".  At 50 grams a 1 gram increment in a weight might just be noticed.  At 100 grams, approaching 2 grams would be necessary.
The following two figures show how the intensity and frequency difference thresholds vary with frequency and intensity of sounds.  Note that Weber's Law is effectively incorporated into a measure of the DL based on <A HREF="#decibel">decibels</A>, and does not hold exactly for speech frequncy and intensity.  See also <A HREF = "#methodOfAverageError">method of average error</A>, <A HREF = "#methodOfLimits">method of limits</A> and <A HREF = "#methodOfConstantStimuli">method of constant stimuli</A>
</P>
<CENTER>
<A HREF = "images/dh20.jpg" TARGET=figs><IMG SRC=images/dh20thum.jpg><BR>
<B>Differential intensity threshold <I>versus</I> frequency and intensity</B></A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>

<A HREF = "images/dh22.jpg" TARGET=figs><IMG SRC=images/dh22thum.jpg><BR>
<B>Differential frequency threshold <I>versus</I> frequency and intensity</B></A>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</CENTER>
</P>

<A NAME = "digramDigraph">
<I>digram/digraph</I>
</A><BR>

Two letters appearing together, in order.  A study of digram frequencies may be of value in automatic language processing, especially coding and <A HREF = "#compression">compression</A>.
</P>

<A NAME = "dimensionless">
<I>dimensionless</I>
</A><BR>

Physical units and quantities are normally associated with dimensions of mass (M), length (L) and time (T).  Thus acceleration has dimensions of L x T<SUP>-2</SUP>.  Some units, however, are dimensionless since they represent ratios of quantities of the same kinds, or mixtures of dimensions that cancel.  Thus any <A HREF = "#db">db</A> scale value is dimensionless, and a pure number, being a ratio of two quantities of the same type.
</P>

<A NAME = "diotic">
<I>diotic</I>
</A><BR>

See <A HREF = "#monotic">monotic</A>.
</P>

<A NAME = "diphone">
<I>diphone</I>
</A><BR>

A combination of two phones representing the instantiation of two successive speech postures.  Since the  interaction effects between the postures concerned are explicitly represented in diphone segments it is attractive as a basis both for synthesis and recognition on the theory that segmentation is carried out at places of minimum rate of spectral change, leading to a reliable algorithm for recognition, and simple assembly for synthesis.  The success (or lack of it) of the approach reflects the validity of the underlying assumptions which are largely unstated.  Thus synthesis based on assembling synthetically generated diphones, which have been carefully evaluated by relays of naive listeners, is relatively successful.  Recognition has enjoyed less success, because appropriate segmentation is uncertain and difficult.  Many "diphone" segments really comprise more than two segments because consonant clusters, for example, do not "obey the (unstated) rules".  Whilst, in principle, around 1600 diphones could represent all the posture combinations in English, in practice, at least 3000 are required, and adding more offers much scope for improvement because <A HREF = "#coarticulation">coarticulation</A> effects range over more than just two neighbouring speech sounds, so that at least 40<SUP>4</SUP> combinations of "tetraphones" (more than 2.5 million) might be considered necessary to account for all the phonetic level interactions, even ignoring other influences.
</P>

<A NAME = "diphthong">
<I>diphthong</I>
</A><BR>

A sound formed by time-sequential combination of two simple vowel articulations.  Diphthongs are frequently regarded as <A HREF = "#phoneme">phonemes</A> in their own right.  This is almost inevitable in view of the classical definition of <A HREF = "#phoneme">phoneme</A>, but is not too helpful for simple synthesis and recognition by machine.  Consonant clusters are not regarded as different phonemes (except for affricates), so why should vowel clusters be regarded as separate phonemes (the standard reply refers to their distributional characteristics and, for the affricates, to the problem of <A HREF = "#juncture">juncture</A>).  There may be considerable modification of the component sounds compared to their "normal" realisation (reduction and shortening).  Glides, which are very close to diphthongs are similarly considered phonemes in their own right, though they may be closely approximated by using segments of the related vowels, again shortened, and with diphthong-rate transitions.  However, the glides also represent a much greater obstruction of the oral cavity than the related vowel sounds (which also leads to a somewhat different spectral character) so that they are properly identified as contoids (i.e. loosely speaking as "consonants"), rather than diphthong combinations.  See <A HREF = "#triphthong">triphthong</A>.
</P>

<A NAME = "diplacusis">
<I>diplacusis</I>
</A><BR>

A disparity in <A HREF = "#pitch">pitch</A> perception between the two ears.  One common way of inducing diplacusis is by using a loud tone to fatigue one ear.  Judgements of pitch in the region of the fatiguing tone may then differ by as much as an octave between the two ears.  The condition can exist to some degree, in many subjects, as a natural state.  It may explain why unaccompanied folk singers conventionally stuff a finger in one ear whilst singing.
</P>

<A NAME = "distinctiveFeatures">
<I>distinctive features</I>
</A><BR>

Distinctive features are defined for speech sounds as binary distinctions between polar extremes of a quality, or between presence versus absence of some attribute of the sound.  It is an approach to speech segment description based upon Daniel Jones' idea of "minimal distinctions"óany lesser distinction between two sounds being incapable of distinguishing two words clearly.  Distinctive features are thus closely related to phonemes and subject to the same limitations.  Phonemes may be considered as concurrent bundles of distinctive features.  Problems arise with consonants.  (Preliminaries to Speech Analysis: the distinctive features and their correlates, by Jakobson, Fant and Halle, MIT Press 1969, is the original reference to this approach).
</P>

<A NAME = "DL">
<I>DL</I>
</A><BR>

See <A HREF = "#differenceLimen">difference limen</A>.
</P>

<A NAME = "DTW">
<I>DTW</I>
</A><BR>

See <A HREF = "#dynamicTimeWarping">Dynamic Time Warping</A>.
</P>


<A NAME = "dynamicTimeWarping">
<I>Dynamic Time Warping</I>
</A><BR>

A process  for normalising the time relationship between an unknown speech waveform and a reference waveform, carried out by reference to <A HREF = "#frequencyDomain">frequency domain</A> data.  If the corresponding parts of the varying speech signal for the reference and the unknown can be mapped onto each other, the problem of deciding whether they represent the same speech sounds is made much easier.  See also <A HREF = "#timeNormalisation">time normalisation</A>.
</P> 

<A NAME = "dyne">
<I>dyne</I>
</A><BR>

A unit of force, being the force required to give one gram of matter (a mass of 1 gram, that is) an acceleration of 1 cm/sec2.  It is now obsolete, having been replaced by the newton in the <A HREF = "#SI">SI</A> system of measurement.  One newton gives 1 kg mass acceleration of 1 m/sec2 and is thus 100,000 dynes.
</P>

<A NAME = "dynesCm2">
<I>dynes/cm<SUP>2</SUP></I>
</A><BR>

A measure of pressure.
1 dyne/square centimetre = 0.1 nt/square metre = 0.1 Pa/square metre = 1 microbar = 0.011 kg/square metre.
Because of the wide range of pressures involved in acoustics, a logarithmic function of power ratios is used -- the <A HREF = "#decibel">decibel</A> scale.  Because this scale is based on power (energy) ratios,  there must be an assumption of constant impedance in the transmission medium, when expressing pressure ratios, and the pressures must be squared, or the logarithm of the pressure ratio multiplied by 2, to convert to an energy ratio.  In air at 20 degrees Celsius and standard pressure, 1dyne/square centimetre is equivalent to a power or intensity of:
2.41 x 10-4 watts/square metre (i.e. about 1/4 of a milliwatt)
See <A HREF = "#acousticIntensity">acoustic intensity</A>, and <A HREF = "#decibel">decibel</A>.
</P>

<A NAME = "eEs">
<I>E/E's</I>
</A><BR>

Experimenter/experimenters.
</P>


<A NAME = "earDrum">
<I>ear drum</I>
</A><BR>

The membrane dividing the outer ear (external <A HREF = "#auditoryMeatus">auditory meatus</A>) from the <A HREF = "#middleEar">middle ear</A> -- the air -- filled cavity containing the <A HREF = "#ossicles">ossicles</A>.  The membrane has a shallow conical shape with the "handle" of the malleus ("hammer" or first ossicle) lying radially on the upper vertical radius, being hinged at the top edge.  The bottom edge of the membrane has a more or less pronounced fold, which frees the lower edge and allows the ear drum to rotate the malleus about its hinge.  As a result, the ear-drum acts as a piston in the sound transmission chain.  Impedance matching with the air is achieved by the mechnical arrangements created by the ossicles and also by the construction of the <A HREF = "#basilarMembrane">basilar membrane</A>/<A HREF = "#organOfCorti">organ of Corti</A>/tectorial membrane combination.  Figures 5 and 9 provide two views, one more diagrammatic than the other, of the ear mechanisms.  See also <A HREF = "#ossicles">ossicles</A> and <A HREF = "#tectorialMembrane">tectorial membrane</A>.
</P>

<CENTER>
<A HREF = "images/dh16.jpg" TARGET="new"><IMG SRC="images/dh16thum.jpg"><BR>
Sectional diagram of ear</A><BR>
<FONT SIZE=1> (From <I>Experiments in Hearing</I>, by Georg von BÈkÈsy.<BR>© 1960 McGraw-Hill Book Company. Used with permission of McGraw-Hill Book Company.)</FONT></P><P>
</CENTER>
</P>

<A NAME = "effectors">
<I>effectors</I>
</A><BR>

Organs (devices) for operating on the environment of an organism (or machine).
</P>

<A NAME = "efferent">
<I>efferent</I>
</A><BR>

Conducting outwards, towards the periphery.  Thus efferent nerves are those which carry impulses from the central nervous system to the <A HREF = "#effectors">effectors</A> (muscles and glands).
</P>

<A NAME = "enclitic">
<I>enclitic</I>
</A><BR>

See <A HREF = "#proclitic">proclitic</A>.
</P>

<A NAME = "endolymph">
<I>endolymph</I>
</A><BR>

Viscous fluid filling the cochlear partition (or duct).  See <A HREF = "#cochlea">cochlea</A><P>

<A NAME = "envelope">
<I>envelope</I>
</A><BR>

Roughly, the "shape" which contains components forming an object of interest.  In mathematical terms, it is similar to the convex hull, though will usually involve concave portions as well.  The real criterion is the amount of detail that the shape reveals.  Thus the envelope of a narrow band spectral section would approximate a wide band spectral section.  The envelope of a speech time-waveform would approximate the function obtained by joining all the positive peaks together and all the negative peaks, smoothing out any significant kinks.  The form of an envelope function depends quite a lot upon the smoothing function applied to the original function.
</P>

<A NAME = "ergonomics">
<I>ergonomics</I>
</A><BR>

From the Greek "ergon" work -- literally ergonomics means "the study of work", but is actually the British term for human factors engineering, which involves a study of the working conditions most suited to the many tasks an operator is asked to perform.
</P>

<A NAME = "esophagealSpeech">
<I>esophageal speech</I>
</A><BR>

see <A HREF = "#oesophageal speech">oesophageal speech</A>.
</P>

<A NAME = "eustachianTube">
<I>eustachian tube</I>
</A><BR>

A narrow canal which connects the <A HREF = "#middleEar">middle ear</A> to the throat.  It is normally closed in humans, except when swallowing or blowing.  In some creatures, notably the frog, a very wide short eustachian tube acts to make the <A HREF = "#earDrum">ear drum</A> act like a noise-cancelling microphone for self-generated sounds, with obvious benefits.
</P>

<A NAME = "externalAuditoryMeatus">
<I>external auditory meatus</I>
</A><BR>

See <A HREF = "#auditoryMeatus">auditory meatus</A><P>

<A NAME = "FH2">
<I>FH2</I>
</A><BR>

The frequency of a filter used in a Parametric Artificial Talker to simulate the spectrum of sibilant sounds (<A HREF = "#fricative">fricatives</A> -- such as those at the beginning of "sun" and "shun"), which are produced by forcing air through a narrow constriction located towards the front of the oral cavity.
</P>


<A NAME = "FxFo">
<I>Fx/Fo</I>
</A><BR>

The glottal vibration frequency or "larynx" frequemcy in speech, especially in synthetic speech.  Fo is strictly the fundamental frequency of a repetitive time-waveform.  The glottal vibration frequency is only quasi-periodic, but it is convenient to treat the glottal waveform as instantaneously periodic (sic) and talk about the fundamental frequency when analysing the frequency spectrum of speech.  See <A HREF = "#fundamentalFrequency">fundamental frequency</A><P>

<A NAME = "F1F2F3">
<I>F1/F2/F3</I>
</A><BR>

<A HREF = "#formant">Formant</A> 1/formant 2/formant 3.  As <A HREF = "#speechSynthesiser">speech synthesiser</A> parameters these represent the frequencies of the resonant peaks in the output spectrum.  There are higher formants (F4 ... Fn) but these are less important in making primary distinctions between speech sounds.  See <A HREF = "#formant">formant</A> and <A HREF = "#articulatorySynthesis">articulatory synthesis</A>.
</P>

<A NAME = "falsetto">
<I>falsetto</I>
</A><BR>

A mode of voicing (principally associated with male speech) in which an unnaturally high <A HREF = "#FxFo">Fo</A> is produced by setting up an abnormal mode of glottal vibration.
</P>

<A NAME = "FastFourierTransform">
<I>Fast Fourier Transform</I>
</A><BR>

A method devised by Cooley & Tukey (Mathematics of Computing 19, 297- 301, April 1965) for computing the coefficients of the discrete <A HREF = "#FourierTransform">Fourier transform</A> (the digital computer equivalent of the <A HREF = "#FourierTransform">Fourier transform</A>). The method is very much faster than previous algorithms and a considerable literature has developed on this special topic because of its importance in signal analysis, and the widespread use of computers in laboratories around the world.  See also <A HREF = "#linearPredictiveCoding">Linear Predictive Coding</A> (LPC).
</P>

<A NAME = "FFT">
<I>FFT</I>
</A><BR>

see <A HREF = "#FastFourierTransform">Fast Fourier Transform</A>.
</P>

<A NAME = "filter">
<I>filter</I>
</A><BR>

A transmission device of limited <A HREF = "#bandwidth">bandwidth</A>.
</P>

<A NAME = "foot">
<I>foot</I>
</A><BR>

Unit division of speech according to the occurrence of <A HREF = "#stress">stressed</A> syllables ("beats"), rather as music is divided into bars.  Each foot begins with a syllable bearing primary <A HREF = "#stress">stress</A>, and ends just before the following primary stress.
</P>

<A NAME = "formant">
<I>formant</I>
</A><BR>

A formant is a significant peak in the <A HREF = "#spectralEnvelope">spectral envelope</A> of the <A HREF = "#frequencySpectrum">frequency spectrum</A> characterising speech.  In general, it varies as a function of frequency against time.  The lowest three <A HREF = "#formant">formants</A> are considered adequate for good intelligibility, though in synthetic speech using a so-called "formant synthesiser" additional higher poles are required to compensate for the missing higher formants and the radiation impedance from the mouth.  Formants appear as dark bands in a broad-band <A HREF="#soundSpectrograph">spectrogram</A>.  See <A HREF = "#articulatorySynthesis">articulatory synthesis</A>.
</P>

<A NAME = "formantSynthesiser">
<I>formant synthesiser</I>
</A><BR>

A "terminal analogue" type of synthesiser which models speech in the acoustic domain by using filters to simulate the resonant behaviour of the vocal and nasal passages.  Pulsed or random energy is fed into the input to simulate the glottal pulses and/or aspiration, and additional random energy, shaped by filters is added to simulate <A GREF="#fricative">fricative</A> energy.  The intensities are controlled directly and the energy balances are not well represented.  Moreover, only the lowest three or four <QA HREF="#formant">formants </A> are properly represented and varied.  The resulting voice quality is less than natural, even discounting the difficulties of representing dynamic variation, timing and intonation accurately.  A "terminal analogue" is an input-output representation of the vocal tract behaviour rather than a model of its distributed acoustic properties.  It is contrasted with a transmission-line or acoustic tube model which directly models the acoustic properties of the vocal apparatus as a collection of tubes and energy balances.  The author was involved in the development of a synthesis system of the latter type.  The paper <A HREF="http://www.firethorne.com/papers/avios95.htm">"Real time articulatory speech synthesis by rules"</A> describes the system.
</P>

<A NAME = "formantTracking">
<I>formant tracking</I>
</A><BR>

The process of determining the frequency position of <A HREF = "#formant">formant</A> peaks automatically, especially the first three formant peaks.  It is not as easy as it might seem.  Decision criteria to distinguish significant peaks from insignificant peaks have to be set, and will be somewhat arbitrary; formants may come so close together that they form a single peak; and in some sounds the formant 1 peak may be considerably reduced in amplitude, or split by a nasal anti-resonance.  Furthermore, formant peak frequency values may change very quickly in just those places where accurate determination is most important (e.g. the transitions associated with a stop sound), yet one strategy for eliminating noise from a formant tracker output is to apply continuity criteria to the values obtained.  Clearly it would be useful to have a  mechanism capable of making a binary distinction between rapid and slow movement, adjusting the application of the continuity criteria appropriately.  The problem is one of many.  It is typical of the problems encountered in the automatic analysis of speech.
</P>

<A NAME = "FrequencyAnalysis">
<I>frequency analysis</I>
</A><BR>

An analysis of a time varying signal into its individual frequency components.  Also known as <A HREF = "#FourierAnalysis">Fourier analysis</A>.

<A NAME = "frequencySpectrum">
<I>frequency spectrum</I>
</A><BR>

The total range of frequencies needed to contain all components of the Fourier analysis of a sound.  The frequency domain description of a phenomenon is, loosely, called its (frequency) spectrum.  See also <A HREF = "#harmonics">harmonics</A>.
</P>

<A NAME = "FourierAnalysis">
<I>Fourier analysis</I>
</A><BR>

The decompostion of a complex waveform into sinusoidal components, and determination of the coefficients (amplitudes) and <A HREF="#phase">phase</A> of the various terms. This gives a Fourier series.  Any physically realisable waveform can be represented to arbitrary accuracy.  In dealing with non-repetitive waveforms there are problems.  An impulse produces a uniform spectral density function in place of a Fourier series.  For non-transient waveforms, a segment of the waveform is excised, and treated as repeated indefinitely.  This arbitrarily selects the collection of sinusoids that underlies the transform and, in particular, implies a non-existent fundamental.  In speech, which, strictly speaking, is non-repetitive, an excellent compromise is achieved by choosing the time-interval between successive glottal pulses as the unit for decomposition in this repeated segment fashion -- making the analysis so-called "pitch-synchronous".  Spectrograms computed on a pitch-synchronous basis show considerably more coherence than those computed on the basis of arbitrary segments.  Of-course, the method should be called glottal-pulse synchronous.  See <A HREF = "#pitch">pitch</A> and <A HREF = "#harmonics">harmonics</A>.
</P>

<A NAME = "FourierSeries">
<I>Fourier series</I>
</A><BR>

That which is produced by <A HREF = "#FourierAnalysis">Fourier analysis</A>.
</P>


<A NAME = "FourierTransform">
<I>Fourier transform</I>
</A><BR>

See <A HREF = "#FourierAnalysis">Fourier analysis</A>.
</P>

<A NAME = "frequency">
<I>frequency</I>
</A><BR>

The repetition rate of a repeating event.  The reciprocal of the time-interval between successive repetitions of the event, especially successive repetitions of a cyclic waveform.
</P>

<A NAME = "frequencyAnalysis">
<I>frequency analysis</I>
</A><BR>

See <A HREF = "#FourierAnalysis">Fourier analysis</A>.
</P>

<A NAME = "frequencyDomain">
<I>frequency domain</I>
</A><BR>

A description of a system or phenomenon based on frequency and phase measures.  See <A HREF = "#timeDomain">time domain</A>.
</P>

<A NAME = "frequencySpectrum">
<I>frequency spectrum</I>
</A><BR>

The range of frequencies allocated or considered or involved in dealing with frequency-related phenomena.
</P>

<A NAME = "fricative">
<I>fricative</I>
</A><BR>

A speech sound produced by forcing air through a narrow constriction, thereby generating noise due to air turbulence which is characteristic of the constrictionóso-called friction noise, and is somewhat shaped by any coupled cavities.  If the <A HREF = "#vocalFolds">vocal folds</A> are maintained in an open position during speech articulation, so that they do not vibrate and there is no voicing, then with constriction higher up in the vocal tract (say between tongue and palate) an unvoiced fricative results.  The acoustic correlates of an unvoiced fricative are: the offset and onset of voicing; early disappearance and late reappearance of <A HREF = "#F1F2F3">F1</A>; <A HREF = "#formant">formant</A> transitions; and a spectral energy distribution appropriate to the particular place of articulation.  If the <A HREF = "#vocalFolds">vocal folds</A> vibrate, then a voiced fricative results.  The friction energy of a voiced fricative is usually somewhat modulated in amplitude at the voicing frequency.  During the constrictive part of a voiced fricative there is a drop in the pitch frequency (one form of <A HREF = "#microIntonation">micro-intonation</A>) due to the supraglottal pressure increase caused by the vocal tract constriction.  See also <A HREF = "#voicedStop">voiced stop</A>.
</P>

<A NAME = "fundamentalFrequency">
<I>fundamental frequency</I>
</A><BR>

The lowest component frequency of a periodic waveform.  See <A HREF = "#FourierAnalysis">Fourier analysis</A>.
</P>

<A NAME = "glottalFlap">
<I>glottal flap</I>
</A><BR>

Excitation of the vocal tract by what are effectively isolated <A HREF = "#glottalPulse">glottal pulses</A>.  This happens typically at the ends of utterances by some male speakers, as the <A HREF = "#voicing">voicing</A> frequency (pitch) is allowed to fall dramatically.  It is similar to creaky voice, which occurs at the beginning as well as the end of utterances for many speakers of educated Southern British English ("RP" from "Received Prunciation", the accent formerly expected of professional radio announcers and journalists in Britain).  In both, the <A HREF = "#glottalRateFrequency">glottal rate</A> reaches abnormally low values, and the pulses are so well separated that they are easily seen as separate in a <A HREF = "#spectrogram">spectrogram</A>.
</P>

<A NAME = "glottalPulse">
<I>glottal pulse</I>
</A><BR>

The result of one cycle, opening and closing, of the glottis during voiced speech.  One cycle of the <A HREF = "#glottalWaveform">glottal waveform</A><P>

<A NAME = "glottalRateFrequency">
<I>glottal rate/frequency</I>
</A><BR>

The <A HREF="#frequency">frequency</A> of vibration of the <A HREF = "#vocalFolds">vocal folds</A> that surround the glottis.  Normally it is not constant, even for two successive cycles.
</P>

<A NAME = "glottalWaveform">
<I>glottal waveform</I>
</A><BR>

The volume velocity waveform of air passing through the <A HREF="#vocalFolds">vocal folds</A> (glottis) during voiced speech.  For normal speech, it approximates a triangular waveform at normal voicing effort.  See <A HREF = "#harmonics">harmonics</A>.
</P>

<A NAME = "glottis">
<I>glottis</I>
</A><BR>

The opening that varies from nothing, through a slit of increasing width, up to a wide open triangle formed between the vocal folds.  When the tension and position of the vocal folds is suitably adjusted, and air pressure applied from the lungs below, fairly regular puffs of air break through the lips of the vocal folds and provide so-called "voiced" excitation of the resonant cavities of the vocal apparatus. The <A HREF = "#vocalFolds">vocal folds</A> are located in the <A HREF="#larynx">larynx</A>.
</P>

<A NAME = "hairCells">
<I>hair cells</I>
</A><BR>

Cells lying in the <A HREF = "#organOfCorti">organ of Corti</A> which provide mechanical-to-electrical conversion of sound vibrations as rendered at the <A HREF="#tectorialMembrane">tectorial membrane</A>, and hence generate electrical activity in the auditory neurons (from which arise the fibres of the VIIIth cranial nerve, which also innervates the vestibular apparatus).  Each cell has small hair-like processes which are mechanically stimulated by lateral movements of the tectorial membrane, with respect to the <A HREF = "#organOfCorti">organ of Corti</A>.  These movements result from the displacement of the <A HREF = "#basilarMembrane">basilar membrane</A> caused by pressure waves transmitted into the <A HREF = "#cochlea">cochlea</A>.  It is of interest that the neurons of the auditory nerve, unlike most others, do not regenerate following injury to the processes, but generally die.  The hair cells innervated by such a neuron then also die.  It was shown in 1988 that the equivalent cells in chickens regenerate after damage (Science, June 1988, Corwin, University of Hawaii and Cotanche, Boston University).  This holds out hope that, one day, it may be possible to help or even cure humans with noise-induced <A HREF = "#hearingLoss">hearing loss</A>. See <A HREF = "Images/Concept/dh13.tiff">Figure 4</A> and <A HREF = "Images/Concept/dh14.tiff">Figure 10</A>.
</P>

<A NAME = "halfOctaveBandSpectrum">
<I>half octave band spectrum</I>
</A><BR>

See <A HREF = "#octaveBandSpectrum">octave band spectrum</A>.
</P>

<A NAME = "hardPalate">
<I>hard palate</I>
</A><BR>

The roof of the mouth (oral) cavity, which divides it from the nasal cavity.  It lies between the velum and the teeth.  See <A HREF = "#alveolarRidge">alveolar ridge</A>.
</P>

<A NAME = "harmonics">
<I>harmonics</I>
</A><BR>

When a note is played on a musical instrument, a sound is produced having a certain pitch, with overtones.  The pitch is closely related to the <A HREF = "#fundamentalFrequency">fundamental frequency</A> of vibration of the mechanism producing the note, while the overtones, comprising some selection of frequencies that are integer multiples of the fundamental, give the instrument its characteristic timbre.  Frequencies falling at integer multiples of the fundamental frequency are called harmonics and a <A HREF = "#FourierAnalysis">Fourier analysis</A> of a repetitive waveform effectively decomposes a complex waveform into the fundamental and its harmonics.  A triangular waveform (symmetrical linear rise and fall during the first and second half cycles respectively) contains all odd harmonics.  So does a square wave (being a differentiated triangular wave, this is a logical consequence).  A triangular wave is a special case.  Any waveform that has a rise and a fall in two linear sections comprising a full cycle will contain all harmonics except those that exactly divide both rise and fall periods an integer number of times (not necessarily the same for the two sections).  The glottal waveform is slightly assymetric and therefore contains most harmonics in the speech frequency range.  Missing or reduced harmonics can affect the output spectrum dramatically when convoluted with the filter function of the vocal tract.  The characteristic sound quality of a church bell results from the rather strange distribution of frequencies it produces which are by no means all harmonically related.  A tubular bell, which rings with a normal set of harmonics produces a much less interesting sound than a church bell.
</P>

<A NAME = "haskins">
<I>Haskins Laboratory</I>
</A><BR>

One of the two or three most important speech research laboratories in the world.  Much of the original work on elucidating speech cues was carried out there, and some of the first serious synthetic speech was produced there in the 1950s using a spectrogram playback apparatus known as <A HREF="#patternPlayback">Pattern Playback</A>.  Frank Cooper, Pierre Delattre, Alvin Liberman, Leigh Lisker and many others laid the ground-work for <A HREF="#speechSynthesisByRules">speech synthesis by rules</A>.  Their current research work is now available on <A HREF="http://www.haskins.yale.edu">their website</A>, replacing their well-known annual reports.
</P>

<A NAME = "hearingLoss">
<I>hearing loss</I>
</A><BR>

A shift in hearing <A HREF = "#sensationLevel">sensation level</A> and thus defined as:
		HL = 10 log (I/I<SUB>o</SUB>) <A HREF = "#db">db</A>
where I is the intensity (sound energy) at the <A HREF = "#thresholdOfHearing">threshold of hearing</A> for the patient, and I<SUB>o</SUB> is the intensity at the <A HREF = "#thresholdOfHearing">threshold of hearing</A> for normal persons (i.e. a statistical average over normal subjects).
</P>

<A NAME = "helicotrema">
<I>helicotrema</I>
</A><BR>

The sole connection between the <A HREF = "#scalaVestibuliScalaTympani">scala vestibuli</A> and the <A HREF = "#scalaVestibuliScalaTympani">scala  tympani</A>, the two spaces either side of the cochlear partition.  The helicotrema allows volume displacement of the fluid in the scala vestibuli by the oval window movements caused by the <A HREF = "#stapes">stapes</A> to be transmitted into the scala tympani, ultimately causing volume displacement of the round window.  This streaming of the fluid, induced by pressure waves on the <A HREF = "#earDrum">ear drum</A>, produces maximum pressure differences across the cochlear partition, and hence distortion of the <A HREF = "#basilarMembrane">basilar membrane</A>, at places corresponding to frequency of excitation (for sounds in the normal hearing range).  As a result, nerve pulses are generated in different fibres corresponding to different frequencies.  The pulses themselves are also grouped to reflect the frequency and phase of the sound.  The highest frequencies produce displacements of the <A HREF = "#basilarMembrane">basilar membrane</A> near the windows, the lowest near the helicotrema, the range being about 20 khz down to 20 hz.  See <A HREF = "#basilarMembrane">basilar membrane</A>, <A HREF = "#volleyTheory">volley theory</A>.
</P>

<A NAME = "HensensCells">
<I>Hensen's cells</I>
</A><BR>

Cells forming part of the <A HREF = "#organOfCorti">organ of Corti</A> and capable of generating steady voltage potentials.
</P>

<A NAME = "herz">
<I>herz</I>
</A><BR>

The unit for measuring repetition rate of a repeating event, especially a regularly cyclic event.  See <A HREF = "#cyclesPerSecond">cycles per second</A>.
</P>

<A NAME = "HiddenMarkovModel">
<I>Hidden Markov Model</I>
</A><BR>

A technique for recognising speech based on a specialised state machine in which the states represent the varying <A HREF = "#spectrum">spectrum</A> of the speech signal, classified into discrete categories, and linked by transitions.  Different <A HREF = "#segment">segments</A> of speech drive the HMM through different paths.  The path allows the segment or succession of segments and therefore the speech input to be identified.  There is a vast literature on the topic, as the algorithms for creating, searching, backtracking and so-on within the network, are very important to economy and success.  See also <A HREF="#segmentSynchronisation">segment synchronisation</A>
</P>

<A NAME = "highBackVowel">
<I>high back vowel</I>
</A><BR>

See <A HREF = "#vowel">vowel</A>.
</P>

<A NAME = "highFrontVowel">
<I>high front vowel</I>
</A><BR>

See <A HREF = "#vowel">vowel</A>.
</P>

<A NAME = "HMM">
<I>HMM</I>
</A><BR>

See <A HREF = "#HiddenMarkovModel">Hidden Markov Model</A>.
</P>

<A NAME = "hz">
<I>hz</I>
</A><BR>

An abbreviation for <A HREF = "#herz">herz</A>.
</P>

<A NAME = "IEEEASSP">
<I>IEEE-ASSP</I>
</A><BR>

Institute of Electrical & Electronics Engineers Transactions on Acoustics, Speech and Signal Processing.  An important source of current research with an emphasis on electronics and signal processing.  Published by the <A HREF="http://www.ieee.org">Institute of Electrical & Electronics Engineers</A>.
</P>

<A NAME = "IJMMSIJHCS">
<I>IJMMS/IJHCS</I>
</A><BR>

International Journal of Man-Machine Studies, now renamed the International Journal of Human-Computer Studies.  An important source of current research with emphasis on all aspects of systems involving Human-Computer Interaction (HCI, also known as Computer-Human Interaction -- CHI).  Published by <A HREF="http://www.harcourt.com">Academic Press</A>.
</P>


<A NAME = "incus">
<I>incus</I>
</A><BR>

See <A HREF = "#ossicles">ossicles</A>.
</P>

<A NAME = "inferiorColliculus">
<I>inferior colliculus</I>
</A><BR>

See <A HREF = "#auditoryPathways">auditory pathways</A>.
</P>


<A NAME = "information">
<I>information</I>
</A><BR>

Information is the subject of Claude Shannon's original theories on the transmission of information (<A HREF="#BSTJ"><I>Bell System Technical Journal</I></A> volume <B>27</B> 1948: A mathematical theory of communication, pages 379ó423 and 623ó656). Information is measured as the logarithm of the probability of a message.  In the case of logarithms to the base "2", the measure becomes a <I>bit</I> for <I>binary digit</I>.  Only when there is no noise, and the possibilities are equally likely, can a physical "1" or "0" convey one bit of information.  The problems of coding to deal with noise, or unequal message probabilities, are the province of information theory and coding theory.  See <A HREF="#redundancy">redundancy</A>.
</P>
 
<A NAME = "informationTheory">
<I>information theory</I>
</A><BR>

See <A HREF="#information">information</A> and <A HREF="#redundancy">redundancy</A>.
</P>


<A NAME = "innerEar">
<I>inner ear</I>
</A><BR>

The <A HREF = "#cochlea">cochlea</A> and all it contains.
</P>

<A NAME = "intelligibility">
<I>intelligibility</I>
</A><BR>

A sound may or may not be heard.  If it is heard, it is audible.  To be intelligible, it must be a speech signal, and the listener must recognise the nonsense syllable, word, sentence, or whatever that was sent to him. The threshold of intelligibility is some 10 to 15 <A HREF = "#decibel">decibels</A> higher than the threshold of audibility for the same speech under the same noise conditions (Hawkins & Stevens 1950).  Tests of intelligibility are used to evaluate communication systems and situations.  See <A HREF = "#articulationIndex">articulation index</A>.
</P>

<A NAME = "intensity">
<I>intensity</I>
</A><BR>

See <A HREF = "#acousticIntensity">acoustic intensity</A><P>

<A NAME = "intensitySpectrumLevel">
<I>intensity spectrum level</I>
</A><BR>

Is defined as the acoustic intensity per herz for the noise in question. If the band of frequency containing the noise is DF hz wide, then:<BR>
<SPACER TYPE=horizontal SIZE=40>ISL = 10 log(I/(I<SUB>o</SUB>.DF)) <A HREF = "#db">db</A> re. I<SUB>o</SUB> = IL - 10 log DF<BR>
where IL is the sound intensity level.  See also <A HREF = "#pressure spectrum level">pressure spectrum level</A>, <A HREF = "#acousticIntensity">acoustic intensity</A>, <A HREF = "#soundIntensityLevel">sound intensity level</A>, <A HREF = "#sound pressure level">sound pressure level</A> and <A HREF = "#decibel">decibel</A>.
</P>

<A NAME = "intervalScale">
<I>interval scale</I>
</A><BR>

See <A HREF = "#scales of measurement">scales of measurement</A>.
</P>

<A NAME = "intonation">
<I>intonation</I>
</A><BR>

Often referred to as the "tune" of an utterance.  The intonation pattern of an utterance is the time-pattern of pitch-variation during an utterance and serves several purposes at different levels.  At the segmental level, so-called "<A HREF = "#microIntonation">micro-intonation</A>" provides cues to constrictive postures (contoids) of the vocal apparatus because the voice <A HREF = "#pitch">pitch</A> (<A HREF = "#glottalRateFrequency">glottal frequency</A>) rises and falls as the pressure difference across the glottis varies as a result of the changes in <A HREF = "#supraglottal">supraglottal</A> airflow caused by the varying constrictions.  At the <A HREF = "#prosody">prosodic</A>  (<A HREF = "#suprasegmental">suprasegmental</A>) level, the intonation pattern gives clues to syllable, word, and sentence structure.  At the <A HREF = "#semantics">semantic</A> level intonation affects meaning.  It should not be thought that intonation achieves its effect in isolation, however, even though it would have a considerable effect under such conditions.  Of considerable importance is the precise relationship between the pitch movements and levels that make up the intonation pattern, and the segmental features, rhythm cues, and perceived loudness effects that run in parallel.  In tone languages (such as Chinese), intonation also directly affects the meanings of words, since words with different meanings may differ only in the tone applied.  See <A HREF = "#stress">stress</A>, <A HREF = "#prosody">prosody</A>, <A HREF = "#salient">salient</A> and <A HREF = "#prominence">prominence</A>.
</P>

<A NAME = "IPOAPR">
<I>IPO APR</I>
</A><BR>

The <A HREF="http://www.ipo.tue.nl">Institut voor Perceptie Onderzoek</A>, part of the Technical University of Eindhoven in the Netherlands.  The <A HREF="http://www.ipo.tue.nl">IPO</A> works on speech perception research, and computer-human interaction amongst other topics.  They have carried out seminal work on English intonation.
</P>

<A NAME = "ipsilateral">
<I>ipsilateral</I>
</A><BR>

The same side.
</P>

<A NAME = "isochrony">
<I>isochrony/isochronicity</I>
</A><BR>

The theory of isochrony states that different <A HREF = "#rhythmicUnits">rhythmic units</A> in speech (feetóAbercrombie/Halliday, rhythm unitsóJassem) have a tendency to be of equal duration regardless of the number of segments or syllables they contain.  Studies have shown that the ratio between the duration of the longest rhythmic unit to that of the shortest is as much as 6 or 7 to 1.  Therefore American linguists (e.g. Lehiste) consider that most of the perceived isochrony effect is just thatóa perception on the part of the listener, rather than an objective tendency towards equal duration.  A statistical study by the present author, with Jassem and Witten, showed that the tendency towards isochrony was one of the only three independent factors determining speech rhythm.  In setting the duration of segments, when modelling speech rhythm, the identity of segment accounted for about 45% of the variance in duration; whether the rhythmic unit was marked (tonic or final) accounted for about 15% of the variance in duration; and the segment durations then needed to be corrected by an inverse linear regression based on the number of segments in the rhythmic unit, accounting for about 10% of the variance in duration.  The effect was almost non-existent for proclitic syllables (anacruses in Jassem's terminology), and Jassem's theory therefore excludes anacruses in estimating rhythmic unit duration for purposes of testing and elaborating the theory. See <A HREF = "#salient">salient</A>, <A HREF = "#foot">foot</A>, <A HREF = "#rhythmicUnits">rhythmic units</A> and <A HREF = "#anacrusis">anacrusis</A>.
</P>


<A NAME = "IWR">
<I>IWR</I>
</A><BR>

Isolated Word Recognition.  See <A HREF = "#automaticSpeechRecognition">automatic speech recognition</A>.
</P>

<A NAME = "JASA">
<I>JASA</I>
</A><BR>

The Journal of the Acoustical Society of America.  A basic source of important work in speech, psycho-acoustics, and other topics. Published by the American Physical Society and extremely well indexed.
</P>

<A NAME = "JSHR">
<I>JSHR</I>
</A><BR>

The Journal of Speech and Hearing Research.  Published by the American Speech and Hearing Association.
</P>

<A NAME = "JVLVB">
<I>JVLVB</I>
</A><BR>

The Journal of Verbal Learning and Verbal Behaviour.  Published by Academic Press.
</P>

<A NAME = "juncture">
<I>juncture</I>
</A><BR>

The difference between the utterances "night rate" and "nitrate" is one of "juncture"ówhere does the word boundary fall.  Juncture is important.  Thus the affricates in spoken English might be regarded simply as two successive speech postures in a row: but, for reasons of <I>juncture</I> and for reasons of <I>distributional characteristics</I> they are treated as distinct single <A HREF = "#phoneme">phonemes</A>. For example, "my chip" has the word boundary before the combined sound of "t" followed by "sh", whilst in "might ship", the juncture is in the middle.  The combined "t-sh" sound is one of the English affricates.  Although the dynamic acoustic elements are spectrally quite similar to the related simpler phonemes, the <I>timing</I> of the elements is significantly different, and signals the position of the word boundary.
</P>

<A NAME = "KaySonagraf">
<I>Kay Sonagraf</I>
</A><BR>

See <A HREF = "#soundSpectrograph">sound spectrograph</A>.
</P>

<A NAME = "kinaestheticSense">
<I>kinaesthetic sense</I>
</A><BR>

A sense of the spatial relation and movement of its own frame possessed by a conscious entity.  See <A HREF = "#proprioception">proprioception</A>.
</P>

<A NAME = "labial">
<I>labial</I>
</A><BR>

To do with a lip, lip-like part, or labium.
</P>

<A NAME = "LandS">
<I>L &amp S</I>
</A><BR>

Language &amp Speech.  A useful reference.  Holme's early work on <A HREF = "#speechSynthesisByRules">speech-synthesis-by-rules</A> was published in this journal.  Published by Robert Draper, Teddington, UK.
</P>

<A NAME = "laryngograph">
<I>laryngograph</I>
</A><BR>

A device for determining the <A HREF = "#voicing">voicing</A> frequency (<A HREF = "#glottalRateFrequency">glottal rate/frequency</A>) with which a live talker is producing speech.  Direct measurement of the frequency of vibration of the vocal folds may be determined on a pulse to pulse basis in a variety of ways involving electrical impedance, light beam modulation and so on.  The original laryngograph worked  by placing non-invasive electrodes either side of the speaker's larynx (on the neck) and measuring the high frequency impedance (impedance to a signal at approximately 1Mhz).  Although the change in impedance as the vocal folds open and close is quite small, it is possible to pick up the varying contact of the folds, and convert successive intervals to a display of glottal pulses.  Ideally some decision criteria as to presence versus absence of voicing should be built in to suppress indications during periods deemed voiceless.  Setting up an adequate test of voicing versus no voicing is not as easy as it might seem.
</P>

<A NAME = "larynx">
<I>larynx</I>
</A><BR>

A cartiligenous upper chamber of the trachea (or wind-pipe) that may be felt from outside the throat as the Adam's apple.  The larynx contains the <A HREF = "#glottis">glottis</A> which is bounded by the <A HREF="#vocalFolds">vocal folds</A>. Enlargement of the larynx, and hence lengthening of the vocal folds, is a secondary sex characteristic for male humans, and is responsible for the voice "breaking" at puberty, and for the lower pitch of the mature male human voice.
</P>

<A NAME = "lateral">
<I>lateral</I>
</A><BR>

To or toward the side:  away from the mid-line.  Thus a lateral consonant is produced by constriction allowing airflow either side of the tongue.
</P>

<A NAME = "lateralLemniscateAndNucleus">
<I>lateral lemniscate and nucleus</I>
</A><BR>

See <A HREF = "#auditoryPathways">auditory pathways</A>.
</P>

<A NAME = "limen">
<I>limen</I>
</A><BR>

Threshold.  An absolute threshold marks the boundary between what is just below the level of sensation and what is just above.  A differential threshold marks the boundary between a difference in sensation which is just below detection, and a difference which is just detectable.  The threshold of feeling or sensation marks the boundary below which feeling or sensation does not occur and above which it does.  Thresholds represent <I>statistical</I> results from defined groups of subjects (usually subjects with normal sensory abilities).  Individual differences are important and any particular individual may differ significantly from the statistical averages.  Figures 6 and 8 illustrate absolutes thresholds of hearing, and differential frequency thresholds at various stimulus frequencies.
</P>
<CENTER>
<A HREF = "images/dh19.jpg" TARGET="new"><IMG SRC="images/dh19thum.jpg">
<BR>Thresholds of audibility and of feeling (various sources)</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>

<A HREF = "images/dh22.jpg" TARGET="new"><IMG SRC="images/dh22thum.jpg"><BR>
Differential frequency threshold against frequency & intensity of the standard tone</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>
</CENTER>

See also <A HREF = "#methodOfAverageError">method of average error</A>, <A HREF = "#methodOfLimits">method of limits</A> and <A HREF = "#methodOfConstantStimuli">method of constant stimuli</A><P>

<A NAME = "linearPredictiveCoding">
<I>linear predictive coding</I>
</A><BR>

A method of representing the sampled speech speech waveform based on the coefficients of a linear polynomial, abbreviated as LPC.  For analysis, the n polynomial coefficients are adjusted to predict the next sample value, based on n previous samples (hence the name).  The varying values of the coefficients can be used to represent the waveform at a lower information rate than would be required to represent the time waveform directly. Thus LPC is a form of compression of the speech waveform (in information rate terms).  The technique has become the method of choice for those involved in the analysis, and compression of speech waveforms.  An excellent summary of the methods for computing the coefficients, and the problems, appears in Witten, I.H. Principles of Computer Speech, Academic Press 1982).
</P>

<A NAME = "lingual">
<I>lingual</I>
</A><BR>

To do with the tongue.
</P>

<A NAME = "lipRounding">
<I>lip rounding</I>
</A><BR>

The formation of a more or less protruding cylindrical passage with the lips.
</P>

<A NAME = "localisation">
<I>localisation</I>
</A><BR>

The name given to the subjective identification of a point in auditory space as being the source of some sound stimulus.
</P>

<A NAME = "loudness">
<I>loudness</I>
</A><BR>

The subjective aural quality correlated with acoustic intensity.  It is not the same thing as <A HREF="#volume">volume</A>  In speech, duration and pitch movement have more effect in making syllables stand out than does simple acoustic intensity.  The two figures below illustrate some of the complexities of subjective <I>versus</I> objective scales of physical phenomena.  The apparent loudness of a tone varies with frequency as well as intensity, and is not linear with either.  The scale of sones is a subjective ratio scale (any tone with a loudness of 2 sones will sound, subjectively, twice as loud as any tone of 1 sone).  However, a tone having a loudness level of 40 phons will not sound twice as loud as one of 20 phons, but two tones with the same loudness level (say <I>n</I> phons) will sound equally aloud.  A 1000 herz tone with an acoustic intensity of 40 db is defined as having a loudness of 1 sone.  Are you sufficiently confused?  Try investigating colour perception!  The bottom line is that perception is relative and non-linear compared to our instrumental approaches to measurement. See also <A HREF = "#stress">stress</A>, <A HREF = "#prominence">prominence</A> and <A HREF = "#salient">salient</A>.
<BR>

</P>
<CENTER>
<A HREF = "images/dh23.jpg" TARGET="new"><IMG SRC="images/dh23thum.jpg">
<BR>Loudness as a function of intensity & frequency</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>\
</P>

<A HREF = "images/equloud.jpg" TARGET="new"><IMG SRC="images/equloudthum.jpg">
<BR>Loudness in phons plotted against frequency intensity in db re. 0.0002 dynes/cm<SUP>2</SUP> and watts/m<SUP>2</SUP></A><BR>
<FONT SIZE=1> (From <I>Theory and problems of acoustics</I>, by William W. Seto.<BR>© 1971 McGraw-Hill Book Company. Used with permission of McGraw-Hill Book Company.)</FONT>
</P>


</CENTER>
</P>

</P>

<A NAME = "lowBackVowel">
<I>low back vowel</I>
</A><BR>

See <A HREF = "#vowel">vowel</A>.
</P>

<A NAME = "lowFrontVowel">
<I>low front vowel</I>
</A><BR>

See <A HREF = "#vowel">vowel</A>.
</P>

<A NAME = "lpc">
<I>LPC</I>
</A><BR>

See <A HREF = "#linearPredictiveCoding">Linear Predictive Coding</A>.
</P>

<A NAME = "lumen">
<I>lumen</I>
</A><BR>

Interior space.
</P>

<A NAME = "malleus">
<I>malleus</I>
</A><BR>

See <A HREF = "#ossicles">ossicles</A>.
</P>

<A NAME = "masking">
<I>masking</I>
</A><BR>

Masking is the process by which one sound hides the occurrence of another.  The masking effect of one sound on another is measured as the increase in threshold (in <A HREF = "#decibel">decibels</A>) for the stimulus sound in the presence of the masking sound, compared to the stimulus threshold when the stimulus is presented alone.  For speech, masking effect is expressed in terms of the decreased <A HREF = "#intelligibility">intelligibility</A> of the speech in the presence of the masking sound, since it is this rather than its <A HREF = "#audibility">audibility</A>, that interests us in the case of speech.  The main facts about masking are:  (a) masking tends to be greater for tones close in frequency than tones widely separated; (b) low frequency tones mask high frequency tones fairly effectively but not vice versa;  (c) the rate at which masking increases with the intensity of a masking tone depends on the frequencies of the tones; (d) when the tones are applied to different ears, the small amount of masking that occurs is mainly due to transcranial conduction (50 <A HREF = "#db">db</A> attentuation) across the head; (e) speech is the most effective masking noise for speech; (f) with sufficiently intense high-frequency noise, sub-harmonics are produced by the ear mechanism which can cause (anomalous) "remote masking"; (g) the effect of a masking noise may be displaced in time by some few tens of milliseconds, both forwards and backwards.
<BR><SPACER TYPE=horizontal SIZE=40>
The complex relationships involved in masking are well illustrated by the following figure which shows the various sensations produced by the simultaneous presentation of two tones to one ear (<A HREF = "#monotic">monotic</A> presentation).
</P>
<CENTER>
 <A HREF = "images/dh10.jpg" TARGET="new"><IMG SRC="images/dh10thum.jpg"><BR>
Sensations produced by a two-component tone</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>
</CENTER>

<A NAME = "maximumTolerableLevel">
<I>maximum tolerable level</I>
</A><BR>

See <A HREF = "#thresholdOfPain">threshold of pain</A>.
</P>

<A NAME = "meatus">
<I>meatus</I>
</A><BR>

See <A HREF = "#auditoryMeatus">auditory meatus</A>.
</P>

<A NAME = "medial">
<I>medial</I>
</A><BR>

In or towards the middle.
</P>

<A NAME = "medialGeniculate">
<I>medial geniculate</I>
</A><BR>

See <A HREF = "#auditoryPathways">auditory pathways</A>.
</P>

<A NAME = "medulla">
<I>medulla</I>
</A><BR>

The brain stem.  Controls basic autonomic functions (respiration, heart- rate) and includes structures (the reticular formation, not to be confused with the <A HREF = "#reticularMembrane">reticular membrane</A>) which directs incoming sensory information.
</P>

<A NAME = "mel">
<I>mel</I>
</A><BR>

A unit of measurement on the <A HREF = "#subjective">subjective</A> scale of pitch.  1000 mels is defined as the (subjective) pitch of a tone which is 40 <A HREF = "#db">db</A> above the <A HREF = "#thresholdOfHearing">threshold of hearing</A> at 1000 hz i.e. the pitch of a 1000 hz tone at a sensation level of 40 <A HREF = "#db">db</A> is 1000 mels.  Since the scale is constructed by asking for judgements of pitch ratio (half and twice the pitch of a reference are fairly readily judged) the mel scale is a ratio scale.  The following plots of pitch <I>versus</I> frequency (using both linear and logartithmic scales):

<CENTER>
<A HREF = images/pvf.jpg TARGET=figs><BR>
<IMG SRC=images/pvfthum.jpg><BR>
Pitch in mels (subjective) plotted against both<BR>
linear and logarithmic frequency (objective) scales
</A>
</P>
</CENTER>

<P>
shows, the subjective pitch is not linear with frequency, nor even (as might be expected) with the logarithm of frequency. See <A HREF = "#scalesOfMeasurement">scales of measurement</A>.
</P>

<A NAME = "membranousLabyrinth">
<I>membranous labyrinth</I>
</A><BR>

This membranous structure is contained within, but distinct from, the bony labyrinth.  It includes the <A HREF = "#semicircularCanals">semicircular canals</A> (concerned with balance) and the <A HREF = "#cochlea">cochlea</A> (concerned with hearing).  The interconnected membranous labyrinth is attached at various places to the bony labyrinth, and is filled with a viscous fluid known as endolymph.
</P>

<A NAME = "metaLanguage">
<I>meta-language</I>
</A><BR>

A higher level of language.  A meta-language implies, of necessity, a language below it.  In order to talk about language one uses a meta-language.
</P>

<A NAME = "methodOfAverageError">
<I>method of average error</I>
</A><BR>

A psychophysical method for determining <A HREF = "#differenceLimen">Difference Limens</A> (thresholds) or DL's.  Also called the adjustment method.  In a psycho-physical experiment, subjects are asked to adjust a comparison stimulus with respect to some standard stimulus until the two appear equal.  There will be some average error in setting which, by normal statistical methods, may be computed.  It provides an estimate of the difference limen (the Just Noticeable Difference, or JND, between values at various levels for the stimulus concerned). See <A HREF="#methodOfConstantStimuli">method of constant stimuli</A> and <A HREF="#methodOfLimits">method of limits</A>.
</P>

<A NAME = "methodOfConstantStimuli">
<I>method of constant stimuli</I>
</A><BR>

A psychophysical method of determining <A HREF = "#limen">limens</A> (thresholds).  A set of stimuli is chosen and repeatedly presented in random order (hence "constant stimuli" really means constant set of stimuli).  Some stimuli will rarely be noticed, some almost always.  The value for which the probability of being noticed is 50 percent is taken as the absolute threshold, or RL (Riez Limen) for the stimulus concerned.  The following two figures show plots of absolute thresholds and of differential thresholds for sounds. See <A HREF="#methodOfAverageError">method of average error</A> and <A HREF="#methodOfLimits">method of limits</A>.

</P>

<CENTER>
<A HREF = "images/dh19.jpg" TARGET="new"><IMG SRC="images/dh19thum.jpg">
<BR>Thresholds of audibility and of feeling (various sources)</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>

<A HREF = "images/dh22.jpg" TARGET="new"><IMG SRC="images/dh22thum.jpg"><BR>
Differential frequency threshold against frequency & intensity of the standard tone</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>
</CENTER>


<A NAME = "methodOfLimits">
<I>method of limits</I>
</A><BR>

A psychophysical method of determining <A HREF = "#limen">limens</A> (thresholds).  For the absolute threshold the subject (S) is asked to respond on successive series of sequential stimulus intensities crossing the threshold region from above and below.  A doubtful response is interpreted as a change in sign from a previous positive or negative response.  Each series of stimulus intensities covers different range around the threshold and/or is of varying length, to avoid recognisable patterning.  By alternating the direction, errors of anticipation or habituation are (hopefully) minimised.  The results are averaged to give the required threshold.  To obtain the difference limen, two stimuli are presented on each trial , one being a standard and the other a varying stimulus.  S's are asked for three categories of response as before, with ascending and descending series of comparison stimuli, again avoiding strict patterning (otherwise S may count, for example).  In the neighbourhood where <A HREF = "#subjective">subjective</A> equality is obtained, S's will give "doubtful" judgements.  In each series the doubtful judgements defining the largest band of uncertainty are taken as the positive and negative difference limina (thresholds), even though the band also contains + and - judgements.  Averaging over many series gives the final results.  See <A HREF="#methodOfConstantStimuli">method of constant stimuli</A> and <A HREF="#methodOfAverageError">method of average error</A>.
</P>

<A NAME = "microbar">
<I>microbar</I>
</A><BR>

A measure of pressure.  One millionth of a <A HREF="#bar"> bar</A>.  See also <A HREF="#dynesCm2">dynes/cm2</A>.
</P>

<A NAME = "microIntonation">
<I>micro-intonation</I>
</A><BR>

Perturbations of voicing frequency during speech caused by the changing pressure applied across the <A HREF = "#glottis">glottis</A> resulting from consonantal constriction of the <A HREF = "#supraglottal">supraglottal</A> airflow.  For example, when a <A HREF = "#voicedStop">voiceless stop</A> is articulated, the voicing frequency falls as the closure occurs, because the pressure across the glottis (<A HREF = "#vocalFolds">vocal folds</A>) falls due to the build up in supraglottal pressure cause by the constriction, and then vibration ceases.  When the constriction is released, and voicing begins again, the voicing frequency initially starts out at an even higher level than it would have been without the stop articulation.  It then falls down to meet the continuation of the smooth track extrapolated from the time prior to stop articulation.  Such micro-intonation, which may involve as much as an octave shift in frequency, can give valuable cues to consonant identity.  Its presence or absence in <A HREF = "#speechSynthesisByRules">speech-synthesised-by-rules</A> can affect both <A HREF = "#naturalness">naturalness</A> and <A HREF = "#intelligibility">intelligibility</A>.
</P>

<A NAME = "middleEar">
<I>middle ear</I>
</A><BR>

That bony space between the outer ear, or <A HREF = "#auditoryMeatus">auditory meatus</A>, and the inner ear, or <A HREF = "#cochlea">cochlea</A>.  Into it face the ear drum (leading from the outer ear canal), the oval and round windows (leading into the cochlear duct), and the eustachian tube (which connects with the throat).  Thus the middle ear contains the ossicles and is connected to the throat by the eustachian tube.  The tiny muscles attached to the ossicles within this space are contained in bony shells to prevent them from being set into vibration by the air-vibrations.  Such induced vibrations would cause the generation of subharmonic frequencies of the input frequencies.  This one of many wonders of the evolved hearing mechanism.
</P>

<A NAME = "millibar">
<I>millibar</I>
</A><BR>

A measure of pressure.  One thousanth of a <A HREF="#bar"> bar</A>.  See also <A HREF="#dynesCm2">dynes/cm2</A>.
</P>

<A NAME = "minimalDistinction">
<I>minimal distinction</I>
</A><BR>

See <A HREF="#distinctiveFeatures">distinctive features</A>.
</P>

<A NAME = "MIT">
<I>MIT</I>
</A><BR>

<A HREF="http://www.mit.edu" TARGET="new">Massachusetts Institute of Technology</A>.  A hive of worthwile activity, including a chunk of speech  linguistics research.  Winograd did his thesis there, Chomsky is there, the <A HREF="http://rleweb.mit.edu" TARGET="new">Research Laboratory of Electronics</A> is there with the <A HREF="http://web.mit.edu/speech" TARGET="new">Speech Communication Group</A> (Jonathan Allenóoriginator of the MITalk text-to-speech system that underlies DECTalkóand Ken Stevens still hold Professorships in the RLE), the <A HREF="http://www.media.mit.edu" TARGET="new">Media Lab</A> is there, and the <A HREF="http://www.ai.mit.edu" TARGET="new">AI Lab</A> is there.  Situated in Cambridge, a suburb of Boston, Massachusetts.
</P>

<A NAME = "MITalk">
<I>MITalk</I>
</A><BR>

See <A HREF="#dectalk">DECtalk</A>.
</P>


<A NAME = "MITRLEQPR">
<I>MIT RLE QPR</I>
</A><BR>

The <A HREF="#MIT">Massachusetts Institute of Technology</A> (MIT), Research Laboratory of Electronics, Quarterly Progress Report.  Now superseded by their <A HREF="http://rleweb.mit.edu" TARGET="new">web-site</A>.  The speech communication group runs two sub sites. <A HREF="http://rleweb.mit.edu/groups/g-spe.htm" TARGET="new">One</A> is within the RLE tree, <A HREF="http://web.mit.edu/speech" TARGET="new">the other</A> is run separately, and contains a lot more detail.
</P>

<A NAME = "mnemonic">
<I>mnemonic</I>
</A><BR>

Strictly "mnemonic symbol".  An easily remembered symbol having a clear relationship to the thing symbolised.
</P>

<A NAME = "modiolus">
<I>modiolus</I>
</A><BR>

The central pillar around which the spiral of the <A HREF = "#cochlea">cochlea</A> winds, and from which the bony shelf, which supports the inner edge of the <A HREF = "#basilarMembrane">basilar membrane</A>, projects.  Lying opposite the bony shelf is the spiral ligament which tethers the tectorial membrane along its outer edge.  The modiolus gathers the nerves and blood vessels that serve the <A HREF = "#basilarMembrane">basilar membrane</A>. The nerves ultimately issue forth to become the auditory nerve, which forms part of the VIIIth Cranial nerve.
</P>

<A NAME = "monaural">
<I>monaural</I>
</A><BR>

Pertaining to one ear. <A HREF="#monotic"> Monotic</A>.
</P>

<A NAME = "monotic">
<I>monotic</I>
</A><BR>

A listening situation in which the stimulus is applied to only one ear.  Contrasts with dioticóthe same stimulus applied to two ears, and dichoticódifferent stimuli applied to each of the two ears.  Dichotic stimuli may differ in as little as one dimensionófor example, <A HREF = "#phase">phase</A>óor they may differ totally.
</P>

<A NAME = "moonIllusion">
<I>moon illusion</I>
</A><BR>

The illusion that the moon gets bigger as it nears the horizon.  Richard Gregory, of the Psychology Department at Cambridge University, England, investigated this and other perceptual illusions.  Although there is some slight refractive effect of the atmosphere that acts to increase the apparent size, the main effect is caused by incorrect (and unconscious) assumptions made by the brain in perceiving the world.  High in the sky, the moon is seen as a "usual" distance, say a few score feet, and is therefore interpreted as small. As it approaches the horizon, very distant objects are increasingly seen as nearer than the moon, which therefore is taken as increasingly far away.  Since it subtends the same physical angle it is interpreted as being very much larger than when high in the sky.  The <A HREF="#AmesRoom">Ames Room</A> effect of people changing size as they walk around an apprently normal room is equally due to incorrect assumptions. This emphasises an important aspect of perception: that perception is a highly learned skill based on what are normally unconcious assumptions about the world, and can go seriously wrong if the assumptions are violated.  It is important to remember this when addressing problems of speech recognition and synthesis.
</P>

<A NAME = "myelin">
<I>myelin</I>
</A><BR>

In the context of nerve transmission we have the myelin sheath, a fatty covering on nerve fibres, interrupted at the nodes of Ranvier.  It accelerates the rate of propagation of nerve pulses by projecting a current field from one node of Ranvier to the next, triggering firing at the next node well ahead of the time the normal depolarisation wave would take to get there.  All the faster mammalian nerves have this feature.  See <A HREF="#nerveCell">nerve cell</A>.
</P>

<A NAME = "nasal">
<I>nasal</I>
</A><BR>

To do with the nose.  In this context, for Educated Southern British English (RP), it applies to nasal consonantsóthose in which the oral cavity of the vocal tract is closed at some point, and the velum is open, allowing energy to be radiated from the nose with a spectrum determined by the response of the vocal apparatus. The spectrum is usually weakened, especially at higher frequencies, compared to normal voiced sounds, and usually shows a split first formant peak which therefore also is reduced in amplitude.  In other English dialects and in languages such as French, nasalised vowels also occur.  Traditional terminal analog <A HREF="#speechSynthesiser">speech synthesisers</A> do not do a very good job on nasal sounds.  A waveguide or <A HREF="#tubeModel"> tube model</A> synthesiser does a much better job because the acoustic simulation, especially the dynamics and the energy balances, are much more realistic.
</P>

<A NAME = "NATOalphabet">
<I>NATO alphabet</I>
</A><BR>

Speech is made up of combinations of sounds.  Such sounds tend to fall into groups which, though readily distinguished group by group, are readily confused within groups.  In addition, the words used to name letters of the alphabet have many of the same sounds in common.  As a result of such factors, using the common names of alphabet letters to spell words over a communication channel (where spelling is resorted to because the transmission conditions are too poor to simply speak the word) is worse than useless. The NATO alphabet is a set of letter names devised by the NATO military organization to provide a high degree of discriminability between the different names, and represents an improvement over the World War II alphabet and its predecessors.  It is widely used in civilian life as an aid to clear communication over noisy channels (for example, when using ship or aircraft radio).<BR>
<SPACER SIZE=30>First war<SPACER SIZE=30>ACK<BR>
<SPACER SIZE=30>Second war<SPACER SIZE=10>ABLE<BR>
<SPACER SIZE=30>NATO<SPACER SIZE=40>ALPHA<BR>
The full NATO alphabet is:<BR>
ALPHA, BRAVO, CHARLIE, DELTA, ECHO, FOXTROT, GOLF, HOTEL, INDIA, JULIET, KILO, LIMA, MIKE, NOVEMBER, OSCAR, PAPA, QUEBEC, ROMEO, SIERRA, TANGO, UNIFORM, VICTOR, WHISKEY, XRAY, YANKEE, ZULU.
</P>

<A NAME = "naturalness">
<I>naturalness</I>
</A><BR>

Naturalness, applied to speech, is a complex concept.  In general, synthetic-speech-by-rules is judged unnatural, and easily recognised as synthetic.  The problems include <I>at least</I> the following: inadequate acoustics (use of a true acoustic model such as the <A HREF = "#tubeModel">tube model</A> will help here); lack of detailed knowledge about how the synthesis parameters should be varied for different sounds in different contexts (and difficulty representing data and knowing which data are appropriate); inadequate models of rhythm; inadequate models of <A HREF = "#intonation">intonation</A>; lack of knowledge on how to tie particular intonation contours to specific <A HREF = "#segment">segment</A> structures; and inability to choose appropriate intonation contours for specific utterances due to lack of ability to understand utterances and the intent of the "speaker".  Truly natural <A HREF = "#speechSynthesisByRules">speech-synthesis-by-rules</A> ultimately requires that we solve the language understanding problem, and the subtleties of dialogue.  In the meantime, various tricks are used, including faking understanding based on punctuation and key words, and using standard phrase structures with predefined intonation.
</P>

<A NAME = "NCA">
<I>NCA</I>
</A><BR>

Noise Criteria (Acoustical).  A set of octave band spectrum curves and a descriptive table are used in combination to describe an acoustic environment ranging from "Very quiet office, suitable for large conferences" (NCA 20-30) to "Communication extremely difficult, telephone use unsatisfactory (NCA > 80)".  The octave band spectrum is plotted for the environment, and the number of the NCA curve which falls completely above the measured curve gives the index with which to enter the table.  (see Human Engineering Guide to Equipment Design, ed Chapanis et al., McGraw-Hill, 1963, pp. 186-188).
</P>

<A NAME = "nerveCell">
<I>nerve cell</I>
</A><BR>

The basic information processing unit in biological systems.  Consists of a cell body (or cyton) and various kinds of <A HREF = "#process">processes</A> or outgrowths.  Input processes are termed dendrites and the output process is the axon.  Inputs may occur directly to the <A HREF = "#cyton">cyton</A>.  Some processes (especially the axon) may reach considerable lengths.  The input to a nerve cell occurs via <A HREF = "#synapse">synapses</A> which, under certain conditions of electrochemical stimulation, may cause the cell to "fire".  When a cell fires a wave of electrical depolarization sweeps from the cell into the axon and away. The depolarisation, giving rise to the so-called "action potential", is a change in the permeability of the membrane of the axon tube which allows an ion exchange between the inside and outside.  Metabolic processes in the membrane subsequently restore the potential gradient ready for a new firing.  The firing path may be blocked by freezing and chemicals.  The processes involved are complicated and not fully understood.  After firing there is an absolute refractory period when the cell cannot fire (0.4 to 2 msec in mammalian fibres), and a longer period for which the threshold of firing is raised (relative refractory period).  The threshold of firing is a measure of the input activity needed to cause firing (via synapses on the dendrites and cell body).  Some synapses are inhibitory, some excitatory, and it is the algebraic sum of activity (spatial summation), integrated over time with a decay factor (temporal summation), that counts.  Some inputs come from specialised receptors (e.g. hair cells in the <A HREF = "#cochlea">cochlea</A>).  End processes of axons are specialised electrochemical transmitters (some end on synapses to other nerve cells, some end at muscles, etc).  The end process specialised to muscle operation is called a motor end-plate.  A given motor end plate, on receipt of a nerve pulse, releases a chemical (acetyl choline) which first causes the corresponding muscle fibre to contract and is then almost instantly broken down again by cholinesterase -- an enzyme specialised to this function. It is now known that there are scores of these so-called "neurotransmitter" enzymes, produced throughout the body and active throughout the body.  Dr. Candace Pert gives a very readable account in her book <I>"Molecules of Emotion"</I> along with much other material of interest.
<BR>
<SPACER TYPE=horizontal size =40>Some nerve gasses interfere with the breakdown process, causing the system to run amok.  Nerve pulses may be observed using very fine electrodes, suitably placed.  Their duration is roughly the same length as the absolute refractory period. Depolarisation of the axon may be initiated by locally applied electric currents far removed from the cell body.  In such cases a nerve pulse will travel in both directions away from the site of stimulation (i.e. both dromic (normal) and anti-dromic).  Speed of conduction in nerves innervating muscle fibres is conveniently measured by applying such stimulation, and then timing the duration of travel by noting the time of subsequent muscle action.  Speeds vary from 2 to 100 metres per second in mammalian fibres.  If an axon branches it had been thought that equal propagation down both branches was normal.  Current research suggests that varying threshold, refractory periods, and the like, may allow selective propagation at such branches, depending on the time history of stimulation to the parent cell which provides yet another form of information processing in the nervous system.  Cut nerves degenerate distally (away from the cell body).  Regeneration usually takes place, if the cell body is undamaged, but is slow (inches per year), and does not seem to occur for a number of important nerve types (the optic nerve, for example).
</P>

<A NAME = "nerveDeafness">
<I>nerve deafness</I>
</A><BR>

Deafness due to some defect in the nerves of the ear and <A HREF = "#auditoryPathways">auditory pathways</A>. See also conductive deafness.
<P>

<A NAME = "nerveFibres">
<I>nerve fibres</I>
</A><BR>

The <A HREF = "#axon">axons</A> and/or <A HREF = "#dendrite">dendrites</A> of <A HREF = "#nerveCell">nerve cell</A>.
</P>

<A NAME = "nervePulse">
<I>nerve pulse</I>
</A><BR>

See <A HREF = "#nerveCell">nerve cell</A>.
</P>

<A NAME = "neuron">
<I>neuron</I>
</A><BR>

See <A HREF = "#nerveCell">nerve cell</A>.
</P>

<A NAME = "newton">
<I>newton</I>
</A><BR>

The unit of force in the <A HREF = "#SI">SI</A> system of measurement, being the force required to give a mass of 1 kg an acceleration of  1 m/sec2  (gravity exerts a force of 9.81 nt on 1 kg).  See also <A HREF = "#dyne">dyne</A><P>

<A NAME = "noise">
<I>noise</I>
</A><BR>

A possible unwanted component of a signal.  An unwanted component that is correlated with the signal is strictly called distortion and may, in principle, be removed.  For this reason it is not considered noise.  Random perturbations present problems if the resulting noise <A HREF = "#frequencySpectrum">frequency spectrum</A> overlaps that of the signal since there is no easy way of removing it.  Shannon showed that information can be transmitted over a communication channel without error, up to a certain rate, by the using <A HREF="#redundancy">redundancy</A> in coding.
</P>

<A NAME = "noiseCancellingMicrophone">
<I>noise cancelling microphone</I>
</A><BR>

A microphone so constructed that the pick-up from the talker's environment has two components in anti-<A HREF = "#phase">phase</A> (thus tending to cancel out), while the talker's voice is picked mainly as one component and does not cancel.  Thus the signal-to-noise ratio entering the audio system is improved.  The necessary <A HREF = "#phase">phase</A> inverter (for audio waves) is rather difficult to make of adequately wide bandwidth, so that cancellation only occurs over part of the audio spectrum.  The device may also be thought of as a highly directional microphone (responding only to sounds from a particular direction).  The Shure AM-10 boom microphone is an excellent example of a noise cancelling microphone, and was for many years the <I>de facto</I> standard for speech recognition equipment.
</P>

<A NAME = "noiseShield">
<I>noise shield</I>
</A><BR>

A structure for providing a degree of acoustic insulation, usually between the environment and a talker's microphone (thus a pilot's oxygen mask is also designed as a noise shield and has microphone built in).
</P>

<A NAME = "nominalScale">
<I>nominal scale</I>
</A><BR>

See <A HREF = "#scalesOfMeasurement">scales of measurement</A>.
</P>

<A NAME = "nt">
<I>nt</I>
</A><BR>

See <A HREF = "#newton">newton</A>.
</P>

<A NAME = "oOs">
<I>O/O's</I>
</A><BR>

observer/observers.
</P>

<A NAME = "octaveBandSpectrum">
<I>octave band spectrum</I>
</A><BR>

A description of a sound <A HREF = "#spectrum">spectrum</A> based upon the <A HREF="#soundPressureLevel">Sound Pressure Level (SPL)</A> in a series of octave bands.  Other band spectra (half-octave, third octave, etc.) are also used. If a given octave band spectrum is converted to half-octave then, since the power in each band is halved, while the reference power defined for the bands will (in general) remain the same, there will be an apparent "drop" of 3<A HREF = "#db">db</A> in the overall spectrum (4.75 <A HREF = "#db">db</A> for 1/3 octave).  If the level is defined per unit bandwidth (hz) rather than some form of octave basis, the apparent drop in any given unit bandwidth will be that much greater, and in general there will be an increasing drop of 3 db/octave at increasing frequencies.  Of course, if, instead of merely converting, the measurements were retaken using narrower band analysis, the overall shape of the spectrum would very likely be considerably modified since there is no reason, in general, why the power in a given frequency band should be uniformly distributed throughout the band.  The so-called <A HREF = "#spectrumLevelCorrection">spectrum level correction</A> is based on the assumption of uniform distribution of acoustic energy.
</P>

<A NAME = "oesophageal speech">
<I>oesophageal speech</I>
</A><BR>

Speech in which vibration of the upper part of the walls of the oesophagus is substituted for vibration of the larynx.  The range and control of pitch is usually rather limited and the art requires considerable practice. When perfected, the speech can be indistinguishable (by naive listeners) from normal speech.  Air is taken into the stomach and released in a controlled manner through the constricted oesophagus, providing an alternative source of excitation for the vocal tract when the normal excitation source (the <A HREF = "#vocalFolds">vocal folds</A>) are unable to function.  The limited air capacity accounts for the characteristic pause pattern.
</P>

<A NAME = "ordinalScale">
<I>ordinal scale</I>
</A><BR>

see <A HREF = "#scalesOfMeasurement">scales of measurement</A>.
</P>

<A NAME = "organOfCorti">
<I>organ of Corti</I>
</A><BR>

A structure inside the <A HREF = "#cochlearPartition">cochlear partition</A>, resting on the <A HREF = "#basilarMembrane">basilar membrane</A>, and concerned with the generation of <A HREF = "#nervePulse">nerve pulses</A> corresponding to displacements of the <A HREF = "#basilarMembrane">basilar membrane</A>.  The pillars of Corti, which bend over and become the <A HREF = "#reticularMembrane">reticular membrane</A>, provide a basic framework and directly support the outer <A HREF = "#hairCells">hair cells</A>.  Along the inner edge (i.e. nearest to the <A HREF = "#modiolus">modiolus</A>) are more hair cells, though the outer are four times more numerous.  Other cells (Hensen's cells) also provide the body of the organ and generate steady voltage (D-C) potentials.  <A HREF = "#nervePulse">Nerve pulses</A> are generated by the hair cells as a result of lateral displacements of the overlying <A HREF = "#tectorialMembrane">tectorial membrane</A> acting in a direct mechanical fashion on the hairs on the hair cells.  These pulses propagate down the nerve fibres that are gathered to form the auditory nerve.  Apparently the outer hair cells provide a form of positive feedback gain control, and are capable of producing audible "squeals" associated with such feedback (that is, a person near the subject can hear noises coming out of the subject's ear).  The following figures show a cross-section of the cochlea channel and the organ of Corti.
</P>

<CENTER>
<A HREF="images/dh12.jpg" TARGET="new"><IMG SRC="images/dh12thum.jpg"><BR>
Cross-section of the cochlear duct</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>
<A HREF="images/dh13.jpg" TARGET="new"><IMG SRC="images/dh13thum.jpg"><BR>
Cross-section of the organ or Corti</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>
</CENTER>

<A NAME = "orthography">
<I>orthography</I>
</A><BR>

The written form of a language.  The written form cannot be spoken without a knowledge of the pronunciation, rhythm and intonation of the language.  English has a particularly complex relationship between spelling and pronunciation, as may be illustrated by a poem, <A HREF = "thechaos.htm" TARGET="new">The Chaos</A>, written at the turn of last century by a Dutchman (G.N. Trenite, alias Charivarius).
</P>

<A NAME = "ossicles">
<I>ossicles</I>
</A><BR>

These are small bones which form a jointed chain from the <A HREF = "#earDrum">ear-drum</A> to the oval window.  They provide impedance matching between the pressure waves in the air, and the required displacements of the perilymph in the inner ear, by intensifying the pressure fluctuations on a 22:1 ratio.  They comprise the malleus (hammer), incus (anvil), and stapes (stirrup).  These are names derived from Latin, based upon their shape, which is at first sight rather curious.  They are, in fact, a miracle of evolution, and not only provide near-perfect matching, but, because of their shape and attachment, protect the ear against overload.  Excess movements cause rotations instead of displacements, reducing the transmission of energy to the <A HREF = "#innerEar">inner ear</A>.  They also reduce bone-conducted input from the owner's vocal efforts, walking, etc. because the centre of gravity and the centre of rotation of the malleus coincide, eliminating transmission to the inner ear of vibrations of the head in a vertical direction.  The oval shape of the footplate of the stapes is another evolved adaptation connected with this kind of action.  The following diagram shows the ossicles in relation to other auditory structures.
</P>

<CENTER>
<A HREF = "images/dh16.jpg" TARGET="new"><IMG SRC="images/dh16thum.jpg"><BR>
Cross-section of the ear from the meatus to the cochlea.</A><BR>
<FONT SIZE=1> (From <I>Experiments in Hearing</I>, by Georg von BÈkÈsy.<BR>© 1960 McGraw-Hill Book Company. Used with permission of McGraw-Hill Book Company.)</FONT>
</P>
</CENTER>

<A NAME = "otoscope">
<I>otoscope</I>
</A><BR>

An optical device designed to allow an investigator to look at the <A HREF = "#earDrum">ear-drum</A> despite the curve in the <A HREF = "#meatus">meatus</A> (the passage between the external ear, or pinna, and the ear-drum).  A source of light contained within the instrument illuminates the view.
</P>


<A NAME = "ovalWindow">
<I>oval window</I>
</A><BR>

The membrane dividing the air space of the <A HREF = "#middleEar">middle ear</A> from the fluid filled scala vestibuli that forms part of the <A HREF = "#cochlea">cochlea</A>.  Attached to it by the annular ligament is the <A HREF = "#stapes">stapes</A>, the last <A HREF = "#ossicles">ossicle</A> (or small bone) in the pressure-intensifying, noise-eliminating chain of ossicles leading from the ear-drum.  Thus the oval window actually transmits the air-pressure-waves, suitably matched to the more resistant perilymph, into volume displacements of the perilymph within the <A HREF = "#scalaVestibuliScalaTympani">scala vestibuli</A>.  See also <A HREF = "#roundWindow">round window</A>.
</P>


<A NAME = "OVE">
<I>OVE</I>
</A><BR>

Pronounced "oovay" -- the <A HREF = "#resonanceAnalog">resonance analog</A> formant synthesiser constructed at the Royal Institute of Technology in Stockholm, <A HREF="http://www.speech.kth.se" TARGET="new">Dept. of Speech, Music and Hearing</A> (as it is now called) by C.G.M. Fant and his associates.  The original device has evolved with time, although the basic principles remain the same.  OVE I was very simple having only two <A HREF = "#formant">formants</A>, controlled manually by a pantograph-style arm moving in a notional <A HREF = "#F1F2F3">F1/F2</A> plane, with additional excitation controls.  Later versions implemented a full-scale parametric, cascaded formant, resonance analog synthesiser, similar to Lawrence's <A HREF = "#ParametricArtificialTalker">Parametric Artificial Talker</A> (PAT), with the addition of a fixed nasal resonance.  Synthesisers based on series or parallel formant filters are an approximation but only recently have the problems associated with controlling a distributed (waveguide/transmission-line/tube model), and providing the necessary data, been solved.  See also <A HREF"#tubeModel">tube model</A>.
<P>

<A NAME = "Pa">
<I>Pa</I>
</A><BR>

See <A HREF = "#Pascal">Pascal</A>.
</P>

<A NAME = "palatogram">
<I>palatogram</I>
</A><BR>

A graphic representation of the area of contact between the tongue and palate in making a speech sound.  The actual areas are mapped.  Moisture sensitive paper in the roof of the mouth has been used.  A plate (similar to the dental support for upper false teeth) with electrodes has also been used to determine places of contact between tongue and palate on a dynamic basis.
</P>

<A NAME = "ParametricArtificialTalker">
<I>Parametric Artificial Talker</I>
</A><BR>

The first successful <A HREF = "#resonanceAnalog">resonance analog</A> synthesiser built.  Invented by Walter Lawrence, of the Signals Research and Development Establishment of the British Government, in the very early 1950's.  <P>

<A NAME = "Pascal">
<I>Pascal</I>
</A><BR>

If taken as being a unit of <A HREF = "#SI">SI</A> measurement, it is a unit of pressure;<P>
		Pa = 1 newton/square metre<P>
Unlike most <A HREF = "#SI">SI</A> units named after famous men of science, this one retains the capital P.
</P>

<A NAME = "PAT">
<I>PAT</I>
</A><BR>

See <A HREF = "#ParametricArtificialTalker">Parametric Artificial Talker</A>.
</P>

<A NAME = "patternPlayback">
<I>pattern playback</I>
</A><BR>

A device built at the Haskins Laboratory in New York and used, during the 1950's, to determine many of the acoustic cues for speech perception, by systematic psychophysical experiments using synthetic speech stimuli generated by the device.  It was based upon an acoustic domain analog of speech.  It allowed representations of <A HREF = "#spectrogram">spectrograms</A> (including hand painted ones) to be turned back into speech.  A strip light source and cylindrical collimation system projected a line of light through a radius of a revolving wheel which turned at constant speed.  Tracks running circumferentially around the wheel, and suitably spaced to correspond to harmonics 120 hz apart when transferred to a spectrogram, contained markings which modulated the light passing through each track.  The rate of modulation for a given track corresponded to some harmonic of a 120 hz pitch rate, at the constant speed of rotation -- the tracks being arranged in appropriate order.  The split beam of light, after passing through this modulating arrangement, contained all necessary harmonics for synthetic speech.  Correct selection of those required for a given utterance was effected by reflection or transmission at the moving spectrogram.  Noise was simulated by random dotting of the spectrogram in the correct frequency region.  The light signals were all collected by a receiver (which therefore also summed the components), and the resulting electrical signal was converted to sound, reproducing the time waveform corresponding to the varying spectrum represented by the input spectrogram.  The original paper by Cooper, Liberman and Borst, "The interconversion of audible and visible patterns as a basis for research in the perception of speech", was published in the <I>Proceedings of the National Academy of Science</I>, <B>37</B>, pages 318-315, in 1951.
</P>

<A NAME = "peakClipping">
<I>peak clipping</I>
</A><BR>

See <A HREF = "#clippedSpeech">clipped speech</A>, <A HREF = "#PPM">PPM</A>, <A HREF = "#VUmeter">VU meter</A>.
</P>

<A NAME = "peakProgramMeter">
<I>Peak Program Meter</I>
</A><BR>

A meter that displays the varying local peak amplitude of an input sound, rather than providing an average power measurement, like the <A HREF="#VUmeter">VU meter</A>.  This is valuable as a basis for avoiding overload conditions for electronic audio equipment. A comparison is made under the <A HREF="#VUmeter">VU meter heading</A>.
</P>

<A NAME = "percept">
<I>percept</I>
</A><BR>

The mental construct built up from sensory data by a biological organism. That thing perceived by an organism.  The result of perception.  What is perceived may differ from what is really there because perception is based on many assumptions about the world which are quite unconscious, as far as the organism is concerned.  Hence many illusions (the <A HREF = "#AmesRoom">Ames room</A>, the <A HREF = "#moon illusion">moon illusion</A>, differing accounts of an accident by witnesses, etc.).  It is interesting to note that the predominant input to the human cortex is internally generated, rather than directly from sensory input (by a factor of possibly 40:1).
</P>

<A NAME = "perception">
<I>perception</I>
</A><BR>

See <A HREF = "#percept">percept</A>.
</P>

<A NAME = "perilymph">
<I>perilymph</I>
</A><BR>

Watery fluid filling the <A HREF = "#scalaVestibuliScalaTympani">scala vestibuli and scala tympani</A> in the <A HREF = "#cochlea">cochlea</A>.
</P>


<A NAME = "pharynx">
<I>pharynx</I>
</A><BR>

The oral pharynx is that part of the vocal tract between the <A HREF = "#glottis">glottis</A> and the <A HREF = "#velum">velum</A>.  The nasal pharynx continues part way along the nasal passage.
</P>

<A NAME = "pharyngeal">
<I>pharyngeal</I>
</A><BR>

To do with the <A HREF = "#pharynx">pharynx</A>.
</P>

<A NAME = "phase">
<I>phase</I>
</A><BR>

A repeating time waveform may be considered to start each cycle at a particular time.  The start time could be changed.  If it were changed enough, it would simply start at what would have been the beginning of the next cycle.  Phase is a description in circular measure of the relative time position of some new start time.  360 dgrees brings the signal back to its original phase.  A phase angle of 180 degrees corresponds to antiphaseócompletely out of phaseóin the case of a sin wave, for example.
</P>

<A NAME = "phon">
<I>phon</I>
</A><BR>

The unit of measurement on the <A HREF = "#subjective">subjective</A> <A HREF = "#intervalScale">interval scale</A> of <A HREF = "#loudness">loudness</A>.  The scale is identical to the <A HREF = "#sensationLevel">sensation level</A> at 1000 hz, by definition, and is constructed by asking subjects to adjust a second tone to be equal in loudness to a reference tone of known sensation level at 1000 hz.  A subjective <A HREF = "#ratio scale">ratio scale</A> has also been constructed, for which the unit is a sone.  At 1000 Hz a loudness of 1 sone is equivalent to a loudness level of 40 phons.  See <A HREF="#loudness">loudness</A>, <A HREF = #scalesOfMeasurement>scales of measurement</a> and <A HREF = #sone>sone</A>.<BR>

</P>
<CENTER>

<A HREF = "images/equloud.jpg" TARGET="new"><IMG SRC="images/equloudthum.jpg">
<BR>Loudness in phons plotted against frequency & intensity in db re. 0.0002 dynes/cm<SUP>2</SUP> and watts/m<SUP>2</SUP></A><BR>
<FONT SIZE=1> (From <I>Theory and problems of acoustics</I>, by William W. Seto.<BR>© 1971 McGraw-Hill Book Company. Used with permission of McGraw-Hill Book Company.)</FONT>
</P>



<A HREF = images/equvol.jpg TARGET=figs><IMG SRC=images/equvolthum.jpg><BR>
Loudness Level (phons) <I>versus</I> Frequency and Intensity (3-D figure)</A><BR>
<FONT SIZE=1>(From <I>Handbook of Experimental Psychology</I> edited by S.S. Stevens. 
Copyright<BR>
1951 John Wiley & Sons Inc.  Used with permission of John Wiley & Sons Inc.)
</FONT>
</P>

<A HREF = images/sonephon.jpg TARGET=figs><IMG SRC=images/sonephonthum.jpg><BR>

Relationship between sones (logarithmic scale) and phons (linear scale)</A><BR>
<FONT SIZE=1>(Note: At 1000 Hz a loudness of 1 sone is equivalent to a loudness level of 40 phons)</FONT></P>
</CENTER>

<A NAME = "phonation">
<I>phonation</I>
</A><BR>

<A HREF = "#voicing">Voicing</A>, when producing a voiced sound.
</P>

<A NAME = "phone">
<I>phone</I>
</A><BR>

The basic unit used in describing an utterance in a language.  A "Broad Transcription" requires considerable knowledge of the particular language, and produces a sequence of <A HREF="#phoneme">phonemes</A> (also called a phonemic transcription).  When trying to capture the details of a particular accent, or a new language, a so-called "Narrow Transcription" must be used, which uses phonetic symbols (defined by the International Phonetic Association or IPA) in a very general way, with a large number of modifiers (diacritics), to capture the fine detail of the utterance.  This is essential for a new language, and provides the basis for determining which differences between sounds distinguish words, and which are purely distributional (that is, depend only on the particular context). See <A HREF = "#segment">segment</A><P>

<A NAME = "phoneme">
<I>phoneme</I>
</A><BR>

The modern notion of phoneme, as distinct from <A HREF = "#phone">phone</A>, was almost certainly first formulated by Jan Baudouin de Courtenay, a Polish philologist, in the 1870's.  Daniel Jones (The Phoneme:  its Nature and Use.  Heffer: Cambridge, U.K. 3rd ed. 1967) gives an appendix on the history and meaning of the term.  Phonemes only have meaning in the context of a specified language.  The sounds produced in speaking the language are termed "phones". These sounds may be grouped into classes or categories, two sounds belonging in the same category if they never distinguish two words in the language. The categories are the phonemes of the language, each containing many different <I>allophones</I>, the variation between which is insignificant as far as meaning is concerned.  Thus phonemes are functionally defined abstract categories, and are specific to a language.  The concept, though useful for describing alphabetically oriented languages, and fundamental to modern phonology through phonetics, has been a snare and delusion to those attempting machine recognition of speech based on acoustic criteria.  The only practical way to view physical correlates of the phonemes is as postures of the vocal apparatus (speech postures, or articulatory targets).  However these target postures are by no means always achieved, let alone maintained and are affected by context. In listening to speech, the ideal targets (or speech postures) can only be inferred from acoustic data (perhaps with some additional visual cues).  They cannot, in general, be directly experienced.  The fact we humans can recognise spoken language provides an existence proof for the possibility of a process that converts the acoustic waveform into a representation of what the speaker intended the listener to understand.  This is the process that those trying to solve the speech understanding problem are trying to emulate. It requires the solution of some very hard problems including, ultimately, the representation and appropriate use of knowledge about the real world. See <A HREF = "#segmentation">segmentation</A>.
</P>

<A NAME = "phoneticallyBalancedWordList">
<I>phonetically balanced word list</I>
</A><BR>

A word list in which the <A HREF = "#phoneme">phoneme</A> of speech occur with the same relative frequency that they do in a random sample of speech.  There are certain conceptual difficulties here since these relative frequencies may vary with the kind of speech (reading aloud, conversation, etc.).
</P>

<A NAME = "physiologicalAnalog">
<I>physiological analog or model</I>
</A><BR>

Some construct that models the functional anatomy of a biological system. Thus a physiological analog <A HREF = "#speechSynthesiser">speech synthesiser</A> represents those parts of the vocal tract anatomy that contribute to the acoustic <A HREF = "#spectrum">spectrum</A> of speech in terms of function.  Such an analog may be varied dynamically by specifying physiological measurements (typically cross-sectional areas) at successive time instants.  Alternatively parameters related to the positions of the articulators may be used, and the corresponding sound specification may be computed on the basis of these parameters, by converting them to the required physical realities (cross-sectional areas).  Such a system would be an articulatory model.  See also <A HREF = "#articulatorySynthesis">articulatory synthesis</A>.
</P>

<A NAME = "pillarsOfCorti">
<I>pillars of Corti</I>
</A><BR>

See <A HREF = "#organOfCorti">organ of Corti</A>.
</P>

<A NAME = "pinna">
<I>pinna</I>
</A><BR>

Plural pinnae.  The pinnae are what we refer to when we say "My ears are cold".  Another term for pinna is auricle and, together with the <A HREF = "#auditoryMeatus">auditory meatus</A>, it forms a reversed megaphone to gather sounds to the ear-drum.  This, coupled with the baffle effect of the head, produces a frequency dependent increase in the pressure by the time the sound waves reach the ear-drum.  This is what an <A HREF = "#artificialEar">artificial ear</A> duplicates.  Maximum increase occurs around 2000 hz, producing a four- to five-fold pressure gain.
</P>

<A NAME = "pitch">
<I>pitch</I>
</A><BR>

Often used to mean the frequency of vibration of the <A HREF = "#vocalFolds">vocal folds</A> (glottal rate).  Strictly, it is the <A HREF = "#subjective">subjective</A> quality correlated with <A HREF = "#fundamentalFrequency">fundamental frequency</A> and measured in <A HREF = "#mel">mels</A> rather than <A HREF = "#herz">herz</A>.  The relationship between mels and hz is not linear, though it is monotonic.  The pitch of a 1000 hz tone 40 <A HREF = "#db">db</A> above threshold is defined as 1000 mels.
</P>

<A NAME = "pitchDetection">
<I>pitch detection</I>
</A><BR>

The process of determining, by machine, the <A HREF = "#glottalRateFrequency">glottal rate/frequency</A> (voicing frequency) of speech.  This is not, strictly, the same as <A HREF = "#pitch">pitch</A>, which is a subjective quality related to the voicing <A HREF="#frequency">frequency</A>. See <A HREF = "#voicingDetection">voicing detection</A>.
</P>

<A NAME = "pitchSynchronousAnalysis">
<I>pitch synchronous analysis</I>
</A><BR>

See <A HREF = "#FourierAnalysis">Fourier analysis</A><P>

<A NAME = "placeTheoriesOfHearing">
<I>place theories of hearing</I>
</A><BR>

Place theories of hearing assert that (perceived) <A HREF = "#pitch">pitch</A> depends on the place of maximal stimulation along the <A HREF = "#basilarMembrane">basilar membrane</A>.  By contrast the <A HREF = "#volleyTheory">volley theory</A> (somewhat related to the <A HREF = "#telephoneTheoryOfHearing">telephone theory</A> says that pitch is perceived by a time analysis of the pattern of <A HREF = "#nervePulse">nerve pulses</A> arriving from the <A HREF = "#basilarMembrane">basilar membrane</A>.  Both mechanisms are undoubtedly operative, with volley theory predominant below 1000 hz and place theory above.
</P>

<A NAME = "PPM">
<I>PPM</I>
</A><BR>

Peak program meter.  See also <A HREF = "#VUmeter">VU meter</A>.
</P>

<A NAME = "pragmatics">
<I>pragmatics</I>
</A><BR>

The study of the relation of signs to users.  The study of meaning, or connotation, at a higher level than the <A HREF = "#semantics">semantic</A>; to do with the situational and practical aspects of meaning, involving the associations of particular experiences in the world, as opposed to formal dictionary meanings.  Milner's <I>semiotic model</I> comprises three levels: symbols (syntax); semantics; and pragmatics (C.D. Milner: Science of Symbols in <I>International Encyclopaedia of Applied Science</I>).  See <A HREF="#syntactics">syntactics</A>, and <A HREF="#semiotics">semiotics</A>.
</P>

<A NAME = "presbycusis">
<I>presbycusis</I>
</A><BR>

Progressive deafness, especially in the higher frequency regions (say above 3000 hz), caused by increasing age.  It is possible that presbycusis is at least in part avoidable noise-induced <A HREF = "#hearingLoss">hearing loss</A>.
</P>

<CENTER>
<A HREF = "images/presbycusis.jpg" TARGET="new"><IMG SRC="images/presbythum.jpg"><BR>
Hearing loss plotted by age</A>.<BR>
(Hearing loss is plotted for each age as decibels loss<BR>
compared to a normal twenty-year-old)
</P>
</CENTER>

<A NAME = "pressure spectrum level">
<I>pressure spectrum level</I>
</A><BR>

Is defined as the sound pressure level per hz, for the noise in question. If the band of frequency containing the noise is DF wide, then:<BR>
<SPACER TYPE=horizontal SIZE=40>PSL = SPL - 10 log DF          (why not 20 log DF?)<BR>
See also <A HREF = "#intensitySpectrumLevel">intensity spectrum level</A>.
</P>

<A NAME = "pretonic">
<I>pretonic</I>
</A><BR>

See <A HREF = "#tonic">tonic</A>.
</P>

<A NAME = "probeMicrophone">
<I>probe microphone</I>
</A><BR>

A microphone designed to investigate sound pressure waves in restricted areas. The active area of the microphone (that part which is affected by incident sound waves) is made as small as possible and the microphone is constructed to allow the active area to be placed in difficult-to-get-at places.
</P>

<A NAME = "process">
<I>process (of nerve)</I>
</A><BR>

A thread, filament, fibre or what have you that extends from a <A HREF = "#nerveCell">nerve cell</A> body.
</P>

<A NAME = "proclitic">
<I>proclitic</I>
</A><BR>

"Proclitic" is a term applied in the context of grammatical structure.  In relating syllables to rhythmic structure in spoken language, some syllables are grammatically related to those preceding and some to those that follow.  The former are called "enclitic" syllables, the latter "proclitic"syllables.  Jassem regards proclitic syllables as analagous to <A HREF = "#anacrusis">anacruses</A> in music, and does not include them in the basic units of his analysis of <A HREF = "#rhythmicUnits">rhythmic units</A> (which he calls "rhythm units").
</P>

<A NAME = "prominence">
<I>prominence</I>
</A><BR>

In speech this refers to the degree to which a syllable or sound <A HREF = "#subjective">subjectively</A> stands out from its neighbours.  It is related to four factors, according to Daniel Jones:  basic sound (or syllable nucleus) quality, quantity (duration), <A HREF = "#stress">stress</A> (<A HREF = "#acousticIntensity">acoustic intensity</A>), and <A HREF = "#intonation">intonation</A> (pattern of change of glottal vibration frequency).  Instrumental and psychophysical approaches confirm the importance of these factors and rank stress, quantity and intonation in that order as having <I>increasing</I> effect on prominence.  Prominence is a subjective effect on the listener as opposed to stress (one correlate of prominence) which is a subjective activity on the part of the speaker.  See also <A HREF = "#salient">salient</A> and <A HREF = "#syllable">syllable</A>.
</P>

<A NAME = "proprioception">
<I>proprioception</I>
</A><BR>

The feeding-back of information concerning the state of joints, muscles etc. in a biological system to the parts of the nervous system concerned with body image.  It is proprioception that underlies much of our kinaesthetic sense, or sense of where the various parts of our body are in relation to one another, and absolutely.  There are special receptors (muscle spindle organs, receptors on tendons, etc.) which transmit specific proprioceptive feedback.
</P>

<A NAME = "prosody">
<I>prosody</I>
</A><BR>

Slightly tricky to define.  Basically <A HREF = "#suprasegmental">suprasegmental</A> (at a higher level than the segmental) but, broadly (following Lehisteó<I>Supra-segmentals</I>: MIT Press 1970)óthose aspects of speech that are other than segmental in character, in the sense that they overlay the segmental aspects and form patterns extending in time, or intensify phonetic factors already present to a lesser degree.  Thus, taking <A HREF = "#pitch">pitch</A>, <A HREF = "#stress">stress</A> and (time) quantity as the prosodic features of English, it is seen that while voicing is an inherent featureóidentifiable at a moment in time, <A HREF="#glottalRateFrequency">glottal rate</A> is an overlaid function.  The <A HREF = "#fundamentalFrequency">fundamental frequency</A> serves both to identify a <A HREF = "#segment">segment</A> as voiced and to contribute toward the <A HREF = "#intonation">intonation</A> pattern of the utterance.  Each segment requires some duration, but rhythm is the formation of time patterns by the manipulation and concatenation of inherent duration, "beats" occurring on strongly stressed (and therefore <A HREF = "#salient">salient</A> or <A HREF = "#prominence">prominent</A>) syllables.  Each segment has some inherent <A HREF = "#subjective">subjective</A> "<A HREF = "#loudness">loudness</A>" but, though this is mediated by a combination of intensity, duration and pitch variation, <A HREF="#prominence">prominence</A> is a result of re-inforcing or diminishing qualities already present, on a contrastive basis, in time.
</P>

<A NAME = "psychologicalSet">
<I>psychological "set"</I>
</A><BR>

The expectation a person has which tends to influence his or her perception of a situation or stimulus.
</P>


<A NAME = "puffScreen">
<I>puff screen</I>
</A><BR>

A device, often a plastic foam cap, attached to a microphone to prevent the unpleasant acoustic effects (and possible damage) caused by puffs of air from the talker's performance striking the sensitive parts of the microphone. A foam cap also protects the microphone against moisture from the breath and against drops of saliva.
</P>

<A NAME = "quefrency">
<I>quefrency</I>
</A><BR>

See <A HREF = "#cepstralAnalysis">cepstral analysis</A>.
</P>

<A NAME = "ratio scale">
<I>ratio scale</I>
</A><BR>

See <A HREF = "#scales of measurement">scales of measurement</A>.
</P>

<A NAME = "receptors">
<I>receptors</I>
</A><BR>

Organs (devices) for accepting input from the environment.  Sensory data receivers.
</P>

<A NAME = "redundancy">
<I>redundancy</I>
</A><BR>

The property a structure (physical, informational, ...) has when there are components which duplicate the role of other components in any way.  If a component is completely redundant, it may be removed without loss of integrity to the structure.  In physical structures, this provides protection from collapse due to damage.  In an information structure, components of the "message" may be lost while still retaining all the information.  In information theoretic terms redundancy is expressed in terms of the conditional probabilities of joint events.  A message with redundancy has more physical symbols than are necessary to represent the formal information content and will still convey the whole original information correctly even if somewhat damaged by noise.  Claude Shannon (<I>Bell System Technical J.</I> 1948: "A mathematical theory of communication") showed that it is possible to transmit information without error, even in the presence of <A HREF="#noise">noise</A>, provided the rate of information transmission does not exceed the well-defined <I>capacity</I> of the channel.  The trick is possible because of the use of redundancy, and the work spawned a major research field concerned with coding theory for purposes of error-free transmission of information.  See <A HREF="#information">information</A>.
</P>

<A NAME = "refractoryPeriod">
<I>refractory period</I>
</A><BR>

See <A HREF = "#nerveCell">nerve cell</A>.
</P>

<A NAME = "ReissnersMembrane">
<I>Reissner's membrane</I>
</A><BR>

An insulating membrane forming one wall of the triangular duct that partitions the spiral space inside the <A HREF = "#cochlea">cochlea</A> into two parts, (the <A HREF = "#scalaVestibuliScalaTympani">scala vestibuli and scala tympani</A>), which are joined only at the helicotrema.  The other walls are the <A HREF = "#basilarMembrane">basilar membrane</A> and the outer shell of the <A HREF = "#cochlea">cochlea</A>.
</P>

<A NAME = "Reiz">
<I>Reiz</I>
</A><BR>

See <A HREF = "#differenceLimen">difference limen</A>.
</P>

<A NAME = "resonanceAnalog">
<I>resonance analog</I>
</A><BR>

An <A HREF = "#acousticDomain">acoustic domain</A> model of speech production based upon the known resonant behaviour of the vocal apparatus and the known characteristic of the exciting source.  In producing synthetic speech, parameters describing the varying resonances and excitation function are supplied to a set of filters (in cascade, or in parallel), which reproduces the speech wave on the basis of the controlling parameters, and the constraints inherent in the embodiment of the filter model.  Lawrence's PAT (Parametric Artificial Talker) is a typical cascade resonance analog synthesiser.  Holmes used a parallel filter model.  The advantage of the former was that fewer controls were required, because the amplitude adjustment occurred automatically.  The advantage of the latter was that more accurate spectral modelling was possible, at the expense of additional formant amplitude controls.  Lawrence's machine was the first successful resonance analogue speech synthesiser when it toured the USA in 1953.  See also <A HREF = "#acousticAnalog">acoustic analog</A>, <A HREF = "#physiologicalAnalog">physiological analog or model</A>, <A HREF="#tubeModel">tube model</A> and <A HREF = "#terminalAnalogSpeechSynthesiser">terminal analog speech synthesiser</A>.
</P>

<A NAME = "resonanceTheoryOfHearing">
<I>resonance theory of hearing</I>
</A><BR>

A place theory of frequency analysis in auditory perception that assumes different sections of the <A HREF = "#basilarMembrane">basilar membrane</A> are tuned to different frequencies and respond like a mechanical frequency analyser, transmitting the place of maximum vibration to the central system and thus marking the frequencies present in the stimulus.  The known properties of the <A HREF = "#basilarMembrane">basilar membrane</A> render the theory untenable, but in practical terms it differs little from the accepted travelling wave theory which is another "place" theory, but one that is generally accepted as part of the mechanism by which frequency content is discriminated.  See also <A HREF="#volleyTheory">volley theory</A>.
</P>


<A NAME = "resonanceVocoder">
<I>resonance vocoder</I>
</A><BR>

See <A HREF = "#vocoder">vocoder</A>.
</P>

<A NAME = "resonator">
<I>resonator</I>
</A><BR>

A system or structure that stores ocillatory energy at particular frequencies (often only one) that depend on its physical properties and the way energy is supplied to the system or structure.
</P>

<A NAME = "reticularMembrane">
<I>reticular membrane</I>
</A><BR>

Part of the <A HREF = "#organOfCorti">organ of Corti</A> which supports the outer hair cells.
</P>

<A NAME = "rhamonics">
<I>rhamonics</I>
</A><BR>

See <A HREF = "#cepstralAnalysis">cepstral analysis</A>.
</P>

<A NAME = "rhythmUnits">
<I>rhythm units</I>
</A><BR>

The name given by Jassem to his formulation of rhythmic units.  See <A HREF = "#rhythmicUnits">rhythmic units</A>, <A HREF = "#foot">foot</A> and <A HREF = "#isochrony">isochrony/isochronicity</A><P>

<A NAME = "rhythmicUnits">
<I>rhythmic units</I>
</A><BR>

In a stress-timed language such as English, the speech may be divided into rhythmic units related to the occurrence of the stresses, or beats.  M.A.K. Halliday, following David Abercrombie (one-time Professor of Phonetics and Linguistics at the University of Edinburgh), puts the rhythmic unit boundaries just before each syllable bearing primary stress and calls the unit a "<A HREF = "#foot">foot</A>".  Jassem's scheme (he was at the Polish Academy of Science, Poznan) is similar, but excludes <A HREF = "#proclitic">proclitic</A> syllables from consideration, likening them to the <A HREF = "#anacrusis">anacruses</A> in music.  Such frameworks can provide a basis for workable models of English rhythm, but require extensive analysis of real speech <A HREF = "#segment">segments</A>.  One such study by the author of this conceptionary, together with Jassem and Witten, slightly favoured the Jassem formulation, but a subsequent model of rhythm for use in <A HREF="#speechSynthesisByRules">speech-synthesis-by-rules</A> used the Halliday formulation because it is a little simpler.
</P>

<A NAME = "RIT">
<I>RIT</I>
</A><BR>

Also KTH (from the Swedish name). The Royal Institute of Technology in Stockholm, where C.G.M. Fant had his Speech Technology Laboratory (now the <A HREF="http://www.speech.kth.se">Dept. of Speech, Music and Hearing</A>).  One of the two or three most important speech research laboratories in the world, and certainly as much a pioneering institution as the<A HREF="#haskins"> Haskins Laboratory</A>.  Like most institutions these days, their research is accessible through their websites.  Their regular paper reports have been discontinued (though papers still appear in the peer reviewed journals).
</P>

<A NAME = "RITSTLQPSR">
<I>RIT STL QPSR</I>
</A><BR>

Royal Institute of Technology (Stockholm) Speech Transmission Laboratory Quarterly Progress and Status Report.  See <A HREF="#RIT">RIT</A>.
</P>

<A NAME = "RL">
<I>RL</I>
</A><BR>

See <A HREF = "#differenceLimen">difference limen</A>.
</P>

<A NAME = "RLE">
<I>RLE</I>
</A><BR>

The <A HREF="http://rleweb.mit.edu">Research Laboratory of Electronics</A>, at Massachusetts Institute of Technology.  Much important speech and language research has been carried out there.  Dennis Klatt was there; Jonathan Allen (originator of DECtalk/MITalk) and Ken Stevensstill hold professorships there in the <A HREF="http://web.mit.edu/speech">speech communication group</A>.  
</P>

<A NAME = "rms">
<I>rms</I>
</A><BR>

"rms" stands for "root mean square".  Typically encountered in measuring things like alternating electrical voltage.  The rms value reflects a usable average electrical pressure (voltage).  A simple minded approach would say the average voltage was zero.  By using an average measure that involves squaring and taking the square root, the apparent cancellation of positive and negative halves of the waveform is avoided, and a measure of power can be computed.
</P>


<A NAME = "roundWindow">
<I>round window</I>
</A><BR>

The membrane dividing the air space of the middle ear from the fluid filled scala tympani.  Without this flexible end to the scala tympani there would be no displacement of the inner ear fluids and no displacement of the <A HREF = "#basilarMembrane">basilar membrane</A>.  It is complementary to the <A HREF = "#ovalWindow">oval window</A>.
</P>

<A NAME = "SNratio">
<I>S/N ratio</I>
</A><BR>

See <A HREF = "#signalToNoiseRatio">signal to noise ratio</A>.
</P>

<A NAME = "sSs">
<I>S/S's</I>
</A><BR>

Subject/subjects.
</P>

<A NAME = "sagittalSection">
<I>sagittal section</I>
</A><BR>

Section taken through the plane dividing an organism into right and left halves.
</P>

<A NAME = "salient">
<I>salient</I>
</A><BR>

In a stress-timed language, such as English, where primary stressed syllables seem to fall at fairly regular intervals, regardless of the number of unstressed syllables intervening, the rhythm may be regarded within a framework of <A HREF = "#foot">feet</A>, like the bars in music.  Each bar either begins with a stressed beat or a silent beat and, in Abercrombie's terminology (which M.A.K. Halliday follows), the former is called a salient syllable and the latter a silent stress or phonological pause.  There is little if any acoustic evidence for the second.  See <A HREF = "#stress">stress</A>, <A HREF = "#prominence">prominence</A> and <A HREF = "#isochrony">isochrony</A>.
</P>

<A NAME = "scalaVestibuliScalaTympani">
<I>scala vestibuli/scala tympani</I>
</A><BR>

The two outer chambers of the tube which, coiled up, forms the <A HREF = "#cochlea">cochlea</A>, separated by the cochlear partitionóitself a fluid filled duct.  The scala vestibuli starts at the oval window (to which is attached the <A HREF = "#stapes">stapes</A> ossicle by the annular ligament) and ends at the helicotrema where it joins to the scala tympani.  The scala tympani extends between the helicotrema and the round window. The cochlear duct ends at the helicotrema and does not communicate with either the scala vestibuli or the scala tympani.  See <A HREF = "#bonyLabyrinth">bony labyrinth</A> and <A HREF = "#membranousLabyrinth">membranous labyrinth</A>.
</P>

<CENTER>

<A HREF = "images/dh16.jpg" TARGET="new"><IMG SRC="images/dh16thum.jpg">
<BR>
Section through the auditory mechanisms</A><BR>
<FONT SIZE=1> (From <I>Experiments in Hearing</I>, by Georg von BÈkÈsy.<BR>© 1960 McGraw-Hill Book Company. Used with permission of McGraw-Hill Book Company.)</FONT>
</P>
</P>

</CENTER>


<A NAME = "scalaMedia">
<I>scala media</I>
</A><BR>

The interior space of the cochlear partitionóthe cochlear duct, filled with endolymph.  There is no fluid connection with the perilymph-filled scala vestibuli or scala tympani.  See <A HREF = "#scalaVestibuliScalaTympani">scala vestibuli/scala tympani</A> and <A HREF = "#cochlea">cochlea</A>.
</P>

<A NAME = "scalesOfMeasurement">
<I>scales of measurement</I>
</A><BR>

Stevens (Handbook of Exptl. Psych., Wiley, 8th printing 1966, pp 22 et seq.) gives a good discussion of scales of measurement.  Measurement is the assignment of numerals to objects or events according to rules.  There are four generally accepted scales of measurement.  Nominal (which simply labels classes and only permits counting of events and determination of mode): Ordinal (which rank orders classes, is preserved by any monotonic increasing functional transformation, preserves the notion of "between", but whose interclass intervals are of unknown relative size); Interval (a commonly encountered scale for which most statistics are valid, for which intervals of equal magnitude really represent equal distances in terms of the quality being measured, but which have an arbitrary zeroómeasurements on one interval scale may be transformed into intervals on another by an (x' = ax'' + c) type of transform, as for Celsius and Fahrenheit temperature scales); and Ratio (for which it makes sense to say that a quantity which is numerically twice as big as another really does represent double the quantity -- conversion from one interval scale to another is simple multiplication by a constant, (x' = ax'') and there is a real hard zero.).  Psychology aspires to interval scales but often achieves only ordinal scales to which it applies invalid statistics with surprising success.  Physics depends on ratio scales.
</P>

<A NAME = "SEF">
<I>SEF</I>
</A><BR>
See <A HREF="#singleEquivalentFormant">single equivalent formant</A>.
</P>

<A NAME = "segment">
<I>segment</I>
</A><BR>

The result of applying <A HREF="#segmentation">segmentation</A> to speech.  Traditionally a speech segment corresponds to a <A HREF = "#phoneme">phoneme</A> and may be associated with a speech posture.  However, a segment is more accurately identified with a <A HREF="#phone">phone</A>.  Phones falling in a given phoneme category vary widely.  This fact, and the difficulty of providing a reliable algorithm for deciding where boundaries between segments are located cause great difficulty for simple approaches to speech recognition by machine.
</P>

<A NAME = "segmentSynchronisation">
<I>segment synchronisation</I>
</A><BR>

An approach to solving the segmentation problem for speech recognition purposes. Early work in America done by Vicens under the supervision of Reddy gives the method its name.  The basic idea is to match input sounds against stored templates of sounds in the machine's vocabulary on the basis of the statistics of the sound segments at the word level.  However, an initial analysis classifies words on the basis of only a few easily detected kinds of segment (silence, friction, vowel) and portions remain unclassified, but statistically described.  When an unknown sound is input, only those stored templates having the same broad pattern are considered for matching.  The known segments are mapped between the unknown sound and a possible matching template, which also puts the unclassified segments into the most plausible relationship. All segments are then compared on a detailed statistical basis (some segments, say those unrepresented in one sound but existing in the other, will contribute negatively).  The best fit from all the candidates is then chosen as the identification response by the machine, thus "recognising" the input word.  Modern methods using <A HREF="#HiddenMarkovModel">Hidden Markov Models</A> (HMMs) have formalised and replaced this approach.  Though simpler to implement, in some sense, it is not clear that HMMs achieve all the advantages of Vicen's approach.  Both approaches take what I call a "salami" approach to speech (they slice the speech into pieces at what are inevitably arbitrary time boundaries, and ignore the asynchronous nature of the acoustic cues to speech perception).  See also <A HREF="#segmentation">segmentation</A>.
</P>

<A NAME = "segmental level">
<I>segmental level</I>
</A><BR>

The level of analysis, description, etc. that attempts to deal with sound segments (phones). See <A HREF = "#phone">phone</A>, <A HREF = "#segment">segment</A> and <A HREF = "#segmentation">segmentation</A><P>

<A NAME = "segmentation">
<I>segmentation</I>
</A><BR>

The division of continuous speech into successive chunks (the "salami" approach), each associated with a single phoneme.  Ken Stevens of the <A HREF="#RLE">RLE</A> at MIT wittily and correctly described the problem of doing this as "the problem that you can't".  Nevertheless it is still attempted on the basis of assumptions that have limited validity in phonetics, and less validity in speech recognition.  The problem is inherent in the definition of what is a <A HREF = "#phoneme">phoneme</A>, as well as the fact that actual speech sounds (<A HREF="#phone">phones</A>) run into one another and influence one another.  See also <A HREF = "#timeNormalisation">time normalisation</A>.
<P>

<A NAME = "semantics">
<I>semantics</I>
</A><BR>

The study of the relation of signs to objects (S.S. Stevens).  The study of meaning and the patterns of meaning in a language.  Semantic constraints in a language exist at a higher level than syntactic constraints.   Thus "The boy ate mount Everest for now" is syntactically impeccable, but semantically wrong, while "Ate mount for boy the now Everest" is wrong on both counts. Constraints imply <A HREF="#redundancy">redundancy</A> which implies the possibility of correcting for errors or inadequacy at lower levels of analysis, which is why semantic and syntactic constraints are important in speech recognition and communication in noise.  See <A HREF = "#pragmatics">pragmatics</A>, <A HREF = "#syntactics">syntactics</A> and <A HREF="#information">information</A>.
</P>

<A NAME = "semicircularCanals">
<I>semicircular canals</I>
</A><BR>

Parts of the bony labyrinth concerned with balance.  They interconnect with the scala which form part of the space inside the <A HREF = "#cochlea">cochlea</A>.  The <A HREF = "#cochlea">cochlea</A> is concerned with hearing.  See bony labyrinth and <A HREF = "#membranousLabyrinth">membranous labyrinth</A>.
</P>

<A NAME = "semiotics">
<I>semiotics</I>
</A><BR>

The study of signs and symbols in various fields.  The study of signalling systems.  Comprises <A HREF="#syntactics">syntactics</A>, <A HREF="#semantics">semantics</A> and <A HREF="#pragmatics">pragmatics</A>.  See <A HREF="#information">information</A>.
</P>

<A NAME = "sensationLevel">
<I>sensation level</I>
</A><BR>

Not to be confused with threshold of sensation.  The sensation level of a tone is the number in decibels by which it exceeds the threshold of hearing for that frequency.
SL = 10 log (I/I<SUB>t</SUB>) <A HREF = "#db">db</A> re I<SUB>t</SUB>, the threshold intensity.
</P>

<A NAME = "sensors">
<I>sensors</I>
</A><BR>

Organs (devices) for accepting input from the environment.  Sensory data receivers (where "sensory" comes from the same root as sensesóthose faculties of perception possessed by biological organisms).  Receptors.
</P>

<A NAME = "SI">
<I>SI</I>
</A><BR>

Systeme International (SI) is a standard metric-based measuring system adopted  in 1960 at the 11th General Conference of Weights and Measures.  It is based on metres, kilograms, seconds, amperes (units of electric current), degrees Kelvin (which measure temperature -- degrees Kelvin = degrees Celsius + 273), candelas (which measure luminance), and moles (which measure the amounts of substances, and relate to the number of molecules).<P> 

<A NAME = "sign">
<I>sign</I>
</A><BR>

Used in linguistics to indicate the way that linguistic expressions (words, phrases, ...) represent the situations, objects, and so on for which they stand.  Swiss linguist de Saussure strongly affected relevant discussion in linguistics by contrasting the <I>signifier</I> with the <I>concept signified</I>.  <A HREF="#semiotics">Semiotics</A> is the science of signs.  Terms such as <I>sign language</I> use "sign" in a very restricted sense.
</P>

<A NAME = "signalToNoiseRatio">
<I>signal to noise ratio</I>
</A><BR>

A measure of how the signal level compares to the noise level.  It is expressed as the power of the signal in <A HREF = "#db">db</A> relative the power of the noise and is thus dimensionless.
</P>

<A NAME = "SIL">
<I>SIL</I>
</A><BR>

See <A HREF = "#speechInterferenceLevel">speech interference level</A>.
</P>

<A NAME="singleEquivalentFormant">
<I>single equivalent formant</I>
</A><BR>

By processing the speech spectrum, Charles Teacher and his associates at Philco-Ford produced this measure of speech spectra for speech recognition purposes.  In essence it follows <A HREF="#formant">formant 1</A> in back <A HREF = "#vowel">vowels</A> and formant 2 in front <A HREF = "#vowel">vowels</A>, providing a single dimension for the discrimination of <A HREF = "#vowel">vowel</A> sounds.  See <A HREF = "#formant">formant</A>
</P>

<A NAME = "Sonagram">
<I>Sonagram</I>
</A><BR>

See <A HREF = "#soundSpectrograph">sound spectrograph</A>.
</P>

<A NAME = "sone">
<I>sone</I>
</A><BR>

The unit of measurement on the subjective ratio scale of loudness.  One sone is defined as the loudness of a tone which is 40 <A HREF = "#db">db</A> above the threshold of hearing at 1000 hz, i.e. the loudness of a 1000 hz tone at a sensation level of 40 <A HREF = "#db">db</A> is 1 sone.  The scale was set up by asking subjects to adjust the ratio of two tones presented simultaneously.  It has a real zero which follows the threshold of hearing as the frequency varies.  A loudness of 2 sones sounds twice as loud as one of 1 sone.  Another subjective unit of loudness, on which an interval scale is based, is the phon.  See <A HREF="#loudness">loudness</A>, <A HREF = "#scalesOfMeasurement">scales of measurement</A> and <A HREF = "#phon">phon</A>.
</P>
<CENTER>
<A HREF="images/sonephon.jpg" TARGET="new"><IMG SRC="images/sonephonthum.jpg"><BR>
Plot of Sones against Phons</A>
</P>
</CENTER>


<A NAME = "soundIntensityLevel">
<I>sound intensity level</I>
</A><BR>

Abbreviated as IL.  Defined as:
IL = 10 log (I/I<SUB>o</SUB>) <A HREF="#db">db</A>re I<SUB>o</SUB> <A HREF="#watt">watts</A>/m<SUP>2</SUP>
</P>

<A NAME = "soundPressureLevel">
<I>sound pressure level</I>
</A><BR>

Abbreviated as SPL.  Defined as:
SPL = 20 log (p/p<SUB>o</SUB>) <A HREF = "#db">db</A> re p<SUB>o</SUB> <A HREF="#nt">nt</A>/m<SUP>2</SUP>
</P>

<A NAME = "soundSpectrograph">
<I>sound spectrograph</I>
</A><BR>

A device for carrying out a frequency analysis of audio signals and presenting the results as a pattern of time-energy-frequency variation called a <I>spectrogram</I>.  The first such apparatus was constructed by Dunn and others at Bell Laboratories in the early '40's and was subsequently produced commercially by the Kay Electric Co. of Pinebrook, New Jersey, under the brand name of "The Kay Sonagraf".  The machine used Teledeltos paper (which darkens when an electric current is passed through it) to record the output which Kay called a <I>Sonagram</I>.  The machine used a mechanically tuned filter linked to a motor-driven scanning/marking head, which was moved, axially,up the outside of a cylinder around which the recording paper was wound.  The cylinder revolved, generating the time scale as an x-axis.  Darkness of marking indicated the energy present, while frequency was displayed up the paper as the y-axis, due to the mechanical linkage between frequency of analysis and the scanning system.  The sound signal being analysed was recorded around the periphery of a disc which was locked on the same axle as the recording drum. It was able to take spectral sections, generate amplitude displays, and vary the frequency scale and analysing filter bandwidth to suit formant or pitch analysis. An extra attachement allowed "contour" spectrograms to be produced which added intensity contours to the fine grey scale energy display.  Purely electronic equipment has replaced the original electromechanical Sonagraf, and these are in the process of being displaced by suitable software running on general purpose desktop and laptop computers.  The new media have yet to duplicate the subtlety of marking that made the Sonagram such an excellent source of speech data. However, they are more flexible, and avoid most of the tedious calibration and maintenance problems of the early equipment.  The figure shows a spectrogram of the vowels that are characteristic of General Amercian (GA).
</P>

<CENTER>
<A HREF="images/vowels.jpg" TARGET="new"><IMG SRC="images/vowelslo.jpg"><BR>
Broad band spectrograms of the eleven vowels of General American<BR>
characteristic of General American</A><BR>
The plots show intensity against frequency and time<BR>
<FONT SIZE=1>
(From: The calculation of vowel resonances, and an electrical vocal tract<BR>
by H.K. Dunn 1950, <I>J. Acoust. Soc. Amer.,</I> <B>22</B>, pp 740-753)
</P></FONT>
</CENTER>


<A NAME = "sourceFilterModelOfSpeechProduction">
<I>source-filter model of speech production</I>
</A><BR>

The speech production process is modelled as a "source", or excitation function (periodic, random, or a combination of both), which generates the <A HREF = "#spectralFineStructure">spectral fine structure</A>):  and a "filter" or vocal tract response function, characterised by the values of resonances (formant peaks), anti-resonances, losses, and radiation impedances.  The source function may be defined crudely by 4 parameters (<A HREF = "#Ax">Ax</A>, <A HREF = "#FxFo">Fx</A>, <A HREF = "#AH1/AH2">AH1 and AH2</A>) and the filter by another 4 (<A HREF = "#formant">formants</A> F1 to F3 and a <A HREF = "#fricative">fricative</A> noise spectrum shaper).  When these act together, a varying spectrum with the correct shape and fine structure may be generated by a synthesiser based on such an approximation to the source-filter properties of the vocal system.  A resonance analog <A HREF = "#speechSynthesiser">speech synthesiser</A> is thus based on the source-filter model of speech production.  The Klatt software synthesiser is such a synthesiser, but implements a far more detailed realisation of the model.  The <A HREF = "#ParametricArtificialTalker">Parametric Artificial Talker</A> (Lawrence 1953) was the first successful, true source-filter-based synthesiser.  See <A HREF = "#transmissionLineAnalogSpeechSynthesiser">transmission line analog speech synthesiser</A> and <A HREF = "#tubeModel">tube model</A>.
</P>

<A NAME = "spectralEnvelope">
<I>spectral envelope</I>
</A><BR>

The broad shape of the (sound) spectrum, as might be revealed by a wide band analysis (300 hz bandwidth) using a <A HREF = "#soundSpectrograph">sound spectrograph</A>.  The chief determiner of the spectral envelope of speech sounds is the filter function of the vocal tract.  The spectral envelope often exhibits peaks and these are termed "<A HREF = "#formant">formant</A>".  See also <A HREF = "#voicing">voicing</A>.
</P>

<A NAME = "spectralFineStructure">
<I>spectral fine structure</I>
</A><BR>

See <A HREF = "#voicing">voicing</A>.
</P>

<A NAME = "spectrogram">
<I>spectrogram</I>
</A><BR>

See <A HREF = "#soundSpectrograph">sound spectrograph</A>.
</P>

<A NAME = "spectrographicAnalysis">
<I>spectrographic analysis</I>
</A><BR>

See <A HREF = "#soundSpectrograph">sound spectrograph</A>.
</P>

<A NAME = "spectrum">
<I>spectrum</I>
</A><BR>

See <A HREF = "#frequencySpectrum">frequency spectrum</A>.
</P>

<A NAME = "spectrumLevel">
<I>spectrum level</I>
</A><BR>

See <A HREF = "#intensitySpectrumLevel">intensity spectrum level</A>.
</P>

<A NAME = "spectrumLevelCorrection">
<I>spectrum level correction</I>
</A><BR>

A sound spectrum may be described by dividing the frequency spectrum into an arbitrary set of bands and expressing the energy present in each band as the sound intensity level (IL) or sound pressure level (SPL).  In general, assuming uniform impedance, and appropriately chosen reference levels, these are the same.  If the chosen bands are 1 hz wide, the resulting description is termed the intensity spectrum level.  If bands of any other width (say DF) are used, the spectrum may be converted to an intensity spectrum level description by applying a correction factor:
			-10 log DF
to each IL or SPL value taken in the various bands.  If the original description was in terms of (say) an octave band description, then the correction factor puts each successive band down by 3 <A HREF = "#db">db</A> more than the last, since each band is twice the width of the last, as frequency increases, and, for octave bands of equal power, the intensity per hz is halved (-10 log 2 is -3).  See also <A HREF = "#intensitySpectrumLevel">intensity spectrum level</A> and <A HREF = "#octaveBandLevel">octave band level</A>.
</P>

<A NAME = "speechInterferenceLevel">
<I>speech interference level</I>
</A><BR>

An approximate method of predicting the <A HREF = "#intelligibility ">intelligibility</A> of speech is based upon the speech interference level, and is valid where the noise spectrum is reasonably continuous (ventilation noise, engines, ...).  The SIL is the arithmetic average of noise in the three octave bands 600-1200, 1200-2400, and 2400-4800.  Given the SIL a table is entered which gives the maximum distance at which 75% of phonetically balanced words (98% of test sentences) will be heard correctly.  This degree of <A HREF = "#intelligibility ">intelligibility</A> corresponds roughly to an <A HREF = "#articulationIndex">articulation index</A> of 0.5.
</P>

<A NAME = "speechPosture">
<I>speech posture</I>
</A><BR>

A target for the vocal apparatus in human speech.  In approximating a succession of speech postures, with appropriate concurrent control of breath and pitch, the human vocal apparatus produces the sounds of speech.  The author prefers to talk about speech postures as a basis for machine synthesis of speech by rules, because <A HREF = "#phone">phones</A> are the sounds of speechóa result rather than a cause, and <A HREF = "#phoneme">phonemes</A> are abstract categories of sounds which may be produced by a variety of speech postures.
</P>

<A NAME = "speechSynthesiser">
<I>speech synthesiser</I>
</A><BR>

A speech synthesiser models some aspect of the speech signal -- its form, production, etc. -- in order that speech sounds (<A HREF = "#phone">phones</A>) may be produced artifically from suitable control signals.  The control signals required for an utterance may be extracted from analyses of real (natural) speech, leading to "<A HREF = "#compression">compressed</A> speech" based synthesis.  The signals may also be computed on the basis of general knowledge about speech cues, articulatory constraints, etc., without reference to a particular natural utterance, and <A HREF = "#intelligibility">intelligible</A> speech still produced. This latter method of synthesis is termed <A HREF = "#speechSynthesisByRules">speech-synthesis-by-rules</A>.  Although the term has been applied to <A HREF = "#segmental level">segmental level</A> synthesis, speech completely by rule would must include algorithms for producing the acoustic correlates of <A HREF = "#suprasegmental">suprasegmentals</A> (rhythm, intonation, stress, and so on), and even converting ordinarily spelled speech into strings of sound symbols first (text-to-phonetic-symbols).  All of these levels (including segmental synthesis) are still very much research topics, but most progress has been made at the <A HREF = "#segmental level">segmental level</A>.  Converting normal <A HREF = "#orthography">orthography</A> to spoken words, by machine, is the "text-to-speech problem".  The first text-to-speech system was created by 
</P>

<A NAME = "speechSynthesisByRules">
<I>speech-synthesis-by-rules</I>
</A><BR>

The production of speech by machine, without direct reference to any original utterance, based on general knowledge of speech characteristics, together with models of the acoustic behaviour of the vocal tract and related speech production essentials, rhythm, and intonation. See <A HREF = "#speechSynthesiser">speech synthesiser</A>.
</P>


<A NAME = "speed of sound">
<I>speed of sound</I>
</A><BR>

The speed of sound in air is given by:<BR>
<SPACER TYPE=horizontal SIZE=100>c = (g.p)/r metres/second<BR>
where g (the ratio of specific heats at constant pressure to that at constant volume) is 1.4, p (static pressure) is in newtons/square metre , and r (density) is in kg/cubic metre . At 20 degrees Celsius and standard pressure, the speed of sound is about 343 m/sec in air, and varies as the square root of the absolute temperature (about .63 m/sec/∞C at room temp).<BR>
<SPACER TYPE=horizontal SIZE=40>In solids of large cross-sectional area:<BR>
<SPACER TYPE=horizontal SIZE=100>c = Y(1-m)/(r(1+m)(1-2m)) metres/second<BR>
where Y is Young's modulus of elasticity in nt/square metre, r the density in kilograms/cubic metre , and m, is Poisson's ratio.  If the cross-section is small, the lateral effect of Poisson's ratio (m) may be neglected giving:<BR>
<SPACER TYPE=horizontal SIZE=100>c = Y/r<BR
where B is the bulk modulus in newtons/square metre.
</P>

<A NAME = "SPL">
<I>SPL</I>
</A><BR>

See <A HREF = "#soundPressureLevel">sound pressure level</A>.
</P>

<A NAME = "stapes">
<I>stapes</I>
</A><BR>

A very small stirrup-shaped bone forming part of the chain from the <A HREF = "#earDrum">ear-drum</A> to the <A HREF = "#ovalWindow">oval window</A>.  See <A HREF = "#ossicles">ossicles</A>.
</P>

<A NAME = "stopSound">
<I>stop sound</I>
</A><BR>

See <A HREF = "#voicedStop">voiced stop</A>.
</P>

<A NAME = "stress">
<I>stress</I>
</A><BR>

Daniel Jones describes stress (in speech) as the degree of force with which a sound or syllable is uttered, and says that it correlates with the <A HREF = "#acousticIntensity">acoustic intensity</A>.  He distinguishes stress from <A HREF = "#prominence">prominence</A> which, he says, depends on combinations of quality (some vowels are inherently more prominent than others), quantity (duration), stress and (for voiced sounds) intonation (the pattern of <A HREF = "#glottalRateFrequency">glottal frequency</A> variation).  He defines a syllable as a sound sequence having a small peak of prominence and notes that this may be increased or decreased by changing length, stress, or <A HREF = "#intonation">intonation</A>, which are closely inter-related in English.  Stress and intonation modifications are usually combined to cause <A HREF = "#prominence">prominence</A>.  Instrumental studies of spoken English confirm the importance of all these factors and suggest that, while many writers discuss levels of stress in spoken English (word stress which may be unstressed, primary stress or secondary stress; and sentence stress), they would be more nearly correct to talk about levels of <A HREF = "#prominence">prominence</A>, which is the important <I>perceptual</I> effect that we are really concerned with.  This would avoid the considerable confusion that has been generated in the field, and keep faith with Jones. See <A HREF = "#tonic">tonic</A>.
</P>

<A NAME = "subjective">
<I>subjective</I>
</A><BR>

As perceived rather than as measured by instruments. Instrumental measurement, in theory, leads to "objective" measurements.  It is worth noting that a person's perception of instrument readings is itself subjective; a well designed instrument gives repeatable readings for the same physical phenomenon regardless of the person who operates itóthough there remains the problem of interpretation.
<BR>
<SPACER TYPE=horizontal SIZE=40>Well-trained people can supply highly repeatable subjective results (e.g. for testing communication systems).  The real difference between objective and subjective measurement may be illustrated as follows.  If a well designed instrumentation system and a well trained team of humans are set the task of quantifying the same physical phenomenon, using a standard reference point (e.g. the loudness (subjective measure) and intensity (objective measure) of a standard tone are agreed to be the same), they will NOT agree on the ratio between other values of the stimulus and the reference, even though they will very likely agree on the the rank order (ordinality) of the other stimuli on their respective scales.  In other words, subjective and objective scales measuring the same phenomenon are not linearly related.  Some objective scales relate better to subjective than others (e.g. <A HREF = "#decibel">decibels</A> to loudness).  See <A HREF = "#scalesOfMeasurement">scales of measurement</A><P>

<A NAME = "subjectiveTesting">
<I>subjective testing</I>
</A><BR>

The basic method of measuring speech <A HREF = "#intelligibility">intelligibility</A> in communication systems.  Requires highly trained test subjects, and careful experimental procedures.  Any method of evaluating speech intelligibility in a system which does not use proper <A HREF = "#subjective">subjective</A> testing is either an estimate (based upon instrumental readings and assumed relationships -- see, for example, <A HREF = "#articulationIndex">articulation index</A>), or is to be regarded with grave suspicion.
</P>

<A NAME = "superiorOlive">
<I>superior olive</I>
</A><BR>

See <A HREF = "#auditoryPathways">auditory pathways</A>.
</P>

<A NAME = "supraglottal">
<I>supraglottal</I>
</A><BR>

Above (higher than, towards the mouth from) the <A HREF = "#glottis">glottis</A>, which is the opening between the <A HREF = "#vocalFolds">vocal folds</A> which are situated within the <A HREF = "#larynx">larynx</A>.
</P>

<A NAME = "suprasegmental">
<I>suprasegmental</I>
</A><BR>

Literally "above the <A HREF = "#segmental level">segmental</A>".  Matters/features extending over more than one speech <A HREF = "#segment">segment</A>.  See also <A HREF = "#prosody">prosody</A>.
</P>

<A NAME = "syllable">
<I>syllable</I>
</A><BR>

"Syllables" are not well defined, though even children seem to have little difficulty detecting syllables in spoken English, so that the meaning of the word is often taken for granted.  A syllable must have a nucleus (usually a vowel or diphthong, though the function may be taken on by some consonants such as /n, l, z/).  The inherent <A HREF = "#intensity">intensity</A> of such sounds, coupled with variations in pitch, often with weaker consonants helping in separation between them, lead to the perception of the "lumps" in speech that we call syllables.  The consonants occurring between nuclei have a definite sense of belonging to one nucleus, rather than the other, though this belonging can be reversed by marked changes in the relevant segmental durations, including the the introduction of pauses, or the occurrence of stress.  See also <A HREF = "#prominence">prominence</A>.
</P>

<A NAME = "syllableNucleus">
<I>syllable nucleus</I>
</A><BR>

See <A HREF = "#syllable">syllable</A>.
</P>

<A NAME = "symbol">
<I>symbol</I>
</A><BR>

Something which stands for something else.  It is usually more accessible than what it stands for, and usually has some obvious relation to the thing symbolised.  Symbols thus have meaning (<A HREF = "#denotation">denotations</A> or <A HREF = "#semantics">semantics</A>), unlike signs which are related by <A HREF = "#syntactics">syntax</A>.  A "mnemonic" symbol is easily remembered.
</P>

<A NAME = "synapse">
<I>synapse</I>
</A><BR>

An electro-chemical junction between the <A HREF = "#axon">axon</A> of one nerve cell and a <A HREF = "#dendrite">dendrite</A> or the cell-body (<A HREF = "#cyton">cyton</A>) of the next.  Unlike a nerve fibre, a synapse will only transmit in one direction.  Synapses may be inhibitory as well as excitatory.
</P>

<A NAME = "syntactics">
<I>syntactics</I>
</A><BR>

The study of syntax, the relation of signs to signs.  The study of the rules for compounding words into sentences and utterances of languages. The study of grammar.  See <A HREF = "#semantics">semantics</A>, <A HREF = "#pragmatics">pragmatics</A> and <A HREF = "#semiotics">semiotics</A>.
</P>

<A NAME = "talkersSideTone">
<I>talkers side tone</I>
</A><BR>

The fraction of speech input that is fed back to the ear(s) of a talker in a communication system.  It ensures the system appears "alive" to a user and may be manipulated to influence how loudly a talker speaks.  Delayed feedback can slow a talker down, and may disrupt the talker's speech altogether if the delay is around 200 millseconds.
</P>

<A NAME = "tectorialMembrane">
<I>tectorial membrane</I>
</A><BR>

The stiff membrane that lies over the hair cells of the <A HREF = "#organOfCorti">organ of Corti</A> and is displaced laterally with respect to these as the <A HREF = "#basilarMembrane">basilar membrane</A> arches up and down under the influence of transmitted pressures in the lymph ducts above and below the cochlear partition.  This lateral displacement activates the hair cells and transmits nerve pulses along the <A HREF = "#auditoryPathways">auditory pathways</A> and ultimately to the auditory cortex.  The combination of the relatively stiff tectorial membrane and flexible <A HREF = "#basilarMembrane">basilar membrane</A>, which act together to stimulate the hair cells by shearing forces, as the <A HREF = "#basilarMembrane">basilar membrane</A> is displaced by pressure gradients across it, provides a near perfect match between the fluid resistance in the perilymph and the much more resistive mechanical system of the hair cells.  Taken together with air-to-fluid matching provided by the ossicular chain, this explains the incredible sensitivity of the human ear, which approaches the theoretical limit. Displacements of the ear-drum, at the threshold of hearing, correspond to about the diameter of a hydrogen atom!  Some subjects may actually hear random motion of air molecules, under optimum conditions, as part of the base level noise in their system.  See <A HREF = "#thresholdOfHearing">threshold of hearing</A>, <A HREF = "#watt">watt</A> and <A HREF = "#ossicles">ossicles</A>.
</P>

<A NAME = "teethRidge">
<I>teeth ridge</I>
</A><BR>

See <A HREF = "#alveolarRidge">alveolar ridge</A>.
</P>

<A NAME = "telephoneTheoryOfHearing">
<I>telephone theory of hearing</I>
</A><BR>

Assumes the ear has the role of simple <A HREF = "#transducer">transducer</A>, rather than one of <A HREF = "#FourierAnalysis">frequency analysis</A>, only converting the sound pressure wave into pulse trains in the nervous system whose time characteristics preserve the time characteristics of the original pressure fluctuations.  The frequency analysis is assumed to be performed by some (unknown) central nervous system mechanism.  Central analysis of pulse frequency undoubtedly plays a role in auditory perception, but peripheral tone discrimination also plays an important role.  See <A HREF = "#placeTheoriesOfHearing">place theories of hearing</A> and <A HREF = "#volleyTheory">volley theory</A>.
</P>

<A NAME = "terminalAnalogSpeechSynthesiser">
<I>terminal analog speech synthesiser</I>
</A><BR>

A lumped property <A HREF = "#blackBox">black-box</A> style of electronic circuit that is designed to reproduce the transfer-function of the vocal tract without regard to the distributed properties (physical properties) the tract is known to possess.  A <A HREF = "#resonanceAnalog">resonance analog</A> speech synthesiser is one such terminal analog speech synthesiser. A physiological analog on the other hand is called (by Flanagan, of <A HREF="#bell">Bell Labs</A> who originated the name terminal analog) a transmission-line analog.  A physiological parameter speech synthesiser might be either a terminal analog or a transmission line analog depending on how the physiological parameters were turned into speech, according to Flanagan.  In all cases (acoustic analog, acoustic parameter or resonance analog, physiological analog, physiological parameter analog, terminal analog, and transmission line analog)  the derivation of sound from the input parameters could be done by: special-purpose analog hardware; special purpose digital hardware (e.g. a Digital Signal Processor or DSP); a general purpose analog computer; or a general purpose digital computer.  Whichever means was used, some underlying model of the speech production process would be essential.  In practical terms, formant synthesisers are terminal analogues and articulatory synthesisers are distributed analogues (<A HREF="#transmissionLineAnalogSpeechSynthesiser">transmission-line analogs</A>, waveguide, or tube models) because the conversions that would otherwise be required are too hard, in the present state of knowledge and technology.
</P>

<A NAME = "terminal limen">
<I>terminal limen</I>
</A><BR>

See <A HREF = "#differenceLimen">difference limen</A>.
</P>

<A NAME = "tetragram">
<I>tetragram</I>
</A><BR>

Four letters appearing together, in order.  A study of tetragram frequencies may be of value in automated language processing.  A list of all legal tetragrams appearing in Webster's dictionary was published in the mid-sixties through the US Armed Services Technical Information Agency (ASTIA).
</P>

<A NAME = "theoriesOfHearing">
<I>theories of hearing</I>
</A><BR>

See <A HREF = "#volleyTheory">volley theory</A>, <A HREF = "#placeTheoriesOfHearing">place theory</A>, <A HREF = "#resonanceTheoryOfHearing">resonance theory</A>, <A HREF = "#travellingWaveTheoryOfHearing">travelling wave theory</A> and <A HREF = "#telephoneTheoryOfHearing">telephone theory</A>.
</P>

<A NAME = "thirdOctaveBandSpectrum">
<I>third octave band spectrum</I>
</A><BR>

See <A HREF = "#octaveBandSpectrum">octave band spectrum.</A>
</P>

<A NAME = "thresholdOfHearing">
<I>threshold of hearing</I>
</A><BR>

The response of the human ear coupled to the brain is a purely <A HREF = "#subjective">subjective</A> quantity and cannot be measured directly, nor directly related to objective physical measures, by any simple or universal function.  Hence there are objective measures (frequency, intensity, ...) and subjective (pitch, loudness, ...) which have different units.  A common method of expressing the objective threshold of hearing is as the <A HREF = "#acousticIntensityLevel">acoustic intensity level</A> with respect to the audiometric zero (2 x 10-5  nt/m2).  At this level, on average, pure tones may just be detected by subjects. The acoustic intensity level is plotted against frequency and shows the variation in threshold as a function of frequency.  Although the range extends from about 20 to 20000 hz, the ear is most sensitive from about 1000 to 4000 hz, where some subjects can hear tones below the standard audiometric zero. The normal threshold range for young adults is around 30 <A HREF = "#db">db</A> and the acoustic intensity at which the most sensitive human ear can detect sounds approaches 10-14 <A HREF="#watt">watts</A>/m<SUP>2</SUP>.  Audiometric zero represents an acoustic intensity of 10-12  watts/m<SUP>2</SUP>.  See <A HREF = "#thresholdOfPain">threshold of pain</A>, <A HREF = "#presbycusis">presbycusis</A>, <A HREF="#watt">watt</A>, and <A HREF = "#audiogram">audiogram</A>.
</P>

<A NAME = "thresholdOfPain">
<I>threshold of pain</I>
</A><BR>

As the intensity of sound increases, there comes a point where subjects report "discomfort", "tickling", "pricking" etc.  The level varies somewhat according to the frequency and experimental design, but not as greatly as does the threshold of hearing.  Higher mean levels may be tolerated for <A HREF = "#clippedSpeech">clipped speech</A> than are tolerated for unclipped speech.  The maximum tolerable level is variously described as the threshold of sensation, threshold of pain, etc.  Hearing damage may still result from levels which are lower than this.  The following figure summarises a number of studies of various thresholds at the lower and upper limits of hearing. See also <A HREF = "#damageRisk">damage risk</A>, and <A HREF = "#hearingLoss">hearing loss</A>.

</P>
<CENTER> <A HREF="images/dh19.jpg" TARGET="new"><IMG SRC="images/dh19thum.jpg"><BR>Thresholds of audibility and hearing</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</CENTER>
</P>


<A NAME = "threshold shift">
<I>threshold shift</I>
</A><BR>

The difference in <A HREF = "#db">db</A> between the threshold of hearing for a tone and the level of the tone at which it is just audible in the presence of a <A HREF = "#masking">masking</A> noise. This difference is used as a measure of the masking effect of the noise.  If the difference is small, the noise does not mask the tone very effectively.  If the differenc is large, the noise is an effective masking noise for the tone.
</P>

<A NAME = "timeDomain">
<I>time domain</I>
</A><BR>

The class of representations of a phenomenon comprising functions of time.  A speech waveform is in the time domain.  This contrasts with the results of a <A HREF = "#FourierAnalysis">Fourier analysis</A> of the waveform which would give a frequency domain representation (a <A HREF = "#frequencySpectrum">frequency spectrum</A>).  A time waveform comprises a distribution of amplitude against time.  A frequency spectrum comprises a distribution of amplitude against frequency.  They bear an inverse relationship.  The procedure for going from the first to the second being the inverse of going from second to first.
</P>

<A NAME = "timeNormalisation">
<I>time normalisation</I>
</A><BR>

A process which, if successful, would convert speech <A HREF = "#segment">segment</A> durations to some standard value so that comparisons between input sounds and reference sounds could easily be carried out for purposes of speech recognition.  <A HREF = "#dynamicTimeWarping">Dynamic Time Warping</A> (DTW), first proposed by Velichko and Zagoryuko in the early '70's, is a technique for partial time normalisation in which reference segments may map onto more than one unknown segment, or vice versa, in order, with some segments perhaps being omitted.  <A HREF = "#HiddenMarkovModel">Hidden Markov Model</A> (HMM) algorithms inherently embody a similar kind of time normalisation insofar as the states of the model may be visited repeatedly or omitted.  If time normalisation could be perfected, segmentation would be unnecessary.  Conversely, if segmentation could be perfected, time normalisation would be unnecessary.  They are different aspects of the same problem.
</P>

<A NAME = "TL">
<I>TL</I>
</A><BR>

Terminal Limen. See <A HREF = "#differenceLimen">difference limen</A>.
</P>

<A NAME = "tonality">
<I>tonality</I>
</A><BR>

The division of an utterance into "tone groups", for purposes of <A HREF = "#intonation">intonation</A> analysis or assignment, according to M.A.K. Halliday's scheme of things.  See <A HREF = "#tonic">tonic</A>.
</P>

<A NAME = "tone">
<I>tone</I>
</A><BR>

Has two meanings.  The common meaning refers to a sound comprising a single sinusoidal signal.  In M.A.K. Halliday's model of British English <A HREF = "#intonation">intonation</A>, it refers to the assignment of a particular intonation pattern to a particular tone group.  In other systems the analogous attribute is variously called "tune" (Kingdon & others), "tone- patterns" (Palmer & Blandford), "pitch morphemes" (Wells, though he says little about the meanings!), "intonation contours" (Pike, who treats them somewhat as supermorphemes) and so on.  See <A HREF = "#tonic">tonic</A>.
</P>

<A NAME = "tonic">
<I>tonic</I>
</A><BR>

In M.A.K. Halliday's formulation of the <A HREF = "#intonation">intonation</A> of British English (A course in spoken English:  Intonation, Oxford U. Press 1970) utterances are broken into <A HREF = "#foot">feet</A> -- analogous to bars of music and beginning with a stressed syllable.  These feet are then grouped into tone groups (tonality), assigned tonic stress (tonicity) and assigned an intonation pattern type (tone).  The tonicity divides the tone group into the pre-tonic and the tonic, with the first syllable of the first foot of the tonic being the tonic syllable, carrying <I>tonic</I> stressómaking it the most <A HREF="#prominence">prominent</A> syllable in the tone group.  The tonic marks the information point (sentence stress) of the tone group which corresponds, usually, to a clause or sentence.
</P>

<A NAME = "tonicity">
<I>tonicity</I>
</A><BR>

The division of a tone group into pre-tonic and tonic. See <A HREF = "#tonic">tonic</A><P>

<A NAME = "transducer">
<I>transducer</I>
</A><BR>

A device, often mechanical, for transforming a physical measure into another form, for some purpose.  Thus a pressure transducer might produce an electrical voltage proportional to the applied pressure.
</P>

<A NAME = "transmissionLineAnalogSpeechSynthesiser">
<I>transmission line analog speech synthesiser</I>
</A><BR>

A model of the vocal apparatus used for speech synthesis that models the distributed properties of the vocal apparatus (i.e. its tube-like properties).  Flanagan contrasts it with a terminal analog.  A physiological analog is an example of a transmission line analog.  See also <A HREF = "#terminalAnalogSpeechSynthesiser">terminal analog</A>, <A HREF = "#articulatorySynthesis">articulatory synthesis</A>, <A HREF = "#tubeModel">tube model</A>.
</P>

<A NAME = "travellingWaveTheoryOfHearing">
<I>travelling wave theory of hearing</I>
</A><BR>

The currently accepted theory of <A HREF = "#FrequencyAnalysis">frequency analysis</A> by the mechanical parts of the human auditory system.  Because of the mechanical set-up in the inner ear, and the hydrodynamics of the various ducts, when vibrations are fed into the oval window, travelling waves along the <A HREF = "#basilarMembrane">basilar membrane</A> are set up which cause the maximum displacement of the membrane at a place along the membrane which varies inversely as the logarithm of frequency.  The theory is supported by experiments with models and observations on live guinea pig chochleas.  Bekesy points out that, despite the differences between telephone, resonance, and travelling wave theories that are always emphasized, they are not so different when it comes to analysing transient sounds.  The actual physical properties of the ear correspond to the travelling wave model, and travelling waves may be observed as transients in models set up to represent the <A HREF = "#basilarMembrane">basilar membrane</A> but there is the problem of being able to hear the "missing fundamental", which is considered to require some central analysis of <A HREF = "#nervePulse">nerve pulse</A> repetition rate, since the pitch that is heard cannot be masked or caused to beat with other sounds.  This would require activity of the <A HREF = "#telephoneTheoryOfHearing">telephone theory of hearing</A> kind.  See also <A HREF = "#volleyTheory">volley theory</A>.
</P>


<A NAME = "trapezoidBody">
<I>trapezoid body</I>
</A><BR>

See <A HREF = "#auditoryPathways">auditory pathways</A>.
</P>

<A NAME = "transducer">
<I>transducer</I>
</A><BR>

A device to transform physical measurements such as pressure or temperature into electrical signals for purposes of processing information.  Although it is a term intended for artificial devices, the human sensory organs are (by this definition) very high quality transducers that exceed the performance of any artificial devices ever built for similar conversion purposes.
</P>

<A NAME = "transverseSection">
<I>transverse section</I>
</A><BR>

A section orthogonal to the axis from ventral to dorsal faces of an organism.  A transverse section would divide a human into front and back portions.
</P>


<A NAME = "trigram">
<I>trigram</I>
</A><BR>

Three letters appearing together, in order.  A study of trigram frequencies may be of use in automated language processing.  See <A HREF = "#digramDigraph">digram</A> and <A HREF = "#tetragram">tetragram</A>.
</P>

<A NAME = "triphthong">
<I>triphthong</I>
</A><BR>

A triphthong comprises three vowel articulations in succession.  Like a diphthong, the vowels tend to be shortened and reduced, with an extended transition from one vowel articulation to the other.  Common in British English (for example "fire" "f-ah-i-uh").  See also <A HREF = "#diphthong">diphthong</A><P>

<A NAME = "tubeModel">
<I>tube model</I>
</A><BR>

A model of speech production that simulates the acoustic behaviour of the vocal tract using a transmission line (wave guide) model.  Because the model provides an accurate emulation of the real acoustic behaviour of the relevant acoustic tubes (pharynx, oral passage, and nasal passage), both the dynamics of change from <A HREF="#speechPosture">posture to posture</A> and the distribution of energy (especially in nasalised sounds) are more accurately modelled than in a conventional serial or parallel <A HREF="#formant">formant</A> filter model.  The control problem may be solved by using CarrÈ's "Distinctive Region Model" (DRM) which is based on formant sensitivity analysis work carried out at the <A HREF="#RIT">Speech Technology Laboratory of the KTH</A> in Stockholm by Fant and his colleagues.  The approach simplifies the control of the vocal tract to the manipulation of eight contiguous regions of unequal size.  The approach is exploited by this author and his colleagues in work reported in their paper <A HREF="http://www.firethorne.com/papers/avios95" TARGET="new">"Real-time articulatory speech-synthesis-by-rules"</A>.
</P>

<A NAME = "unvoicedStop">
<I>unvoiced stop</I>
</A><BR>

See <A HREF = "#voicedStop">voiced stop</A>.
</P>


<A NAME = "uvula">
<I>uvula</I>
</A><BR>

See <A HREF = "#velum">velum</A>.
</P>

<A NAME = "velocitySfSound">
<I>velocity of sound</I>
</A><BR>

See <A HREF = "#speed of sound">speed of sound</A>.
</P>

<A NAME = "velum">
<I>velum</I>
</A><BR>

The extreme rear portion of the soft palate that terminates with the uvula (the bit that hangs down and can be seen at the back of the throat by looking into a mirror with the mouth open).  The velum may be raised and lowered, opening and closing the connection between the main vocal passage and the nasal passage. The main vocal passage ultimately terminates at the mouth and the nasal passage at the nostrils.
</P>

<A NAME = "vocalCords">
<I>vocal cords</I>
</A><BR>

See <A HREF = "#vocalFolds">vocal folds</A>.
</P>

<A NAME = "vocalFolds">
<I>vocal folds</I>
</A><BR>

More accurate name for vocal cords.  The fleshy lips at either side of the glottis, within the larynx.  They vibrate when tensioned and subject to enough airflow (from the lungs), giving rise to voiced speech sounds, and providing a basis for intonation patterns as the pitch may be changed by varying the tension.  See <A HREF = "#glottis">glottis</A> and <A HREF = "#voicedStop">voiced stop</A>.
</P>

<A NAME = "vocoder">
<I>vocoder</I>
</A><BR>

A device for <A HREF = "#compression">compressing</A> the bandwidth of speech transmission by exploiting the <A HREF = "#redundancy">redundancy</A> inherent in the speech signal.  There are as many different kinds of vocoder as there are ways of analysing speech.  The first kind of vocoder built, and used for military security in the second world war, was the channel vocoder which exploits the fact that, in a <A HREF = "#spectrographicAnalysis">spectrographic analysis</A>, adjacent areas in the time-frequency-energy plot are highly correlated (or looking at it another way the output of a filter in a given frequency region only varies comparatively slowly -- say not faster than 100 hz, and the output is often quite similar to neighbouring filters).  The speech is analysed into about 15 or 20 frequency channels and <A HREF = "#buzzHissSwitching">buzz/hiss detection</A> and <A HREF = "#voicingDetection">voicing detection</A> are carried out to determine and transmit the excitation function.  Just the energy per channel, the buzz/hiss state, and the voicing frequency are transmitted.  The speech is then reconstituted using the channel signals to give the <A HREF = "#spectralEnvelope">spectral envelope</A>, and the buzz/hiss signal to determine the excitation type, with appropriate <A HREF = "#pitch">pitch</A> (which also varies comparatively slowly) specified by the pitch frequency signal.  A resonance vocoder carries out a similar excitation analysis but sends signals from a <A HREF = "#formantTracking">formant tracker</A> to determine the <A HREF = "#spectralEnvelope">spectral envelope</A>. A <A HREF = "#resonanceAnalog">resonance analog</A> synthesiser then remakes the speech.  There are many other kinds including several based on <A HREF = "#linearPredictiveCoding">Linear Predictive Coding</A> (LPC) analysis.  The current state of the art achieves about 10 or 20:1 compression ratio.  Many types suffer problems of tracking (pitch, formant frequency, etc.).  LPC analysis currently provides the best basis for such speech compression schemes.  LPC compression was used in the <I>Speak 'n Spell</I> toy from Texas Instruments which came out in the summer of 1978.  Because excessive compression was apparently used to meet cost targets for the intended market (roughly a 100:1 compression ratio, with the speech taking only 1200 <A HREF="#bit">bits</A> per second) the speech quality left a something be desired, but was usable.  The speech analysis was carried out "off-line" and the resulting compressed information stored in memory chips inside the machine, but the process was essentially the same as an LPC vocoder. It was an amazing advance in cost reduction for what had been until then an expensive military and research device.
</P>

<A NAME = "voicedStop">
<I>voiced stop</I>
</A><BR>

A type of consonantal speech sound produced by completely closing off the vocal tract at some point, while keeping the <A HREF = "#velum">velum</A> also closed.  Voiced stops are distinguished from unvoiced stops by the posture of the <A HREF = "#vocalFolds">vocal folds</A>. In voiced stops, the folds are adducted -- held close together in the <A HREF = "#voicing">voicing</A> position.  In unvoiced stops, the <A HREF = "#vocalFolds">vocal folds</A> are abducted, opened wide.  For this reason voicing starts much later for unvoiced than for voiced stops in moving into any following following soundóan important acoustic cue. The transitions of formants are partly imposed on <A HREF = "#aspiration">aspiration</A> noise in unvoiced stops for some ten to thirty milliseconds, but on almost entirely on voiced <A HREF = "#spectralFineStructure">spectral fine structure</A> for voiced stops.  If voicing continues during the closure of a voiced stop (as usually happens), then there is a pressure build up above the <A HREF = "#glottis">glottis</A>, reducing the rate of air-flow through the <A HREF = "#vocalFolds">vocal folds</A>.  This causes a drop of up to an octave in the instantaneous <A HREF = "#pitch">pitch</A> and an associated rise in <A HREF = "#pitch">pitch</A> upon release. Such small <A HREF = "#pitch">pitch</A> movements are referred to as "<A HREF = "#microIntonation">micro-intonation</A>" and are mainly associated with constriction of the vocal tract.  Other languages have finer distinctions, and divide stops into three categories based on the time of onset of voicing relative to the transitions associated with the release of the stop <A HREF="#speechPosture">posture</A>.  See also <A HREF = "#intonation">intonation</A>.
</P>

<A NAME = "voicing">
<I>voicing</I>
</A><BR>

A kind of excitation of the vocal tract produced by forcing air through suitably adjusted <A HREF = "#vocalFolds">vocal folds</A>, causing them to vibrate fairly regularly in a frequency range stretching from 20 hz (at the end of utterances,  for males) to 1000 hz (in a high note, for a female singer).  The train of air pulses entering the vocal system has a harmonic energy distribution falling with frequency at about 6 <A HREF = "#db">db</A> per octave.  This spectrum is responsible for the spectral fine structure of voiced sounds which "carries" the resonant response of the vocal tract cavities (technically speaking, the <A HREF = "#spectrum">spectrum</A> of the voicing is convoluted with the spectral response of the vocal tract; both phenomena can be identified in the resulting speech spectrum). The spectral fine structure (voicing harmonics which occur at multiples of the voicing frequency) may be observed in a narrow band spectral analysis (say filter bandwidths of around 45 <A HREF = "#herz">herz</A> for male voices), which smears time variation and thus emphasizes the more slowly varying <A HREF = "#harmonics">harmonic</A> structure. The normal range of male spoken speech, neglecting <A HREF = "#glottalFlap">glottal flap</A> (creaky voice), is approximately 80 hz to 250 hz.  The normal female range is roughly 30% higher on both limits due to a smaller larynx structure (the enlarged male larynx is a secondary sex characteristic).  The range can be much greater in dramatic speech, and female singers can hit 1000 herz (which leads to interesting problems since 1000 herz is certainly higher than <A HREF = "#F1F2F3">F1</A>).  See also <A HREF = "#harmonics">harmonics</A> and <A HREF = "#sourceFilterModelOfSpeechProduction">source-filter model of speech\
 production</A>.
</P>

<A NAME = "voicingDetection">
<I>voicing detection</I>
</A><BR>

The process of detecting by machine whether or not the <A HREF = "#vocalFolds">vocal folds</A> are vibrating during speech, on a moment to moment basis.  The problem is difficult and not completely solved.  It is not quite the same problem as pitch detection, though they are related.  A good pitch detector would require a voicing detector component to avoid spurious pitch values being generated when voicing was absent. It is important to be able to detect the start and end of voicing accurately in all manner of speech processing systems, from bandwidth compressors to speech recognisers, since the distinction between a voiced stop and its voiceless counterpart, for example, may depend on ten to thirty milliseconds difference in the time of onset of voicing, compared to the timing of the <A HREF = "#formant">formant</A> transitions.  With a "live" talker both the presence of voicing and its pitch may be determined fairly easily on a pitch-pulse to pitch-pulse basis using a <A HREF = "#laryngograph">laryngograph</A>, or related equipment that measures the physical phenomena directly using high-frequency electrical resistance across the glottis, light transmission, or other directly related variable</A>.
</P>

<A NAME = "volleyTheory">
<I>volley theory</I>
</A><BR>

The volley theory of hearing arises out of studies originated by Wever & Bray (1930) relating to the nerve impulses produced from the <A HREF="#cochlea"> cochlea</A>.  The theory explains how nerve impulses occurring at frequencies exceeding 900 hz may be transmitted despite the 900 hz cap on individual fibres firing rates arising from the <A HREF = "#absoluteRefractoryPeriod">absolute refractory period</A> which prevents a fibre from firing sooner than about a millisecond after a previous firing.  At a particular phase of successive cycles of the stimulus sound pressure waveform, only some fraction of the potentially active nerve fibres fire, depending on their exact state of readiness, leaving others to fire during subsequent cycles.  Experimental evidence supports the theory.  With increasing frequency of stimulus up to 900 hz, the total intensity of nerve firing (pulses/msec) rises, and at 900 hz a further increase in frequency causes a drop in the density of <A HREF = "#nervePulse">nerve pulses</A> to half what it was at 900 hz, but the inherent nerve pulse repetition rate continues to rise in phase with the stimulus frequency. At 1800 hz a similar effect occurs, with only one third of the nerves firing on any given cycle.  This is consistent with the volley theory.
</P>

<A NAME = "volume">
<I>volume</I>
</A><BR>
The term volume denotes a quality of a sound that is distinct from <A HREF="#loudness">loudness</A> though both are subjective measures.  Volume relates to the "size" or "extensity" of a sound.  Loudness correlates best with acoustic intensity.  Volume correlates better with the spread of associated neural activity.  In "Equal volume judgements of tones." <I>Am. J. Psychology</I><B> 62</B>, 182-201, 1949, Thomas showed that if the total noise power of a band of continuous-spectrum noise is kept constant while the bandwidth is increased (thus activating a wider band on the <A HREF="#basilarMembrane">basilar membrane</A>), the volume increases more rapidly than the loudness.  The figure shows the equal volume contours Thomas obtained.  In addition to volume, loudness and pitch, sounds may differ from one another in terms of "brightness" (a triangle note sounds brighter than a wooden xylophone note), and in terms of "density" (compactness)and a trumpet note is harder, more compact, than the same note on the standard pipes of a pipe organ.</P>

<CENTER>
<A HREF = "images/equvol.jpg" TARGET="new"><IMG SRC="images/equvolthum.jpg">
<BR>Equal volume contours for pure tones</A><BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P>


</CENTER>


<A NAME = "vowel">
<I>vowel</I>
</A><BR>

A speech sound produced using a relatively unobstructed vocal tract, and (except in whispered speech) with <A HREF = "#larynx">larynx</A> excitation (<A HREF = "#voicing">voicing</A>).  Vowels are characterised by "quasi-steady-state" <A HREF = "#formant">formant</A> values, though in practice speech does not contain "steady states", from an acoustic point of view.  From an articulatory point of view vowels are characterised by the position of the tongue hump (high/low, back/front, and degrees in between) that is associated with the notional vowel posture.  In British English high front, low front, high back and low back extremes are exemplified by the vowel sounds in "heed", "had", "who'd", and "hard".  John Holmes, of the Joint Speech Research Unit in England, showed that the notional vowel postures are by no means fixed, and vary greatly according to context.  He displayed continuously varying F1/F2 values on a CRT.  There were no obvious regions corresponding to the vowel articulations.  Donald Broadbent, of the Cambridge University Psychology Department in England showed that the same word may be perceived as having a different vowel, depending on the carrier sentence in his famous "bit/bet/bat" experiment. The differences between the vowel phonemes of Educated Southern British English and those of General American are one of the important characteristics that distinguish the two accents  The International Phetics Association (IPA) has ratified a set of symbols to represent all kinds of speech sounds, but it is hard to quantify the unquantifiable.  Even with the use of so-called <I>cardinal vowels</I>, which represent the extremes of vowel articulation (high front, high back, low front and low back vowels, together with numbered intermediate articulations, specifying the moving targets that comprise vowels is fraught with difficulty. See <A HREF = "#vowelTriangles">vowel triangle</A>.
</P>

<A NAME = "vowelTriangle">
<I>vowel triangle</I>
</A><BR>

If a plot of vowel sound is made against variation in the frequencies of <A HREF = "#formant">formant</A> 1 & 2 (F1 and F2), they are seen to form a roughly triangular figure.  It is of interest that, correctly oriented, this figure also corresponds roughly to tongue hump positions required to produce the corresponding vowels.  Some vowels (central vowels) fall inside the figure, as might be expected.  The vowels and dipthongs represented in the diagrams are for General American.  For the left-hand figure, clock-wise from the top left, the vowels are the ones in: "beat, bit, bet, bat, bart, bought, boot" with "book" and "but" for the left and right "inside" vowels respectively.  For the right-hand figure, from top to bottom, the diphthongs are those in: "bate, bite, cute, quoit, bout, boat" respectively.
</P>
<CENTER>
<A HREF = "images/dh11.jpg" TARGET="new"><IMG SRC="images/dh11thum.jpg"><BR>
The vowel triangle showing locations of vowels and diphthongs</A><BR>
(Note that the vowels are represented by
International Phonetics Association (IPA) symbols)<BR>
<FONT SIZE=1> (From <I>Handbook of Experimental Psychology</I>, edited by S.S. Stevens.<BR>© 1951 John Wiley & Sons. Used with permission of John Wiley & Sons)</FONT>
</P></CENTER>

<A NAME = "VUmeter">
<I>VU meter</I>
</A><BR>

In transmitting or processing of the waveforms of speech and music there is a conflict between the most efficient use of the processing system and the fidelity of reproduction.  If the system is set up to avoid overload on waveform peaks, then much of the time the depth of modulation, say on radio transmissions, may be less than half that possible.  The signal to noise ratio, power requirements for given performance of transmitter, and range (which are inter-related) are likely to suffer.  Automatic gain contol is widely used to alleviate such problems but AGC circuits can overload so it is essential to be able to monitor the input and various stages of processing for maximum performance without degradation.  It is also necessary to monitor subjective loudness (for the listener's benefit), which correlates with the waveform power rather than waveform peaks.  Two classes of meter have developed for speech monitoring, those that respond to speech volume or energy and those that relate to speech peaks.  The former are most widely implemented as the VU (or Volume Unit) meters used in America, and the latter as Peak Program Meters generally used throughout Europe and especially in the BBC.  Both are calibrated in dbm (<A HREF = "#db">db</A> re 1 milliwatt into 600 ohms).  The real difference between properly designed meters of either type are (VUM <I>versus</I> PPM): (1) passive <I>versus</I> active; (2) load the line <I>versus</I> negligible load on the line; (3) different time constant definitions; (4) different scale markings; (5) hard to read <I>versus</I> easy to read.
</P>

<A NAME = "watt">
<I>watt</I>
</A><BR>

A measure of power generation or consumption rate.  Energy flows whenever a force moves through a distance, including electrical forces in electrically resistive media and air pressure waves in air.  Amounts of energy are equivalent regardless of how they are generated or consumed.  To determine a total amount of energy generated or consumed, a time multiplier must be added.  A watt is the rate at which energy is consumed when 1 amp of electrical current flows through 1 ohm or electrical resitance.  The voltage (or electrical pressure) required to cause this to happen is 1 volt.  A realistic measure of domestic electrical power consumption is the Kilowatt, and energy is metered as Kilowatt-hours.  For small amounts of energy, energy consumption rates may be measured in milliwatts. One can talk about the energy flow per unit area.  The threshold of human hearing corresponds to the ability to detect as little as 10 watts/m<SUP2</SUP>, which is equivalent to a milliwatt per cm<SUP>2</SUP>, or less than a milliwatt over the surface of the ear drum (as a continuous pressure fluctuationóa tone).  It has also been stated in different termsóthat the ear can detect movement nearly as small as the diameter of a hydrogen atom.  The movement and the energy input must, in some sense, be equivalent.
</P>

<A NAME = "waveguide">
<I>waveguide</I>
</A><BR>

See <A HREF="#transmissionLineAnalogSpeechSynthesiser">transmission line analog speech synthesiser</A>.
</P>

<A NAME = "WebersLaw">
<I>Weber's law</I>
</A><BR>

Also called the Weber/Fechner law.  It states that, for a given stimulus dimension, the amount of increment (DI) needed to produce a just noticeable difference (JND) in the stimulus (I) is proportional to the stimulus intensity in that dimension.  That is, DI/I = constant.
</P>

<A NAME = "WIP">
<I>WIP</I>
</A><BR>

Edinburgh University, Department of Applied and Theoretical Linguistics (as it now is) "Work in Progress"ówas annual.  There is now a <A HREF="http://www.ling.ed.ac.uk" TARGET="new">web site</A> for the department, and a <A HREF="http://www.cstr.ed.ac.uk" TARGET="new">web site</A> for the Centre for Speech Technology Research (CSTR).
</P>

<A NAME = "WPP">
<I>WPP</I>
</A><BR>

Phonetics Laboratory of the University of California of Los Angeles (UCLA) "Working Papers in Phonetics".  Peter Ladefoged's laboratory.  Their <A HREF="http://www.humnet.ucla.edu/humnet/linguistics/faciliti/uclaplab.html" TARGET="new">web site</A> provides access to a photograph of Peter Ladefoged (no longer the director) teaching Rex Harrison and Wilfred Hyde-Whyte the niceties of Sweet's phonetic symbols on the set of <I>My Fair Lady</I>, the classic adaptation of George Bernard Shaw's <I>Pygmalion</I>.  The lab is now part of the interdisciplinary <A HREF="http://www.icsl.ucla.edu/~spapl/spapl.htm" TARGET="new">Speech Processing & Auditory Perception Labroatory</A> at UCLA.
</P>

<A NAME = "zeroCrossingAnalysis">
<I>zero-crossing analysis</I>
</A><BR>

A speech waveform is bi-polar, crossing and recrossing the zero pressure reference corresponding to the prevailing static pressure (atmospheric pressure).  The rate at which these crossings occur is called the zero-crossing rate and will be proportional to the frequency content of the speech in a rather complicated fashion. The greater and the higher the high frequency content is, the higher will be the zero-crossing rate.  A more accurate idea of the waveform, though still not bearing a simple relationship to the frequency components, is the so-called zero-crossing interval analysis, which generates a histogram of the frequency of occurrence of intervals of different duration between successive zero-crossings.  The value of this representation is improved by differentiating the speech before processing it.  Since the differentiation effectively makes what were local peaks in the original waveform (or "turn-arounds") into zero-crossings, a zero-crossing interval analysis of speech which has been differentiated (and a telephone carbon transmitter does this for nothing!) is exactly the same as an analysis of turnaround intervals.  In either case the higher frequency components are better represented, as might be expected.  A zero-crossing interval histogram is reminiscent of a frequency spectrum, though different.  A zero-crossing analysis of an unprocessed speech waveform misses many of the higher frequency components.
<P>

<HR>
<P>Page last updated 01-05-26</P>

</BODY>
</HTML>